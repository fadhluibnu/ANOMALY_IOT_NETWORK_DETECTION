{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fadhluibnu/ANOMALY_IOT_NETWORK_DETECTION/blob/main/2_IOT_ANOMALI_DETECTION_GROX_GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bagian 1: Preprocessing"
      ],
      "metadata": {
        "id": "DIqHMFUc9KDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-C6AxyE-qpG",
        "outputId": "66f06d12-4679-4056-e038-bd52cf1e7b74"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "OyTIGlgL8AYW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, RobustScaler\n",
        "from sklearn.feature_selection import mutual_info_classif, SelectKBest, VarianceThreshold\n",
        "from imblearn.over_sampling import ADASYN\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline\n",
        "from scipy.stats import zscore\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def enhanced_preprocessing(train_data, test_data):\n",
        "    \"\"\"\n",
        "    Enhanced preprocessing pipeline for IoT network anomaly detection.\n",
        "    Includes improved feature engineering, selection, and balancing.\n",
        "\n",
        "    Args:\n",
        "        train_data: Original training dataframe\n",
        "        test_data: Original testing dataframe\n",
        "\n",
        "    Returns:\n",
        "        processed_data: Dictionary with all processed datasets and loaders\n",
        "    \"\"\"\n",
        "    print(\"\\n🔍 Starting enhanced preprocessing pipeline...\")\n",
        "\n",
        "    # Make copies to avoid modifying originals\n",
        "    train_data = train_data.copy()\n",
        "    test_data = test_data.copy()\n",
        "\n",
        "    # 1. Data Exploration\n",
        "    print(\"\\n📊 Dataset Overview:\")\n",
        "    print(f\"Training data shape: {train_data.shape}\")\n",
        "    print(f\"Testing data shape: {test_data.shape}\")\n",
        "\n",
        "    # Check for missing values\n",
        "    train_missing = train_data.isnull().sum().sum()\n",
        "    test_missing = test_data.isnull().sum().sum()\n",
        "    print(f\"Missing values - Training: {train_missing}, Testing: {test_missing}\")\n",
        "\n",
        "    # 2. Drop rows with missing attack_cat (target variable)\n",
        "    print(\"\\n🔍 Handling missing target values...\")\n",
        "    train_data = train_data.dropna(subset=['attack_cat'])\n",
        "    test_data = test_data.dropna(subset=['attack_cat'])\n",
        "    print(f\"After dropping rows with missing targets - Train: {train_data.shape}, Test: {test_data.shape}\")\n",
        "\n",
        "    # 3. Identify numerical and categorical columns\n",
        "    numerical_cols = train_data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "    numerical_cols = [col for col in numerical_cols if col not in ['id', 'label', 'attack_cat']]\n",
        "    categorical_cols = train_data.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "    # 4. Handle remaining missing values\n",
        "    print(\"\\n🔍 Handling remaining missing values...\")\n",
        "    for col in numerical_cols:\n",
        "        # Use median for numerical features (more robust than mean)\n",
        "        train_data[col] = train_data[col].fillna(train_data[col].median())\n",
        "        test_data[col] = test_data[col].fillna(train_data[col].median())\n",
        "\n",
        "    for col in categorical_cols:\n",
        "        if col != 'attack_cat':\n",
        "            # Use mode (most frequent) for categorical features\n",
        "            train_data[col] = train_data[col].fillna(train_data[col].mode()[0])\n",
        "            test_data[col] = test_data[col].fillna(train_data[col].mode()[0])\n",
        "    train_data.to_csv('/content/drive/MyDrive/ANOMALI DUN DUN/train_data.csv', index=False)\n",
        "    test_data.to_csv('/content/drive/MyDrive/ANOMALI DUN DUN/test_data.csv', index=False)\n",
        "\n",
        "    # 5. Remove duplicate rows\n",
        "    print(\"\\n🔍 Removing duplicate entries...\")\n",
        "    train_data = train_data.drop_duplicates()\n",
        "    test_data = test_data.drop_duplicates()\n",
        "    print(f\"After removing duplicates - Train: {train_data.shape}, Test: {test_data.shape}\")\n",
        "\n",
        "    # 6. Encode attack_cat (target variable)\n",
        "    print(\"\\n🔍 Encoding target variable...\")\n",
        "    attack_mapping = {\n",
        "        'Normal': 0, 'Generic': 1, 'Exploits': 2, 'Fuzzers': 3, 'DoS': 4,\n",
        "        'Reconnaissance': 5, 'Analysis': 6, 'Backdoor': 7, 'Shellcode': 8, 'Worms': 9\n",
        "    }\n",
        "    train_data['attack_cat'] = train_data['attack_cat'].map(attack_mapping)\n",
        "    test_data['attack_cat'] = test_data['attack_cat'].map(attack_mapping)\n",
        "\n",
        "    # print(\"Info\")\n",
        "    # print(train_data.info())\n",
        "\n",
        "    # Check class distribution\n",
        "    class_counts = train_data['attack_cat'].value_counts().sort_index()\n",
        "    print(\"Class distribution after encoding:\")\n",
        "    for class_id, count in class_counts.items():\n",
        "        print(f\"  Class {class_id}: {count} samples ({100*count/len(train_data):.2f}%)\")\n",
        "\n",
        "    # 7. IMPROVED: Handle outliers first, then transform data (before scaling)\n",
        "    print(\"\\n🔍 Handling outliers with improved Winsorization...\")\n",
        "    for col in numerical_cols:\n",
        "        # Use more conservative percentiles for winsorization (0.5% - 99.5%)\n",
        "        lower_bound = train_data[col].quantile(0.005)\n",
        "        upper_bound = train_data[col].quantile(0.995)\n",
        "\n",
        "        # Apply clipping to both train and test data\n",
        "        train_data[col] = train_data[col].clip(lower=lower_bound, upper=upper_bound)\n",
        "        test_data[col] = test_data[col].clip(lower=lower_bound, upper=upper_bound)\n",
        "\n",
        "\n",
        "    # 8. Advanced feature transformation for skewed features\n",
        "    print(\"\\n🔍 Applying transformations for skewed numerical features...\")\n",
        "    for col in numerical_cols:\n",
        "        # Check if data is significantly skewed\n",
        "        skewness = train_data[col].skew()\n",
        "        if abs(skewness) > 1.5:  # More aggressive threshold for transformation\n",
        "            # Apply log transformation (adding a constant to handle zeros/negatives)\n",
        "            train_min = train_data[col].min()\n",
        "            offset = 1 - min(0, train_min)  # Ensure all values are positive\n",
        "\n",
        "            train_data[col] = np.log1p(train_data[col] + offset)\n",
        "            test_data[col] = np.log1p(test_data[col] + offset)\n",
        "            print(f\"  Applied log transform to {col} (skewness: {skewness:.2f})\")\n",
        "\n",
        "    # 9. Encoding categorical features with enhanced handling\n",
        "    print(\"\\n🔍 Encoding categorical features...\")\n",
        "    # Use OneHotEncoder with improved handling for test data\n",
        "    if len(categorical_cols) > 0:\n",
        "        # encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "        # encoded_train = encoder.fit_transform(train_data[categorical_cols])\n",
        "        # encoded_test = encoder.transform(test_data[categorical_cols])\n",
        "\n",
        "        # # Get feature names\n",
        "        # feature_names = encoder.get_feature_names_out(categorical_cols)\n",
        "\n",
        "        # # Create DataFrames with encoded features\n",
        "        # encoded_train_df = pd.DataFrame(encoded_train, columns=feature_names, index=train_data.index)\n",
        "        # encoded_test_df = pd.DataFrame(encoded_test, columns=feature_names, index=test_data.index)\n",
        "\n",
        "        # # Drop original categorical columns and join encoded ones\n",
        "        # train_data = train_data.drop(columns=categorical_cols).reset_index(drop=True)\n",
        "        # test_data = test_data.drop(columns=categorical_cols).reset_index(drop=True)\n",
        "\n",
        "        # train_data = pd.concat([train_data, encoded_train_df], axis=1)\n",
        "        # test_data = pd.concat([test_data, encoded_test_df], axis=1)\n",
        "\n",
        "        categorical_cols = train_data.select_dtypes(include=['object']).columns\n",
        "        encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "        encoded_train = encoder.fit_transform(train_data[categorical_cols])\n",
        "        encoded_test = encoder.transform(test_data[categorical_cols])\n",
        "\n",
        "        encoded_train = pd.DataFrame(encoded_train, columns=encoder.get_feature_names_out(categorical_cols))\n",
        "        encoded_test = pd.DataFrame(encoded_test, columns=encoder.get_feature_names_out(categorical_cols))\n",
        "\n",
        "        train_data = train_data.drop(columns=categorical_cols).reset_index(drop=True)\n",
        "        test_data = test_data.drop(columns=categorical_cols).reset_index(drop=True)\n",
        "\n",
        "        train_data = pd.concat([train_data, encoded_train], axis=1)\n",
        "        test_data = pd.concat([test_data, encoded_test], axis=1)\n",
        "\n",
        "        print(f\"  Encoded {len(categorical_cols)} categorical features into {encoded_train.shape[1]} binary features\")\n",
        "\n",
        "\n",
        "\n",
        "    print(\"AWKWKWInfo\")\n",
        "    print(train_data.info())\n",
        "    print(test_data.info())\n",
        "    # 10. IMPROVED: Two-stage feature selection using variance and mutual information\n",
        "    print(\"\\n🔍 Performing improved two-stage feature selection...\")\n",
        "    # Exclude target and ID columns\n",
        "    X_train = train_data.drop(columns=['id', 'label', 'attack_cat'])\n",
        "    y_train = train_data['attack_cat']\n",
        "\n",
        "    # Stage 1: Remove low variance features\n",
        "    selector_var = VarianceThreshold(threshold=0.01)\n",
        "    selector_var.fit(X_train)\n",
        "    low_var_features = X_train.columns[~selector_var.get_support()].tolist()\n",
        "    if low_var_features:\n",
        "        print(f\"  Removed {len(low_var_features)} low variance features\")\n",
        "        X_train = X_train.drop(columns=low_var_features)\n",
        "\n",
        "    # Stage 2: Select features using mutual information\n",
        "    # Keep more features (95% of remaining or max 150 features)\n",
        "    k = min(150, int(X_train.shape[1] * 0.95))\n",
        "    selector_mi = SelectKBest(mutual_info_classif, k=k)\n",
        "    selector_mi.fit(X_train, y_train)\n",
        "\n",
        "    # Get selected feature names\n",
        "    selected_features = X_train.columns[selector_mi.get_support()].tolist()\n",
        "    print(f\"  Selected {len(selected_features)} features after two-stage selection\")\n",
        "\n",
        "    # Keep only selected features\n",
        "    features_to_keep = ['id', 'label', 'attack_cat'] + selected_features\n",
        "    train_data = train_data[features_to_keep]\n",
        "    test_data = test_data[features_to_keep]\n",
        "\n",
        "    # Update numerical columns list to reflect selected features only\n",
        "    numerical_cols = [col for col in selected_features if col in numerical_cols]\n",
        "\n",
        "    # 11. IMPROVED: Apply scaling after outlier handling\n",
        "    print(\"\\n🔍 Applying robust scaling to numerical features...\")\n",
        "    scaler = RobustScaler()  # Still more robust than StandardScaler\n",
        "\n",
        "    # If no numerical columns remain after selection, skip scaling\n",
        "    if numerical_cols:\n",
        "        train_data[numerical_cols] = scaler.fit_transform(train_data[numerical_cols])\n",
        "        test_data[numerical_cols] = scaler.transform(test_data[numerical_cols])\n",
        "\n",
        "    # 12. IMPROVED: Enhanced resampling with ADASYN and undersampling\n",
        "    print(\"\\n🔍 Applying enhanced balanced resampling...\")\n",
        "    # Prepare data for resampling\n",
        "    X_train = train_data.drop(columns=['id', 'label', 'attack_cat'])\n",
        "    y_train = train_data['attack_cat']\n",
        "\n",
        "    # Print class distribution before resampling\n",
        "    print(\"  Class distribution before resampling:\")\n",
        "    for class_id, count in y_train.value_counts().sort_index().items():\n",
        "        print(f\"    Class {class_id}: {count} samples ({100*count/len(y_train):.2f}%)\")\n",
        "\n",
        "    # Calculate max samples per class - balanced but with limits\n",
        "    max_samples = min(8000, int(len(y_train) * 0.15))  # Limit max samples per minority class\n",
        "    sampling_strategy = {i: min(max_samples, count) for i, count in y_train.value_counts().items()}\n",
        "\n",
        "    # For majority class, keep more samples but not too many\n",
        "    majority_class = y_train.value_counts().idxmax()\n",
        "    sampling_strategy[majority_class] = min(int(len(y_train) * 0.3), y_train.value_counts()[majority_class])\n",
        "\n",
        "    print(\"  Target sample counts per class:\")\n",
        "    for class_id, target_count in sorted(sampling_strategy.items()):\n",
        "        print(f\"    Class {class_id}: {target_count} samples\")\n",
        "\n",
        "    # Apply the resampling pipeline\n",
        "    try:\n",
        "        # First try ADASYN which requires multiple samples per class\n",
        "        resampler = Pipeline([\n",
        "            ('over', ADASYN(sampling_strategy='minority', random_state=42, n_neighbors=5)),\n",
        "            ('under', RandomUnderSampler(sampling_strategy=sampling_strategy, random_state=42))\n",
        "        ])\n",
        "        X_train_resampled, y_train_resampled = resampler.fit_resample(X_train, y_train)\n",
        "    except ValueError as e:\n",
        "        print(f\"  ADASYN failed: {e}. Falling back to SMOTE.\")\n",
        "        # Fall back to SMOTE with different neighbor settings\n",
        "        from imblearn.over_sampling import SMOTE\n",
        "        resampler = Pipeline([\n",
        "            ('over', SMOTE(sampling_strategy='minority', random_state=42, k_neighbors=3)),\n",
        "            ('under', RandomUnderSampler(sampling_strategy=sampling_strategy, random_state=42))\n",
        "        ])\n",
        "        X_train_resampled, y_train_resampled = resampler.fit_resample(X_train, y_train)\n",
        "\n",
        "    # Print class distribution after resampling\n",
        "    print(\"  Class distribution after resampling:\")\n",
        "    for class_id, count in pd.Series(y_train_resampled).value_counts().sort_index().items():\n",
        "        print(f\"    Class {class_id}: {count} samples ({100*count/len(y_train_resampled):.2f}%)\")\n",
        "\n",
        "    # Create new DataFrame with resampled data\n",
        "    train_data_resampled = pd.DataFrame(X_train_resampled, columns=X_train.columns)\n",
        "    train_data_resampled['attack_cat'] = y_train_resampled\n",
        "    train_data_resampled['id'] = range(len(train_data_resampled))\n",
        "    train_data_resampled['label'] = train_data_resampled['attack_cat'] != 0  # Binary label (0=Normal)\n",
        "\n",
        "    # 13. Check for any remaining issues\n",
        "    print(\"\\n🔍 Final data quality check...\")\n",
        "    # Check for infinities\n",
        "    train_data_resampled = train_data_resampled.replace([np.inf, -np.inf], np.nan)\n",
        "    test_data = test_data.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "    # Check for NaN and fill if any\n",
        "    if train_data_resampled.isnull().sum().sum() > 0:\n",
        "        print(f\"  Found {train_data_resampled.isnull().sum().sum()} NaN values in training data. Filling with column medians.\")\n",
        "        for col in train_data_resampled.columns:\n",
        "            if train_data_resampled[col].isnull().sum() > 0:\n",
        "                if train_data_resampled[col].dtype in ['int64', 'float64']:\n",
        "                    train_data_resampled[col] = train_data_resampled[col].fillna(train_data_resampled[col].median())\n",
        "\n",
        "    if test_data.isnull().sum().sum() > 0:\n",
        "        print(f\"  Found {test_data.isnull().sum().sum()} NaN values in test data. Filling with column medians.\")\n",
        "        for col in test_data.columns:\n",
        "            if test_data[col].isnull().sum() > 0:\n",
        "                if test_data[col].dtype in ['int64', 'float64']:\n",
        "                    test_data[col] = test_data[col].fillna(test_data[col].median())\n",
        "\n",
        "    # 14. IMPROVED: Create validation set from training data\n",
        "    print(\"\\n🔍 Creating train/validation split...\")\n",
        "    # Extract features and targets\n",
        "    X_train_final = train_data_resampled.drop(columns=['id', 'label', 'attack_cat'])\n",
        "    y_train_final = train_data_resampled['attack_cat']\n",
        "    X_test_final = test_data.drop(columns=['id', 'label', 'attack_cat'])\n",
        "    y_test_final = test_data['attack_cat']\n",
        "\n",
        "    # Split into training and validation sets with stratification\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
        "        X_train_final, y_train_final, test_size=0.2, random_state=42, stratify=y_train_final\n",
        "    )\n",
        "\n",
        "    print(f\"  Training data: {X_train_split.shape[0]} samples\")\n",
        "    print(f\"  Validation data: {X_val_split.shape[0]} samples\")\n",
        "    print(f\"  Test data: {X_test_final.shape[0]} samples\")\n",
        "\n",
        "    # 15. Convert to tensors\n",
        "    X_train_tensor = torch.tensor(X_train_split.values, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train_split.values, dtype=torch.long)\n",
        "\n",
        "    X_val_tensor = torch.tensor(X_val_split.values, dtype=torch.float32)\n",
        "    y_val_tensor = torch.tensor(y_val_split.values, dtype=torch.long)\n",
        "\n",
        "    X_test_tensor = torch.tensor(X_test_final.values, dtype=torch.float32)\n",
        "    y_test_tensor = torch.tensor(y_test_final.values, dtype=torch.long)\n",
        "\n",
        "    # 16. Create datasets\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "    # 17. Create data loaders with optimized parameters\n",
        "    print(\"\\n🔍 Creating data loaders...\")\n",
        "    batch_size = 64  # Can adjust based on available memory\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        drop_last=False\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        drop_last=False\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        drop_last=False\n",
        "    )\n",
        "\n",
        "    print(f\"\\n✅ Preprocessing complete!\")\n",
        "    print(f\"  Final training set: {len(train_dataset)} samples with {X_train_split.shape[1]} features\")\n",
        "    print(f\"  Final validation set: {len(val_dataset)} samples\")\n",
        "    print(f\"  Final test set: {len(test_dataset)} samples with {X_test_final.shape[1]} features\")\n",
        "\n",
        "    # 18. Visualize class distribution\n",
        "    try:\n",
        "        plt.figure(figsize=(15, 5))\n",
        "\n",
        "        plt.subplot(1, 3, 1)\n",
        "        train_class_counts = pd.Series(y_train_split).value_counts().sort_index()\n",
        "        plt.bar(train_class_counts.index.astype(str), train_class_counts.values)\n",
        "        plt.title('Training Data Class Distribution')\n",
        "        plt.xlabel('Attack Category')\n",
        "        plt.ylabel('Count')\n",
        "        plt.xticks(rotation=45)\n",
        "\n",
        "        plt.subplot(1, 3, 2)\n",
        "        val_class_counts = pd.Series(y_val_split).value_counts().sort_index()\n",
        "        plt.bar(val_class_counts.index.astype(str), val_class_counts.values)\n",
        "        plt.title('Validation Data Class Distribution')\n",
        "        plt.xlabel('Attack Category')\n",
        "        plt.ylabel('Count')\n",
        "        plt.xticks(rotation=45)\n",
        "\n",
        "        plt.subplot(1, 3, 3)\n",
        "        test_class_counts = pd.Series(y_test_final).value_counts().sort_index()\n",
        "        plt.bar(test_class_counts.index.astype(str), test_class_counts.values)\n",
        "        plt.title('Test Data Class Distribution')\n",
        "        plt.xlabel('Attack Category')\n",
        "        plt.ylabel('Count')\n",
        "        plt.xticks(rotation=45)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('class_distribution.png')\n",
        "        plt.close()\n",
        "    except:\n",
        "        print(\"  Could not create visualization (possibly running in non-graphical environment)\")\n",
        "\n",
        "    # Calculate class weights for later use in loss function\n",
        "    num_classes = len(np.unique(y_train_final))\n",
        "    class_counts = np.bincount(y_train_split)\n",
        "\n",
        "    # Handle potential missing classes in the bincount\n",
        "    if len(class_counts) < num_classes:\n",
        "        temp_counts = np.zeros(num_classes)\n",
        "        temp_counts[:len(class_counts)] = class_counts\n",
        "        class_counts = temp_counts\n",
        "\n",
        "    # Calculate inverse frequency class weights, bounded to prevent extreme values\n",
        "    class_weights = np.ones(num_classes)\n",
        "    non_zero_counts = class_counts[class_counts > 0]\n",
        "    if len(non_zero_counts) > 0:\n",
        "        class_weights[class_counts > 0] = 1.0 / class_counts[class_counts > 0]\n",
        "        # Normalize weights to sum to num_classes\n",
        "        class_weights = class_weights * (num_classes / class_weights.sum())\n",
        "        # Bound weights to reasonable range\n",
        "        class_weights = np.clip(class_weights, 0.1, 10.0)\n",
        "\n",
        "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32)\n",
        "\n",
        "    # Return processed data and related objects\n",
        "    return {\n",
        "        'train_data': train_data_resampled,\n",
        "        'test_data': test_data,\n",
        "        'X_train': X_train_split,\n",
        "        'y_train': y_train_split,\n",
        "        'X_val': X_val_split,\n",
        "        'y_val': y_val_split,\n",
        "        'X_test': X_test_final,\n",
        "        'y_test': y_test_final,\n",
        "        'train_loader': train_loader,\n",
        "        'val_loader': val_loader,\n",
        "        'test_loader': test_loader,\n",
        "        'input_dim': X_train_split.shape[1],\n",
        "        'num_classes': num_classes,\n",
        "        'class_weights': class_weights_tensor\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bagian 2: Model CNN + DBN Ensemble"
      ],
      "metadata": {
        "id": "2a1DL2D79R8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import time\n",
        "import os\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Improved 2D CNN for tabular data\n",
        "class RevisedCNN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, dropout_rate=0.3):\n",
        "        super(RevisedCNN, self).__init__()\n",
        "\n",
        "        # Reshape input to 2D representation\n",
        "        self.input_dim = input_dim\n",
        "        self.reshape_dim = int(np.sqrt(input_dim)) + 1\n",
        "        self.pad_size = self.reshape_dim**2 - input_dim\n",
        "\n",
        "        # Convolutional layers with larger kernel and better stride\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=5, stride=1, padding=2)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Compute output size after first conv+pool\n",
        "        conv1_out_size = self.reshape_dim // 2\n",
        "\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Compute output size after second conv+pool\n",
        "        conv2_out_size = conv1_out_size // 2\n",
        "\n",
        "        # Calculate flattened size\n",
        "        self.flattened_size = 128 * max(1, conv2_out_size) * max(1, conv2_out_size)\n",
        "\n",
        "        # Fully connected layers with proper sizes\n",
        "        self.fc1 = nn.Linear(self.flattened_size, 512)\n",
        "        self.bn_fc1 = nn.BatchNorm1d(512)\n",
        "        self.dropout_fc1 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.bn_fc2 = nn.BatchNorm1d(256)\n",
        "        self.dropout_fc2 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        self.fc3 = nn.Linear(256, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Reshape input to 2D (square-like format for better convolution)\n",
        "        x_padded = F.pad(x, (0, self.pad_size))\n",
        "        x = x_padded.view(batch_size, 1, self.reshape_dim, self.reshape_dim)\n",
        "\n",
        "        # Apply convolutional layers\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = F.leaky_relu(x, negative_slope=0.1)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = F.leaky_relu(x, negative_slope=0.1)\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        # Flatten\n",
        "        x = x.view(batch_size, -1)\n",
        "\n",
        "        # Fully connected layers\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn_fc1(x)\n",
        "        x = F.leaky_relu(x, negative_slope=0.1)\n",
        "        x = self.dropout_fc1(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = self.bn_fc2(x)\n",
        "        x = F.leaky_relu(x, negative_slope=0.1)\n",
        "        x = self.dropout_fc2(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Improved RBM with proper pretraining abilities\n",
        "class ImprovedRBM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(ImprovedRBM, self).__init__()\n",
        "\n",
        "        # Initialize weights with small values for stability\n",
        "        self.W = nn.Parameter(torch.randn(input_size, hidden_size) * 0.01)\n",
        "        self.b = nn.Parameter(torch.zeros(hidden_size))\n",
        "        self.c = nn.Parameter(torch.zeros(input_size))\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Xavier/Glorot initialization for better convergence\n",
        "        nn.init.xavier_uniform_(self.W)\n",
        "\n",
        "    def forward(self, v):\n",
        "        \"\"\"Visible to hidden layer probabilities\"\"\"\n",
        "        batch_size = v.size(0)\n",
        "\n",
        "        # Reshape input to 2D if needed\n",
        "        if v.dim() > 2:\n",
        "            v = v.view(batch_size, -1)\n",
        "\n",
        "        # Handle dimension mismatch gracefully\n",
        "        if v.size(1) != self.input_size:\n",
        "            v_resized = torch.zeros(batch_size, self.input_size, device=v.device)\n",
        "            min_size = min(v.size(1), self.input_size)\n",
        "            v_resized[:, :min_size] = v[:, :min_size]\n",
        "            v = v_resized\n",
        "\n",
        "        # Apply L2 normalization for better stability\n",
        "        v = F.normalize(v, p=2, dim=1)\n",
        "\n",
        "        # Compute hidden activations\n",
        "        h_activation = torch.matmul(v, self.W) + self.b\n",
        "        h_probs = torch.sigmoid(h_activation)\n",
        "\n",
        "        return h_probs\n",
        "\n",
        "    def sample_h(self, v):\n",
        "        \"\"\"Sample from the hidden layer given visible state\"\"\"\n",
        "        h_probs = self.forward(v)\n",
        "        h_samples = torch.bernoulli(h_probs)\n",
        "        return h_samples, h_probs\n",
        "\n",
        "    def sample_v(self, h):\n",
        "        \"\"\"Sample from the visible layer given hidden state\"\"\"\n",
        "        v_activation = torch.matmul(h, self.W.t()) + self.c\n",
        "        v_probs = torch.sigmoid(v_activation)\n",
        "        v_samples = torch.bernoulli(v_probs)\n",
        "        return v_samples, v_probs\n",
        "\n",
        "    def cd_k(self, v_data, k=1):\n",
        "        \"\"\"Contrastive Divergence with k steps\"\"\"\n",
        "        h_data, h_data_probs = self.sample_h(v_data)\n",
        "\n",
        "        # Initialize the chain with data samples\n",
        "        h_model = h_data\n",
        "\n",
        "        # Gibbs sampling\n",
        "        for _ in range(k):\n",
        "            v_model, v_model_probs = self.sample_v(h_model)\n",
        "            h_model, h_model_probs = self.sample_h(v_model)\n",
        "\n",
        "        return v_data, h_data_probs, v_model_probs, h_model_probs\n",
        "\n",
        "    def free_energy(self, v):\n",
        "        \"\"\"Calculate free energy\"\"\"\n",
        "        wx_b = torch.matmul(v, self.W) + self.b\n",
        "        hidden_term = torch.sum(F.softplus(wx_b), dim=1)\n",
        "        vbias_term = torch.matmul(v, self.c)\n",
        "        return -hidden_term - vbias_term\n",
        "\n",
        "\n",
        "# Function to pretrain an RBM\n",
        "def pretrain_rbm(rbm, dataloader, device, epochs=5, lr=0.001):\n",
        "    \"\"\"Pretrain RBM using Contrastive Divergence\"\"\"\n",
        "    optimizer = torch.optim.Adam(rbm.parameters(), lr=lr)\n",
        "    rbm.train()\n",
        "\n",
        "    print(f\"Pretraining RBM {rbm.input_size} -> {rbm.hidden_size}...\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        mean_loss = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        for batch_idx, (data, _) in enumerate(dataloader):\n",
        "            data = data.to(device)\n",
        "\n",
        "            # Run k-step Contrastive Divergence\n",
        "            v_data, h_data, v_model, h_model = rbm.cd_k(data, k=1)\n",
        "\n",
        "            # Compute gradients using CD loss\n",
        "            # Positive phase - negative phase\n",
        "            pos_associations = torch.matmul(v_data.t(), h_data)\n",
        "            neg_associations = torch.matmul(v_model.t(), h_model)\n",
        "\n",
        "            # Update weights and biases\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Manually set gradients\n",
        "            rbm.W.grad = -(pos_associations - neg_associations) / data.size(0)\n",
        "            rbm.b.grad = -(h_data - h_model).mean(0)\n",
        "            rbm.c.grad = -(v_data - v_model).mean(0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            # Compute reconstruction error\n",
        "            recon_error = F.mse_loss(v_model, v_data)\n",
        "            mean_loss += recon_error.item()\n",
        "            num_batches += 1\n",
        "\n",
        "        # Print epoch stats\n",
        "        mean_loss /= num_batches\n",
        "        print(f\"  Epoch {epoch+1}/{epochs}, Reconstruction Error: {mean_loss:.6f}\")\n",
        "\n",
        "    return rbm\n",
        "\n",
        "\n",
        "# Improved DBN with proper pretraining\n",
        "class ImprovedDBN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims, output_dim, dropout_rate=0.3):\n",
        "        super(ImprovedDBN, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dims = hidden_dims\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        # Stack of RBM layers\n",
        "        self.rbm_layers = nn.ModuleList()\n",
        "\n",
        "        # First RBM\n",
        "        self.rbm_layers.append(ImprovedRBM(input_dim, hidden_dims[0]))\n",
        "\n",
        "        # Additional RBM layers\n",
        "        for i in range(1, len(hidden_dims)):\n",
        "            self.rbm_layers.append(ImprovedRBM(hidden_dims[i-1], hidden_dims[i]))\n",
        "\n",
        "        # Fully connected layers after RBMs\n",
        "        self.fc_layers = nn.ModuleList()\n",
        "\n",
        "        # First FC layer from last RBM's output\n",
        "        self.fc_layers.append(nn.Sequential(\n",
        "            nn.Linear(hidden_dims[-1], 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Dropout(dropout_rate)\n",
        "        ))\n",
        "\n",
        "        # Second FC layer\n",
        "        self.fc_layers.append(nn.Sequential(\n",
        "            nn.Linear(256, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Dropout(dropout_rate)\n",
        "        ))\n",
        "\n",
        "        # Output layer\n",
        "        self.output_layer = nn.Linear(128, output_dim)\n",
        "\n",
        "        # Better initialization for linear layers\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def pretrain(self, dataloader, device, epochs=5):\n",
        "        \"\"\"Pretrain the DBN layer by layer\"\"\"\n",
        "        print(\"Pretraining DBN layers...\")\n",
        "\n",
        "        # Train first RBM with input data\n",
        "        self.rbm_layers[0] = pretrain_rbm(\n",
        "            self.rbm_layers[0], dataloader, device, epochs)\n",
        "\n",
        "        # For each subsequent layer, train with features from previous layer\n",
        "        for i in range(1, len(self.rbm_layers)):\n",
        "            # Extract features from previous layer\n",
        "            prev_layer_features = []\n",
        "            with torch.no_grad():\n",
        "                for data, _ in dataloader:\n",
        "                    data = data.to(device)\n",
        "\n",
        "                    # Forward pass through previous layers\n",
        "                    for j in range(i):\n",
        "                        data = self.rbm_layers[j](data)\n",
        "\n",
        "                    prev_layer_features.append(data.cpu())\n",
        "\n",
        "            # Create dataset with extracted features\n",
        "            prev_features = torch.cat(prev_layer_features, dim=0)\n",
        "            feature_dataset = TensorDataset(prev_features, torch.zeros(prev_features.size(0)))\n",
        "            feature_loader = DataLoader(\n",
        "                feature_dataset,\n",
        "                batch_size=dataloader.batch_size,\n",
        "                shuffle=True,\n",
        "                num_workers=0\n",
        "            )\n",
        "\n",
        "            # Pretrain this RBM with extracted features\n",
        "            self.rbm_layers[i] = pretrain_rbm(\n",
        "                self.rbm_layers[i], feature_loader, device, epochs)\n",
        "\n",
        "        print(\"DBN pretraining complete.\")\n",
        "        return self\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass through the DBN\"\"\"\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Flatten input if not already flat\n",
        "        if x.dim() > 2:\n",
        "            x = x.view(batch_size, -1)\n",
        "\n",
        "        # Forward through RBM layers\n",
        "        for rbm in self.rbm_layers:\n",
        "            x = rbm(x)\n",
        "\n",
        "        # Forward through FC layers\n",
        "        for fc in self.fc_layers:\n",
        "            x = fc(x)\n",
        "\n",
        "        # Output layer\n",
        "        x = self.output_layer(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Improved dynamic ensemble model\n",
        "class ImprovedDynamicEnsembleModel(nn.Module):\n",
        "    def __init__(self, cnn_model, dbn_model, output_dim):\n",
        "        super(ImprovedDynamicEnsembleModel, self).__init__()\n",
        "        self.cnn_model = cnn_model\n",
        "        self.dbn_model = dbn_model\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        # Class-wise attention - one attention weight per class\n",
        "        self.class_attention = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(2, 16),\n",
        "                nn.LeakyReLU(0.1),\n",
        "                nn.Linear(16, 2),\n",
        "                nn.Softmax(dim=1)\n",
        "            ) for _ in range(output_dim)\n",
        "        ])\n",
        "\n",
        "        # Gating network to decide contribution from each model\n",
        "        self.gate_network = nn.Sequential(\n",
        "            nn.Linear(output_dim * 2, 64),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Linear(64, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Combiner network\n",
        "        self.combiner = nn.Sequential(\n",
        "            nn.Linear(output_dim * 2, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Save original input for DBN\n",
        "        x_original = x.clone()\n",
        "\n",
        "        # Forward pass through CNN\n",
        "        cnn_logits = self.cnn_model(x_original)\n",
        "\n",
        "        # Forward pass through DBN\n",
        "        dbn_logits = self.dbn_model(x_original)\n",
        "\n",
        "        # Get probabilities\n",
        "        cnn_probs = F.softmax(cnn_logits, dim=1)\n",
        "        dbn_probs = F.softmax(dbn_logits, dim=1)\n",
        "\n",
        "        # Confidence values\n",
        "        cnn_conf = torch.max(cnn_probs, dim=1, keepdim=True)[0]\n",
        "        dbn_conf = torch.max(dbn_probs, dim=1, keepdim=True)[0]\n",
        "\n",
        "        # Calculate gating value to balance models\n",
        "        model_confs = torch.cat([cnn_logits, dbn_logits], dim=1)\n",
        "        gate_value = self.gate_network(model_confs)\n",
        "\n",
        "        # Class-wise weighting\n",
        "        weighted_outputs = []\n",
        "        for i in range(self.output_dim):\n",
        "            # Get logits for this class\n",
        "            class_values = torch.cat([\n",
        "                cnn_logits[:, i:i+1],\n",
        "                dbn_logits[:, i:i+1]\n",
        "            ], dim=1)\n",
        "\n",
        "            # Get weights for this class\n",
        "            weights = self.class_attention[i](class_values)\n",
        "\n",
        "            # Weight the outputs\n",
        "            weighted_class = (weights[:, 0:1] * cnn_logits[:, i:i+1] +\n",
        "                             weights[:, 1:2] * dbn_logits[:, i:i+1])\n",
        "            weighted_outputs.append(weighted_class)\n",
        "\n",
        "        # Stack all class outputs\n",
        "        class_weighted_output = torch.cat(weighted_outputs, dim=1)\n",
        "\n",
        "        # Combine with original outputs\n",
        "        combined = torch.cat([cnn_logits, dbn_logits], dim=1)\n",
        "\n",
        "        # Dynamic final output\n",
        "        final_output = gate_value * self.combiner(combined) + (1 - gate_value) * class_weighted_output\n",
        "\n",
        "        return final_output\n",
        "\n",
        "\n",
        "# Focal Loss implementation for handling class imbalance\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha  # Class weights\n",
        "        self.gamma = gamma  # Focusing parameter\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        # Standard cross entropy\n",
        "        ce_loss = F.cross_entropy(\n",
        "            inputs, targets, weight=self.alpha,\n",
        "            reduction='none'\n",
        "        )\n",
        "\n",
        "        # Get probabilities\n",
        "        pt = torch.exp(-ce_loss)\n",
        "\n",
        "        # Calculate focal term\n",
        "        focal_term = (1 - pt) ** self.gamma\n",
        "\n",
        "        # Apply focal term to CE loss\n",
        "        loss = focal_term * ce_loss\n",
        "\n",
        "        # Apply reduction\n",
        "        if self.reduction == 'mean':\n",
        "            return loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return loss.sum()\n",
        "        else:\n",
        "            return loss\n",
        "\n",
        "\n",
        "# Mixup data augmentation function\n",
        "def mixup_data(x, y, alpha=0.2, device='cuda'):\n",
        "    \"\"\"Mixup data augmentation function\"\"\"\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "\n",
        "    batch_size = x.size(0)\n",
        "    index = torch.randperm(batch_size).to(device)\n",
        "\n",
        "    mixed_x = lam * x + (1 - lam) * x[index]\n",
        "    y_a, y_b = y, y[index]\n",
        "\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "\n",
        "# Mixup criterion function\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    \"\"\"Criterion for mixup data augmentation\"\"\"\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "\n",
        "# Training function with advanced techniques\n",
        "def train_ensemble_model(ensemble_model, train_loader, val_loader, test_loader,\n",
        "                        device, class_weights=None, epochs=30, lr=0.001,\n",
        "                        focal_gamma=2.0, mixup_alpha=0.2, save_dir='.'):\n",
        "    print(\"Starting advanced training process...\")\n",
        "\n",
        "    # Create save directory if it doesn't exist\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Setup optimizer with weight decay\n",
        "    optimizer = optim.AdamW(ensemble_model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=3, verbose=True, min_lr=1e-6\n",
        "    )\n",
        "\n",
        "    # Loss function - use focal loss for imbalanced classes\n",
        "    if class_weights is not None:\n",
        "        class_weights = class_weights.to(device)\n",
        "\n",
        "    criterion = FocalLoss(alpha=class_weights, gamma=focal_gamma)\n",
        "\n",
        "    # Tracking metrics\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "    best_val_acc = 0.0\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "\n",
        "    # Training loop with early stopping\n",
        "    for epoch in range(epochs):\n",
        "        # Training phase\n",
        "        start_time = time.time()\n",
        "        ensemble_model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        # Progress bar for training\n",
        "        train_iterator = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n",
        "\n",
        "        for inputs, labels in train_iterator:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Apply mixup augmentation randomly with 50% probability\n",
        "            use_mixup = np.random.random() < 0.5 and mixup_alpha > 0\n",
        "\n",
        "            if use_mixup:\n",
        "                inputs, labels_a, labels_b, lam = mixup_data(inputs, labels, mixup_alpha, device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = ensemble_model(inputs)\n",
        "                loss = mixup_criterion(criterion, outputs, labels_a, labels_b, lam)\n",
        "            else:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = ensemble_model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping to avoid exploding gradients\n",
        "            torch.nn.utils.clip_grad_norm_(ensemble_model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update statistics\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy only for non-mixup batches\n",
        "            if not use_mixup:\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Calculate epoch metrics\n",
        "        epoch_train_loss = running_loss / len(train_loader)\n",
        "\n",
        "        # If we used mixup on all batches, run a separate evaluation pass\n",
        "        if total == 0:\n",
        "            ensemble_model.eval()\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            with torch.no_grad():\n",
        "                for inputs, labels in train_loader:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "                    outputs = ensemble_model(inputs)\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    total += labels.size(0)\n",
        "                    correct += (predicted == labels).sum().item()\n",
        "\n",
        "        epoch_train_acc = 100.0 * correct / total\n",
        "        train_losses.append(epoch_train_loss)\n",
        "        train_accs.append(epoch_train_acc)\n",
        "\n",
        "        # Validation phase\n",
        "        ensemble_model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        all_preds = []\n",
        "        all_targets = []\n",
        "\n",
        "        # Progress bar for validation\n",
        "        val_iterator = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_iterator:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = ensemble_model(inputs)\n",
        "\n",
        "                # Calculate loss\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                # Calculate accuracy\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "                # Save predictions and targets for confusion matrix\n",
        "                all_preds.extend(predicted.cpu().numpy())\n",
        "                all_targets.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Calculate validation metrics\n",
        "        epoch_val_loss = val_loss / len(val_loader)\n",
        "        epoch_val_acc = 100.0 * val_correct / val_total\n",
        "        val_losses.append(epoch_val_loss)\n",
        "        val_accs.append(epoch_val_acc)\n",
        "\n",
        "        # Update learning rate based on validation loss\n",
        "        scheduler.step(epoch_val_loss)\n",
        "\n",
        "        # Calculate epoch duration\n",
        "        epoch_time = time.time() - start_time\n",
        "\n",
        "        # Print epoch results\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Time: {epoch_time:.1f}s | \"\n",
        "              f\"Train Loss: {epoch_train_loss:.4f} | Train Acc: {epoch_train_acc:.2f}% | \"\n",
        "              f\"Val Loss: {epoch_val_loss:.4f} | Val Acc: {epoch_val_acc:.2f}%\")\n",
        "\n",
        "        # Check if this is the best model so far\n",
        "        if epoch_val_acc > best_val_acc:\n",
        "            best_val_acc = epoch_val_acc\n",
        "            best_val_loss = epoch_val_loss\n",
        "            patience_counter = 0\n",
        "\n",
        "            # Save the best model\n",
        "            torch.save(ensemble_model.state_dict(), os.path.join(save_dir, 'best_model.pt'))\n",
        "            print(f\"✅ New best model saved with Val Acc: {epoch_val_acc:.2f}%\")\n",
        "\n",
        "            # Generate confusion matrix for best model\n",
        "            try:\n",
        "                cm = confusion_matrix(all_targets, all_preds)\n",
        "                plt.figure(figsize=(12, 10))\n",
        "                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "                plt.title(f'Validation Confusion Matrix - Epoch {epoch+1}')\n",
        "                plt.xlabel('Predicted')\n",
        "                plt.ylabel('True')\n",
        "                plt.savefig(os.path.join(save_dir, f'confusion_matrix_epoch_{epoch+1}.png'))\n",
        "                plt.close()\n",
        "\n",
        "                # Print detailed classification report\n",
        "                print(\"\\nClassification Report:\")\n",
        "                print(classification_report(all_targets, all_preds))\n",
        "            except Exception as e:\n",
        "                print(f\"Could not generate confusion matrix: {e}\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        # Early stopping check\n",
        "        if patience_counter >= 7:  # Stop if no improvement for 7 epochs\n",
        "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
        "            break\n",
        "\n",
        "    # Final evaluation on test set\n",
        "    print(\"\\nEvaluating final model on test set...\")\n",
        "    ensemble_model.load_state_dict(torch.load(os.path.join(save_dir, 'best_model.pt')))\n",
        "    ensemble_model.eval()\n",
        "    test_correct = 0\n",
        "    test_total = 0\n",
        "    all_test_preds = []\n",
        "    all_test_targets = []\n",
        "    all_test_scores = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = ensemble_model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            test_total += labels.size(0)\n",
        "            test_correct += (predicted == labels).sum().item()\n",
        "            all_test_preds.extend(predicted.cpu().numpy())\n",
        "            all_test_targets.extend(labels.cpu().numpy())\n",
        "            all_test_scores.extend(F.softmax(outputs, dim=1).cpu().numpy())\n",
        "\n",
        "    test_acc = 100.0 * test_correct / test_total\n",
        "    print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "    # Calculate metrics per class\n",
        "    from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
        "\n",
        "    # Convert to 1D arrays\n",
        "    y_true = np.array(all_test_targets)\n",
        "    y_pred = np.array(all_test_preds)\n",
        "    y_score = np.array(all_test_scores)\n",
        "\n",
        "    # Calculate precision, recall, and F1-score per class\n",
        "    precision, recall, f1, support = precision_recall_fscore_support(\n",
        "        y_true, y_pred, average=None)\n",
        "\n",
        "    # Calculate metrics\n",
        "    weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(\n",
        "        y_true, y_pred, average='weighted')\n",
        "\n",
        "    # Attempt to calculate ROC AUC score\n",
        "    try:\n",
        "        roc_auc = roc_auc_score(y_true, y_score, multi_class='ovr', average='weighted')\n",
        "        print(f\"ROC AUC Score (weighted): {roc_auc:.4f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not calculate ROC AUC: {e}\")\n",
        "\n",
        "    print(f\"Weighted Precision: {weighted_precision:.4f}\")\n",
        "    print(f\"Weighted Recall: {weighted_recall:.4f}\")\n",
        "    print(f\"Weighted F1 Score: {weighted_f1:.4f}\")\n",
        "\n",
        "    # Generate final confusion matrix and report\n",
        "    try:\n",
        "        cm = confusion_matrix(all_test_targets, all_test_preds)\n",
        "        plt.figure(figsize=(14, 12))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "        plt.title('Final Test Confusion Matrix')\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('True')\n",
        "        plt.savefig(os.path.join(save_dir, 'final_test_confusion_matrix.png'))\n",
        "        plt.close()\n",
        "\n",
        "        # Per-class metrics visualization\n",
        "        plt.figure(figsize=(14, 8))\n",
        "        indices = np.arange(len(precision))\n",
        "        width = 0.25\n",
        "\n",
        "        plt.bar(indices - width, precision, width, label='Precision')\n",
        "        plt.bar(indices, recall, width, label='Recall')\n",
        "        plt.bar(indices + width, f1, width, label='F1-Score')\n",
        "\n",
        "        plt.xlabel('Class')\n",
        "        plt.ylabel('Score')\n",
        "        plt.title('Per-Class Classification Metrics')\n",
        "        plt.xticks(indices)\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(save_dir, 'per_class_metrics.png'))\n",
        "        plt.close()\n",
        "\n",
        "        print(\"\\nFinal Classification Report:\")\n",
        "        print(classification_report(all_test_targets, all_test_preds))\n",
        "    except Exception as e:\n",
        "        print(f\"Could not generate final confusion matrix: {e}\")\n",
        "\n",
        "    # Plot training history\n",
        "    plt.figure(figsize=(15, 6))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.title('Loss over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(train_accs, label='Training Accuracy')\n",
        "    plt.plot(val_accs, label='Validation Accuracy')\n",
        "    plt.title('Accuracy over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(save_dir, 'training_history.png'))\n",
        "    plt.close()\n",
        "\n",
        "    return ensemble_model, test_acc\n"
      ],
      "metadata": {
        "id": "71EORVoQ9Y9N"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running Preprocessing dan CNN+DBN"
      ],
      "metadata": {
        "id": "mXGcyX409kyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function to run the whole pipeline\n",
        "def run_improved_iot_anomaly_detection(train_data, test_data, save_dir='model_results'):\n",
        "    print(\"\\n🚀 Starting improved IoT network anomaly detection pipeline...\")\n",
        "\n",
        "    # Step 1: Preprocessing data\n",
        "    print(\"\\n📊 Running enhanced data preprocessing...\")\n",
        "    processed_data = enhanced_preprocessing(train_data, test_data)\n",
        "\n",
        "    # Extract necessary components from processed data\n",
        "    train_loader = processed_data['train_loader']\n",
        "    val_loader = processed_data['val_loader']\n",
        "    test_loader = processed_data['test_loader']\n",
        "    input_dim = processed_data['input_dim']\n",
        "    num_classes = processed_data['num_classes']\n",
        "    class_weights = processed_data['class_weights']\n",
        "    train_data = processed_data['train_data']  # Get updated train_data\n",
        "    test_data = processed_data['test_data']\n",
        "\n",
        "    # Step 2: Set up device for training\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    class_weights = class_weights.to(device)\n",
        "    print(f\"\\n🔧 Using device: {device}\")\n",
        "\n",
        "    # Step 3: Create improved CNN model\n",
        "    print(\"\\n🧠 Creating improved CNN model...\")\n",
        "    cnn_model = RevisedCNN(\n",
        "        input_dim=input_dim,\n",
        "        output_dim=num_classes,\n",
        "        dropout_rate=0.3\n",
        "    ).to(device)\n",
        "    print(f\"CNN model created with {input_dim} input features and {num_classes} output classes\")\n",
        "\n",
        "    # Step 4: Create improved DBN model with pretraining\n",
        "    print(\"\\n🧠 Creating improved DBN model...\")\n",
        "    dbn_model = ImprovedDBN(\n",
        "        input_dim=input_dim,\n",
        "        hidden_dims=[512, 384, 256],\n",
        "        output_dim=num_classes,\n",
        "        dropout_rate=0.3\n",
        "    ).to(device)\n",
        "\n",
        "    # Pretrain DBN layers\n",
        "    dbn_model.pretrain(train_loader, device, epochs=3)\n",
        "    print(\"DBN model created and pretrained\")\n",
        "\n",
        "    # Step 5: Create improved ensemble model\n",
        "    print(\"\\n🧠 Creating improved dynamic ensemble model...\")\n",
        "    ensemble_model = ImprovedDynamicEnsembleModel(\n",
        "        cnn_model=cnn_model,\n",
        "        dbn_model=dbn_model,\n",
        "        output_dim=num_classes\n",
        "    ).to(device)\n",
        "    print(\"Ensemble model created\")\n",
        "\n",
        "    # Step 6: Train the ensemble model\n",
        "    print(\"\\n🏋️‍♀️ Starting advanced training process...\")\n",
        "    trained_model, test_accuracy = train_ensemble_model(\n",
        "        ensemble_model=ensemble_model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        test_loader=test_loader,\n",
        "        device=device,\n",
        "        class_weights=class_weights,\n",
        "        epochs=30,\n",
        "        lr=0.001,\n",
        "        focal_gamma=2.0,\n",
        "        mixup_alpha=0.2,\n",
        "        save_dir=save_dir\n",
        "    )\n",
        "\n",
        "    print(f\"\\n🏁 Final test accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "    # Return the trained model and results\n",
        "    results = {\n",
        "        'model': trained_model,\n",
        "        'test_accuracy': test_accuracy,\n",
        "        'input_dim': input_dim,\n",
        "        'num_classes': num_classes,\n",
        "        'model_path': os.path.join(save_dir, 'best_model.pt')\n",
        "    }\n",
        "\n",
        "    return results\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Load dataset UNSW-NB15\n",
        "# Asumsikan Anda memiliki file CSV untuk training dan testing\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/ANOMALI DUN DUN/UNSW_NB15_training-set.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/ANOMALI DUN DUN/UNSW_NB15_testing-set.csv')\n",
        "\n",
        "# 2. Jalankan pipeline pemrosesan dan model\n",
        "results = run_improved_iot_anomaly_detection(train_data, test_data, save_dir='iot_anomaly_results')\n",
        "\n",
        "print(f\"Final model accuracy: {results['test_accuracy']:.2f}%\")\n",
        "print(f\"Model saved to: {results['model_path']}\")"
      ],
      "metadata": {
        "id": "_Az7ztq69efe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a3db3620-7712-4b5f-ef9c-a74ba97bf826"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 Starting improved IoT network anomaly detection pipeline...\n",
            "\n",
            "📊 Running enhanced data preprocessing...\n",
            "\n",
            "🔍 Starting enhanced preprocessing pipeline...\n",
            "\n",
            "📊 Dataset Overview:\n",
            "Training data shape: (82332, 45)\n",
            "Testing data shape: (175341, 45)\n",
            "Missing values - Training: 0, Testing: 0\n",
            "\n",
            "🔍 Handling missing target values...\n",
            "After dropping rows with missing targets - Train: (82332, 45), Test: (175341, 45)\n",
            "\n",
            "🔍 Handling remaining missing values...\n",
            "\n",
            "🔍 Removing duplicate entries...\n",
            "After removing duplicates - Train: (82332, 45), Test: (175341, 45)\n",
            "\n",
            "🔍 Encoding target variable...\n",
            "Class distribution after encoding:\n",
            "  Class 0: 37000 samples (44.94%)\n",
            "  Class 1: 18871 samples (22.92%)\n",
            "  Class 2: 11132 samples (13.52%)\n",
            "  Class 3: 6062 samples (7.36%)\n",
            "  Class 4: 4089 samples (4.97%)\n",
            "  Class 5: 3496 samples (4.25%)\n",
            "  Class 6: 677 samples (0.82%)\n",
            "  Class 7: 583 samples (0.71%)\n",
            "  Class 8: 378 samples (0.46%)\n",
            "  Class 9: 44 samples (0.05%)\n",
            "\n",
            "🔍 Handling outliers with improved Winsorization...\n",
            "\n",
            "🔍 Applying transformations for skewed numerical features...\n",
            "  Applied log transform to dur (skewness: 8.53)\n",
            "  Applied log transform to spkts (skewness: 6.10)\n",
            "  Applied log transform to dpkts (skewness: 7.56)\n",
            "  Applied log transform to sbytes (skewness: 7.65)\n",
            "  Applied log transform to dbytes (skewness: 8.91)\n",
            "  Applied log transform to rate (skewness: 3.50)\n",
            "  Applied log transform to sload (skewness: 4.44)\n",
            "  Applied log transform to dload (skewness: 4.70)\n",
            "  Applied log transform to sloss (skewness: 5.67)\n",
            "  Applied log transform to dloss (skewness: 8.04)\n",
            "  Applied log transform to sinpkt (skewness: 9.27)\n",
            "  Applied log transform to dinpkt (skewness: 11.19)\n",
            "  Applied log transform to sjit (skewness: 11.26)\n",
            "  Applied log transform to djit (skewness: 5.71)\n",
            "  Applied log transform to tcprtt (skewness: 1.99)\n",
            "  Applied log transform to synack (skewness: 2.12)\n",
            "  Applied log transform to ackdat (skewness: 1.97)\n",
            "  Applied log transform to smean (skewness: 3.48)\n",
            "  Applied log transform to dmean (skewness: 3.05)\n",
            "  Applied log transform to trans_depth (skewness: 2.82)\n",
            "  Applied log transform to response_body_len (skewness: 9.50)\n",
            "  Applied log transform to ct_srv_src (skewness: 1.83)\n",
            "  Applied log transform to ct_state_ttl (skewness: 1.52)\n",
            "  Applied log transform to ct_dst_ltm (skewness: 2.65)\n",
            "  Applied log transform to ct_src_dport_ltm (skewness: 2.84)\n",
            "  Applied log transform to ct_dst_sport_ltm (skewness: 2.50)\n",
            "  Applied log transform to ct_dst_src_ltm (skewness: 2.16)\n",
            "  Applied log transform to is_ftp_login (skewness: 10.88)\n",
            "  Applied log transform to ct_ftp_cmd (skewness: 10.87)\n",
            "  Applied log transform to ct_flw_http_mthd (skewness: 5.92)\n",
            "  Applied log transform to ct_src_ltm (skewness: 2.41)\n",
            "  Applied log transform to ct_srv_dst (skewness: 1.88)\n",
            "  Applied log transform to is_sm_ips_ports (skewness: 9.32)\n",
            "\n",
            "🔍 Encoding categorical features...\n",
            "  Encoded 3 categorical features into 151 binary features\n",
            "AWKWKWInfo\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 82332 entries, 0 to 82331\n",
            "Columns: 193 entries, id to state_RST\n",
            "dtypes: float64(186), int64(7)\n",
            "memory usage: 121.2 MB\n",
            "None\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 175341 entries, 0 to 175340\n",
            "Columns: 193 entries, id to state_RST\n",
            "dtypes: float64(186), int64(7)\n",
            "memory usage: 258.2 MB\n",
            "None\n",
            "\n",
            "🔍 Performing improved two-stage feature selection...\n",
            "  Removed 143 low variance features\n",
            "  Selected 44 features after two-stage selection\n",
            "\n",
            "🔍 Applying robust scaling to numerical features...\n",
            "\n",
            "🔍 Applying enhanced balanced resampling...\n",
            "  Class distribution before resampling:\n",
            "    Class 0: 37000 samples (44.94%)\n",
            "    Class 1: 18871 samples (22.92%)\n",
            "    Class 2: 11132 samples (13.52%)\n",
            "    Class 3: 6062 samples (7.36%)\n",
            "    Class 4: 4089 samples (4.97%)\n",
            "    Class 5: 3496 samples (4.25%)\n",
            "    Class 6: 677 samples (0.82%)\n",
            "    Class 7: 583 samples (0.71%)\n",
            "    Class 8: 378 samples (0.46%)\n",
            "    Class 9: 44 samples (0.05%)\n",
            "  Target sample counts per class:\n",
            "    Class 0: 24699 samples\n",
            "    Class 1: 8000 samples\n",
            "    Class 2: 8000 samples\n",
            "    Class 3: 6062 samples\n",
            "    Class 4: 4089 samples\n",
            "    Class 5: 3496 samples\n",
            "    Class 6: 677 samples\n",
            "    Class 7: 583 samples\n",
            "    Class 8: 378 samples\n",
            "    Class 9: 44 samples\n",
            "  Class distribution after resampling:\n",
            "    Class 0: 24699 samples (44.08%)\n",
            "    Class 1: 8000 samples (14.28%)\n",
            "    Class 2: 8000 samples (14.28%)\n",
            "    Class 3: 6062 samples (10.82%)\n",
            "    Class 4: 4089 samples (7.30%)\n",
            "    Class 5: 3496 samples (6.24%)\n",
            "    Class 6: 677 samples (1.21%)\n",
            "    Class 7: 583 samples (1.04%)\n",
            "    Class 8: 378 samples (0.67%)\n",
            "    Class 9: 44 samples (0.08%)\n",
            "\n",
            "🔍 Final data quality check...\n",
            "\n",
            "🔍 Creating train/validation split...\n",
            "  Training data: 44822 samples\n",
            "  Validation data: 11206 samples\n",
            "  Test data: 175341 samples\n",
            "\n",
            "🔍 Creating data loaders...\n",
            "\n",
            "✅ Preprocessing complete!\n",
            "  Final training set: 44822 samples with 44 features\n",
            "  Final validation set: 11206 samples\n",
            "  Final test set: 175341 samples with 44 features\n",
            "\n",
            "🔧 Using device: cuda\n",
            "\n",
            "🧠 Creating improved CNN model...\n",
            "CNN model created with 44 input features and 10 output classes\n",
            "\n",
            "🧠 Creating improved DBN model...\n",
            "Pretraining DBN layers...\n",
            "Pretraining RBM 44 -> 512...\n",
            "  Epoch 1/3, Reconstruction Error: 0.433921\n",
            "  Epoch 2/3, Reconstruction Error: 0.394370\n",
            "  Epoch 3/3, Reconstruction Error: 0.388315\n",
            "Pretraining RBM 512 -> 384...\n",
            "  Epoch 1/3, Reconstruction Error: 0.035413\n",
            "  Epoch 2/3, Reconstruction Error: 0.006119\n",
            "  Epoch 3/3, Reconstruction Error: 0.004527\n",
            "Pretraining RBM 384 -> 256...\n",
            "  Epoch 1/3, Reconstruction Error: 0.007380\n",
            "  Epoch 2/3, Reconstruction Error: 0.005036\n",
            "  Epoch 3/3, Reconstruction Error: 0.004916\n",
            "DBN pretraining complete.\n",
            "DBN model created and pretrained\n",
            "\n",
            "🧠 Creating improved dynamic ensemble model...\n",
            "Ensemble model created\n",
            "\n",
            "🏋️‍♀️ Starting advanced training process...\n",
            "Starting advanced training process...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/30 [Train]:   0%|          | 0/701 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 3",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-5e462be18181>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;31m# 2. Jalankan pipeline pemrosesan dan model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_improved_iot_anomaly_detection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'iot_anomaly_results'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Final model accuracy: {results['test_accuracy']:.2f}%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-5e462be18181>\u001b[0m in \u001b[0;36mrun_improved_iot_anomaly_detection\u001b[0;34m(train_data, test_data, save_dir)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;31m# Step 6: Train the ensemble model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n🏋️‍♀️ Starting advanced training process...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     trained_model, test_accuracy = train_ensemble_model(\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mensemble_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensemble_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-c5c23c2c9e40>\u001b[0m in \u001b[0;36mtrain_ensemble_model\u001b[0;34m(ensemble_model, train_loader, val_loader, test_loader, device, class_weights, epochs, lr, focal_gamma, mixup_alpha, save_dir)\u001b[0m\n\u001b[1;32m    525\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensemble_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-c5c23c2c9e40>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0;31m# Forward pass through CNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m         \u001b[0mcnn_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_original\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0;31m# Forward pass through DBN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-c5c23c2c9e40>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;31m# Prepare for LSTM (batch_size, seq_len, features)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;31m# LSTM Layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 3"
          ]
        }
      ]
    }
  ]
}