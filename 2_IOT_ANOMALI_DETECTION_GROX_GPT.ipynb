{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fadhluibnu/ANOMALY_IOT_NETWORK_DETECTION/blob/main/2_IOT_ANOMALI_DETECTION_GROX_GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bagian 1: Preprocessing"
      ],
      "metadata": {
        "id": "DIqHMFUc9KDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-C6AxyE-qpG",
        "outputId": "49c54d3c-e3a3-4248-a0e5-7808df517f47"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "OyTIGlgL8AYW"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import torch\n",
        "# from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "# from sklearn.preprocessing import OneHotEncoder, StandardScaler, RobustScaler\n",
        "# from sklearn.feature_selection import mutual_info_classif, SelectKBest, VarianceThreshold\n",
        "# from imblearn.over_sampling import ADASYN\n",
        "# from imblearn.under_sampling import RandomUnderSampler\n",
        "# from imblearn.pipeline import Pipeline\n",
        "# from scipy.stats import zscore\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "# import warnings\n",
        "# warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# def enhanced_preprocessing(train_data, test_data):\n",
        "#     \"\"\"\n",
        "#     Enhanced preprocessing pipeline for IoT network anomaly detection.\n",
        "#     Includes improved feature engineering, selection, and balancing.\n",
        "\n",
        "#     Args:\n",
        "#         train_data: Original training dataframe\n",
        "#         test_data: Original testing dataframe\n",
        "\n",
        "#     Returns:\n",
        "#         processed_data: Dictionary with all processed datasets and loaders\n",
        "#     \"\"\"\n",
        "#     print(\"\\n🔍 Starting enhanced preprocessing pipeline...\")\n",
        "\n",
        "#     # Make copies to avoid modifying originals\n",
        "#     train_data = train_data.copy()\n",
        "#     test_data = test_data.copy()\n",
        "\n",
        "#     # 1. Data Exploration\n",
        "#     print(\"\\n📊 Dataset Overview:\")\n",
        "#     print(f\"Training data shape: {train_data.shape}\")\n",
        "#     print(f\"Testing data shape: {test_data.shape}\")\n",
        "\n",
        "#     # Check for missing values\n",
        "#     train_missing = train_data.isnull().sum().sum()\n",
        "#     test_missing = test_data.isnull().sum().sum()\n",
        "#     print(f\"Missing values - Training: {train_missing}, Testing: {test_missing}\")\n",
        "\n",
        "#     # 2. Drop rows with missing attack_cat (target variable)\n",
        "#     print(\"\\n🔍 Handling missing target values...\")\n",
        "#     train_data = train_data.dropna(subset=['attack_cat'])\n",
        "#     test_data = test_data.dropna(subset=['attack_cat'])\n",
        "#     print(f\"After dropping rows with missing targets - Train: {train_data.shape}, Test: {test_data.shape}\")\n",
        "\n",
        "#     # 3. Identify numerical and categorical columns\n",
        "#     numerical_cols = train_data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "#     numerical_cols = [col for col in numerical_cols if col not in ['id', 'label', 'attack_cat']]\n",
        "#     categorical_cols = train_data.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "#     # 4. Handle remaining missing values\n",
        "#     print(\"\\n🔍 Handling remaining missing values...\")\n",
        "#     for col in numerical_cols:\n",
        "#         # Use median for numerical features (more robust than mean)\n",
        "#         train_data[col] = train_data[col].fillna(train_data[col].median())\n",
        "#         test_data[col] = test_data[col].fillna(train_data[col].median())\n",
        "\n",
        "#     for col in categorical_cols:\n",
        "#         if col != 'attack_cat':\n",
        "#             # Use mode (most frequent) for categorical features\n",
        "#             train_data[col] = train_data[col].fillna(train_data[col].mode()[0])\n",
        "#             test_data[col] = test_data[col].fillna(train_data[col].mode()[0])\n",
        "#     # train_data.to_csv('/content/drive/MyDrive/ANOMALI DUN DUN/train_data.csv', index=False)\n",
        "#     # test_data.to_csv('/content/drive/MyDrive/ANOMALI DUN DUN/test_data.csv', index=False)\n",
        "\n",
        "#     # 5. Remove duplicate rows\n",
        "#     print(\"\\n🔍 Removing duplicate entries...\")\n",
        "#     train_data = train_data.drop_duplicates()\n",
        "#     test_data = test_data.drop_duplicates()\n",
        "#     print(f\"After removing duplicates - Train: {train_data.shape}, Test: {test_data.shape}\")\n",
        "\n",
        "#     # 6. Encode attack_cat (target variable)\n",
        "#     print(\"\\n🔍 Encoding target variable...\")\n",
        "#     attack_mapping = {\n",
        "#         'Normal': 0, 'Generic': 1, 'Exploits': 2, 'Fuzzers': 3, 'DoS': 4,\n",
        "#         'Reconnaissance': 5, 'Analysis': 6, 'Backdoor': 7, 'Shellcode': 8, 'Worms': 9\n",
        "#     }\n",
        "#     train_data['attack_cat'] = train_data['attack_cat'].map(attack_mapping)\n",
        "#     test_data['attack_cat'] = test_data['attack_cat'].map(attack_mapping)\n",
        "\n",
        "#     # print(\"Info\")\n",
        "#     # print(train_data.info())\n",
        "\n",
        "#     # Check class distribution\n",
        "#     class_counts = train_data['attack_cat'].value_counts().sort_index()\n",
        "#     print(\"Class distribution after encoding:\")\n",
        "#     for class_id, count in class_counts.items():\n",
        "#         print(f\"  Class {class_id}: {count} samples ({100*count/len(train_data):.2f}%)\")\n",
        "\n",
        "#     # 7. IMPROVED: Handle outliers first, then transform data (before scaling)\n",
        "#     print(\"\\n🔍 Handling outliers with improved Winsorization...\")\n",
        "#     for col in numerical_cols:\n",
        "#         # Use more conservative percentiles for winsorization (0.5% - 99.5%)\n",
        "#         lower_bound = train_data[col].quantile(0.005)\n",
        "#         upper_bound = train_data[col].quantile(0.995)\n",
        "\n",
        "#         # Apply clipping to both train and test data\n",
        "#         train_data[col] = train_data[col].clip(lower=lower_bound, upper=upper_bound)\n",
        "#         test_data[col] = test_data[col].clip(lower=lower_bound, upper=upper_bound)\n",
        "\n",
        "\n",
        "#     # 8. Advanced feature transformation for skewed features\n",
        "#     print(\"\\n🔍 Applying transformations for skewed numerical features...\")\n",
        "#     for col in numerical_cols:\n",
        "#         # Check if data is significantly skewed\n",
        "#         skewness = train_data[col].skew()\n",
        "#         if abs(skewness) > 1.5:  # More aggressive threshold for transformation\n",
        "#             # Apply log transformation (adding a constant to handle zeros/negatives)\n",
        "#             train_min = train_data[col].min()\n",
        "#             offset = 1 - min(0, train_min)  # Ensure all values are positive\n",
        "\n",
        "#             train_data[col] = np.log1p(train_data[col] + offset)\n",
        "#             test_data[col] = np.log1p(test_data[col] + offset)\n",
        "#             print(f\"  Applied log transform to {col} (skewness: {skewness:.2f})\")\n",
        "\n",
        "#     # 9. Encoding categorical features with enhanced handling\n",
        "#     print(\"\\n🔍 Encoding categorical features...\")\n",
        "#     # Use OneHotEncoder with improved handling for test data\n",
        "#     if len(categorical_cols) > 0:\n",
        "#         # encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "#         # encoded_train = encoder.fit_transform(train_data[categorical_cols])\n",
        "#         # encoded_test = encoder.transform(test_data[categorical_cols])\n",
        "\n",
        "#         # # Get feature names\n",
        "#         # feature_names = encoder.get_feature_names_out(categorical_cols)\n",
        "\n",
        "#         # # Create DataFrames with encoded features\n",
        "#         # encoded_train_df = pd.DataFrame(encoded_train, columns=feature_names, index=train_data.index)\n",
        "#         # encoded_test_df = pd.DataFrame(encoded_test, columns=feature_names, index=test_data.index)\n",
        "\n",
        "#         # # Drop original categorical columns and join encoded ones\n",
        "#         # train_data = train_data.drop(columns=categorical_cols).reset_index(drop=True)\n",
        "#         # test_data = test_data.drop(columns=categorical_cols).reset_index(drop=True)\n",
        "\n",
        "#         # train_data = pd.concat([train_data, encoded_train_df], axis=1)\n",
        "#         # test_data = pd.concat([test_data, encoded_test_df], axis=1)\n",
        "\n",
        "#         categorical_cols = train_data.select_dtypes(include=['object']).columns\n",
        "#         encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "#         encoded_train = encoder.fit_transform(train_data[categorical_cols])\n",
        "#         encoded_test = encoder.transform(test_data[categorical_cols])\n",
        "\n",
        "#         encoded_train = pd.DataFrame(encoded_train, columns=encoder.get_feature_names_out(categorical_cols))\n",
        "#         encoded_test = pd.DataFrame(encoded_test, columns=encoder.get_feature_names_out(categorical_cols))\n",
        "\n",
        "#         train_data = train_data.drop(columns=categorical_cols).reset_index(drop=True)\n",
        "#         test_data = test_data.drop(columns=categorical_cols).reset_index(drop=True)\n",
        "\n",
        "#         train_data = pd.concat([train_data, encoded_train], axis=1)\n",
        "#         test_data = pd.concat([test_data, encoded_test], axis=1)\n",
        "\n",
        "#         print(f\"  Encoded {len(categorical_cols)} categorical features into {encoded_train.shape[1]} binary features\")\n",
        "\n",
        "\n",
        "\n",
        "#     print(\"AWKWKWInfo\")\n",
        "#     print(train_data.info())\n",
        "#     print(test_data.info())\n",
        "#     # 10. IMPROVED: Two-stage feature selection using variance and mutual information\n",
        "#     print(\"\\n🔍 Performing improved two-stage feature selection...\")\n",
        "#     # Exclude target and ID columns\n",
        "#     X_train = train_data.drop(columns=['id', 'label', 'attack_cat'])\n",
        "#     y_train = train_data['attack_cat']\n",
        "\n",
        "#     # Stage 1: Remove low variance features\n",
        "#     selector_var = VarianceThreshold(threshold=0.01)\n",
        "#     selector_var.fit(X_train)\n",
        "#     low_var_features = X_train.columns[~selector_var.get_support()].tolist()\n",
        "#     if low_var_features:\n",
        "#         print(f\"  Removed {len(low_var_features)} low variance features\")\n",
        "#         X_train = X_train.drop(columns=low_var_features)\n",
        "\n",
        "#     # Stage 2: Select features using mutual information\n",
        "#     # Keep more features (95% of remaining or max 150 features)\n",
        "#     k = min(150, int(X_train.shape[1] * 0.95))\n",
        "#     selector_mi = SelectKBest(mutual_info_classif, k=k)\n",
        "#     selector_mi.fit(X_train, y_train)\n",
        "\n",
        "#     # Get selected feature names\n",
        "#     selected_features = X_train.columns[selector_mi.get_support()].tolist()\n",
        "#     print(f\"  Selected {len(selected_features)} features after two-stage selection\")\n",
        "\n",
        "#     # Keep only selected features\n",
        "#     features_to_keep = ['id', 'label', 'attack_cat'] + selected_features\n",
        "#     train_data = train_data[features_to_keep]\n",
        "#     test_data = test_data[features_to_keep]\n",
        "\n",
        "#     # Update numerical columns list to reflect selected features only\n",
        "#     numerical_cols = [col for col in selected_features if col in numerical_cols]\n",
        "\n",
        "#     # 11. IMPROVED: Apply scaling after outlier handling\n",
        "#     print(\"\\n🔍 Applying robust scaling to numerical features...\")\n",
        "#     scaler = RobustScaler()  # Still more robust than StandardScaler\n",
        "\n",
        "#     # If no numerical columns remain after selection, skip scaling\n",
        "#     if numerical_cols:\n",
        "#         train_data[numerical_cols] = scaler.fit_transform(train_data[numerical_cols])\n",
        "#         test_data[numerical_cols] = scaler.transform(test_data[numerical_cols])\n",
        "\n",
        "#     # 12. IMPROVED: Enhanced resampling with ADASYN and undersampling\n",
        "#     print(\"\\n🔍 Applying enhanced balanced resampling...\")\n",
        "#     # Prepare data for resampling\n",
        "#     X_train = train_data.drop(columns=['id', 'label', 'attack_cat'])\n",
        "#     y_train = train_data['attack_cat']\n",
        "\n",
        "#     # Print class distribution before resampling\n",
        "#     print(\"  Class distribution before resampling:\")\n",
        "#     for class_id, count in y_train.value_counts().sort_index().items():\n",
        "#         print(f\"    Class {class_id}: {count} samples ({100*count/len(y_train):.2f}%)\")\n",
        "\n",
        "#     # Calculate max samples per class - balanced but with limits\n",
        "#     max_samples = min(8000, int(len(y_train) * 0.15))  # Limit max samples per minority class\n",
        "#     sampling_strategy = {i: min(max_samples, count) for i, count in y_train.value_counts().items()}\n",
        "\n",
        "#     # For majority class, keep more samples but not too many\n",
        "#     majority_class = y_train.value_counts().idxmax()\n",
        "#     sampling_strategy[majority_class] = min(int(len(y_train) * 0.3), y_train.value_counts()[majority_class])\n",
        "\n",
        "#     print(\"  Target sample counts per class:\")\n",
        "#     for class_id, target_count in sorted(sampling_strategy.items()):\n",
        "#         print(f\"    Class {class_id}: {target_count} samples\")\n",
        "\n",
        "#     # Apply the resampling pipeline\n",
        "#     try:\n",
        "#         # First try ADASYN which requires multiple samples per class\n",
        "#         resampler = Pipeline([\n",
        "#             ('over', ADASYN(sampling_strategy='minority', random_state=42, n_neighbors=5)),\n",
        "#             ('under', RandomUnderSampler(sampling_strategy=sampling_strategy, random_state=42))\n",
        "#         ])\n",
        "#         X_train_resampled, y_train_resampled = resampler.fit_resample(X_train, y_train)\n",
        "#     except ValueError as e:\n",
        "#         print(f\"  ADASYN failed: {e}. Falling back to SMOTE.\")\n",
        "#         # Fall back to SMOTE with different neighbor settings\n",
        "#         from imblearn.over_sampling import SMOTE\n",
        "#         resampler = Pipeline([\n",
        "#             ('over', SMOTE(sampling_strategy='minority', random_state=42, k_neighbors=3)),\n",
        "#             ('under', RandomUnderSampler(sampling_strategy=sampling_strategy, random_state=42))\n",
        "#         ])\n",
        "#         X_train_resampled, y_train_resampled = resampler.fit_resample(X_train, y_train)\n",
        "\n",
        "#     # Print class distribution after resampling\n",
        "#     print(\"  Class distribution after resampling:\")\n",
        "#     for class_id, count in pd.Series(y_train_resampled).value_counts().sort_index().items():\n",
        "#         print(f\"    Class {class_id}: {count} samples ({100*count/len(y_train_resampled):.2f}%)\")\n",
        "\n",
        "#     # Create new DataFrame with resampled data\n",
        "#     train_data_resampled = pd.DataFrame(X_train_resampled, columns=X_train.columns)\n",
        "#     train_data_resampled['attack_cat'] = y_train_resampled\n",
        "#     train_data_resampled['id'] = range(len(train_data_resampled))\n",
        "#     train_data_resampled['label'] = train_data_resampled['attack_cat'] != 0  # Binary label (0=Normal)\n",
        "\n",
        "#     # 13. Check for any remaining issues\n",
        "#     print(\"\\n🔍 Final data quality check...\")\n",
        "#     # Check for infinities\n",
        "#     train_data_resampled = train_data_resampled.replace([np.inf, -np.inf], np.nan)\n",
        "#     test_data = test_data.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "#     # Check for NaN and fill if any\n",
        "#     if train_data_resampled.isnull().sum().sum() > 0:\n",
        "#         print(f\"  Found {train_data_resampled.isnull().sum().sum()} NaN values in training data. Filling with column medians.\")\n",
        "#         for col in train_data_resampled.columns:\n",
        "#             if train_data_resampled[col].isnull().sum() > 0:\n",
        "#                 if train_data_resampled[col].dtype in ['int64', 'float64']:\n",
        "#                     train_data_resampled[col] = train_data_resampled[col].fillna(train_data_resampled[col].median())\n",
        "\n",
        "#     if test_data.isnull().sum().sum() > 0:\n",
        "#         print(f\"  Found {test_data.isnull().sum().sum()} NaN values in test data. Filling with column medians.\")\n",
        "#         for col in test_data.columns:\n",
        "#             if test_data[col].isnull().sum() > 0:\n",
        "#                 if test_data[col].dtype in ['int64', 'float64']:\n",
        "#                     test_data[col] = test_data[col].fillna(test_data[col].median())\n",
        "\n",
        "#     # 14. IMPROVED: Create validation set from training data\n",
        "#     print(\"\\n🔍 Creating train/validation split...\")\n",
        "#     # Extract features and targets\n",
        "#     X_train_final = train_data_resampled.drop(columns=['id', 'label', 'attack_cat'])\n",
        "#     y_train_final = train_data_resampled['attack_cat']\n",
        "#     X_test_final = test_data.drop(columns=['id', 'label', 'attack_cat'])\n",
        "#     y_test_final = test_data['attack_cat']\n",
        "\n",
        "#     # Split into training and validation sets with stratification\n",
        "#     from sklearn.model_selection import train_test_split\n",
        "#     X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
        "#         X_train_final, y_train_final, test_size=0.2, random_state=42, stratify=y_train_final\n",
        "#     )\n",
        "\n",
        "#     print(f\"  Training data: {X_train_split.shape[0]} samples\")\n",
        "#     print(f\"  Validation data: {X_val_split.shape[0]} samples\")\n",
        "#     print(f\"  Test data: {X_test_final.shape[0]} samples\")\n",
        "\n",
        "#     # 15. Convert to tensors\n",
        "#     X_train_tensor = torch.tensor(X_train_split.values, dtype=torch.float32)\n",
        "#     y_train_tensor = torch.tensor(y_train_split.values, dtype=torch.long)\n",
        "\n",
        "#     X_val_tensor = torch.tensor(X_val_split.values, dtype=torch.float32)\n",
        "#     y_val_tensor = torch.tensor(y_val_split.values, dtype=torch.long)\n",
        "\n",
        "#     X_test_tensor = torch.tensor(X_test_final.values, dtype=torch.float32)\n",
        "#     y_test_tensor = torch.tensor(y_test_final.values, dtype=torch.long)\n",
        "\n",
        "#     # 16. Create datasets\n",
        "#     train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "#     val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "#     test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "#     # 17. Create data loaders with optimized parameters\n",
        "#     print(\"\\n🔍 Creating data loaders...\")\n",
        "#     batch_size = 64  # Can adjust based on available memory\n",
        "\n",
        "#     train_loader = DataLoader(\n",
        "#         train_dataset,\n",
        "#         batch_size=batch_size,\n",
        "#         shuffle=True,\n",
        "#         num_workers=2,\n",
        "#         pin_memory=True,\n",
        "#         drop_last=False\n",
        "#     )\n",
        "\n",
        "#     val_loader = DataLoader(\n",
        "#         val_dataset,\n",
        "#         batch_size=batch_size,\n",
        "#         shuffle=False,\n",
        "#         num_workers=2,\n",
        "#         pin_memory=True,\n",
        "#         drop_last=False\n",
        "#     )\n",
        "\n",
        "#     test_loader = DataLoader(\n",
        "#         test_dataset,\n",
        "#         batch_size=batch_size,\n",
        "#         shuffle=False,\n",
        "#         num_workers=2,\n",
        "#         pin_memory=True,\n",
        "#         drop_last=False\n",
        "#     )\n",
        "\n",
        "#     print(f\"\\n✅ Preprocessing complete!\")\n",
        "#     print(f\"  Final training set: {len(train_dataset)} samples with {X_train_split.shape[1]} features\")\n",
        "#     print(f\"  Final validation set: {len(val_dataset)} samples\")\n",
        "#     print(f\"  Final test set: {len(test_dataset)} samples with {X_test_final.shape[1]} features\")\n",
        "\n",
        "#     # 18. Visualize class distribution\n",
        "#     try:\n",
        "#         plt.figure(figsize=(15, 5))\n",
        "\n",
        "#         plt.subplot(1, 3, 1)\n",
        "#         train_class_counts = pd.Series(y_train_split).value_counts().sort_index()\n",
        "#         plt.bar(train_class_counts.index.astype(str), train_class_counts.values)\n",
        "#         plt.title('Training Data Class Distribution')\n",
        "#         plt.xlabel('Attack Category')\n",
        "#         plt.ylabel('Count')\n",
        "#         plt.xticks(rotation=45)\n",
        "\n",
        "#         plt.subplot(1, 3, 2)\n",
        "#         val_class_counts = pd.Series(y_val_split).value_counts().sort_index()\n",
        "#         plt.bar(val_class_counts.index.astype(str), val_class_counts.values)\n",
        "#         plt.title('Validation Data Class Distribution')\n",
        "#         plt.xlabel('Attack Category')\n",
        "#         plt.ylabel('Count')\n",
        "#         plt.xticks(rotation=45)\n",
        "\n",
        "#         plt.subplot(1, 3, 3)\n",
        "#         test_class_counts = pd.Series(y_test_final).value_counts().sort_index()\n",
        "#         plt.bar(test_class_counts.index.astype(str), test_class_counts.values)\n",
        "#         plt.title('Test Data Class Distribution')\n",
        "#         plt.xlabel('Attack Category')\n",
        "#         plt.ylabel('Count')\n",
        "#         plt.xticks(rotation=45)\n",
        "\n",
        "#         plt.tight_layout()\n",
        "#         plt.savefig('class_distribution.png')\n",
        "#         plt.close()\n",
        "#     except:\n",
        "#         print(\"  Could not create visualization (possibly running in non-graphical environment)\")\n",
        "\n",
        "#     # Calculate class weights for later use in loss function\n",
        "#     num_classes = len(np.unique(y_train_final))\n",
        "#     class_counts = np.bincount(y_train_split)\n",
        "\n",
        "#     # Handle potential missing classes in the bincount\n",
        "#     if len(class_counts) < num_classes:\n",
        "#         temp_counts = np.zeros(num_classes)\n",
        "#         temp_counts[:len(class_counts)] = class_counts\n",
        "#         class_counts = temp_counts\n",
        "\n",
        "#     # Calculate inverse frequency class weights, bounded to prevent extreme values\n",
        "#     class_weights = np.ones(num_classes)\n",
        "#     non_zero_counts = class_counts[class_counts > 0]\n",
        "#     if len(non_zero_counts) > 0:\n",
        "#         class_weights[class_counts > 0] = 1.0 / class_counts[class_counts > 0]\n",
        "#         # Normalize weights to sum to num_classes\n",
        "#         class_weights = class_weights * (num_classes / class_weights.sum())\n",
        "#         # Bound weights to reasonable range\n",
        "#         class_weights = np.clip(class_weights, 0.1, 10.0)\n",
        "\n",
        "#     class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32)\n",
        "\n",
        "#     # Return processed data and related objects\n",
        "#     return {\n",
        "#         'train_data': train_data_resampled,\n",
        "#         'test_data': test_data,\n",
        "#         'X_train': X_train_split,\n",
        "#         'y_train': y_train_split,\n",
        "#         'X_val': X_val_split,\n",
        "#         'y_val': y_val_split,\n",
        "#         'X_test': X_test_final,\n",
        "#         'y_test': y_test_final,\n",
        "#         'train_loader': train_loader,\n",
        "#         'val_loader': val_loader,\n",
        "#         'test_loader': test_loader,\n",
        "#         'input_dim': X_train_split.shape[1],\n",
        "#         'num_classes': num_classes,\n",
        "#         'class_weights': class_weights_tensor\n",
        "#     }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, RobustScaler\n",
        "from sklearn.feature_selection import mutual_info_classif, SelectKBest, VarianceThreshold\n",
        "from imblearn.over_sampling import ADASYN\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline\n",
        "from scipy.stats import zscore\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def enhanced_preprocessing(train_data, test_data):\n",
        "    \"\"\"\n",
        "    Enhanced preprocessing pipeline for IoT network anomaly detection.\n",
        "    Includes improved feature engineering, selection, and balancing.\n",
        "\n",
        "    Args:\n",
        "        train_data: Original training dataframe\n",
        "        test_data: Original testing dataframe\n",
        "\n",
        "    Returns:\n",
        "        processed_data: Dictionary with all processed datasets and loaders\n",
        "    \"\"\"\n",
        "    print(\"\\n🔍 Starting enhanced preprocessing pipeline...\")\n",
        "\n",
        "    # Make copies to avoid modifying originals\n",
        "    train_data = train_data.copy()\n",
        "    test_data = test_data.copy()\n",
        "\n",
        "    # 1. Data Exploration\n",
        "    print(\"\\n📊 Dataset Overview:\")\n",
        "    print(f\"Training data shape: {train_data.shape}\")\n",
        "    print(f\"Testing data shape: {test_data.shape}\")\n",
        "\n",
        "    # Check for missing values\n",
        "    train_missing = train_data.isnull().sum().sum()\n",
        "    test_missing = test_data.isnull().sum().sum()\n",
        "    print(f\"Missing values - Training: {train_missing}, Testing: {test_missing}\")\n",
        "\n",
        "    # 2. Drop rows with missing attack_cat (target variable)\n",
        "    print(\"\\n🔍 Handling missing target values...\")\n",
        "    train_data = train_data.dropna(subset=['attack_cat'])\n",
        "    test_data = test_data.dropna(subset=['attack_cat'])\n",
        "    print(f\"After dropping rows with missing targets - Train: {train_data.shape}, Test: {test_data.shape}\")\n",
        "\n",
        "    # 3. Identify numerical and categorical columns\n",
        "    numerical_cols = train_data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "    numerical_cols = [col for col in numerical_cols if col not in ['id', 'label', 'attack_cat']]\n",
        "    categorical_cols = train_data.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "    # 4. Handle remaining missing values\n",
        "    print(\"\\n🔍 Handling remaining missing values...\")\n",
        "    for col in numerical_cols:\n",
        "        # Use median for numerical features (more robust than mean)\n",
        "        train_data[col] = train_data[col].fillna(train_data[col].median())\n",
        "        test_data[col] = test_data[col].fillna(train_data[col].median())\n",
        "\n",
        "    for col in categorical_cols:\n",
        "        if col != 'attack_cat':\n",
        "            # Use mode (most frequent) for categorical features\n",
        "            train_data[col] = train_data[col].fillna(train_data[col].mode()[0])\n",
        "            test_data[col] = test_data[col].fillna(train_data[col].mode()[0])\n",
        "\n",
        "    # 5. Remove duplicate rows\n",
        "    print(\"\\n🔍 Removing duplicate entries...\")\n",
        "    train_data = train_data.drop_duplicates()\n",
        "    test_data = test_data.drop_duplicates()\n",
        "    print(f\"After removing duplicates - Train: {train_data.shape}, Test: {test_data.shape}\")\n",
        "\n",
        "    # 6. Encode attack_cat (target variable)\n",
        "    print(\"\\n🔍 Encoding target variable...\")\n",
        "    attack_mapping = {\n",
        "        'Normal': 0, 'Generic': 1, 'Exploits': 2, 'Fuzzers': 3, 'DoS': 4,\n",
        "        'Reconnaissance': 5, 'Analysis': 6, 'Backdoor': 7, 'Shellcode': 8, 'Worms': 9\n",
        "    }\n",
        "    train_data['attack_cat'] = train_data['attack_cat'].map(attack_mapping)\n",
        "    test_data['attack_cat'] = test_data['attack_cat'].map(attack_mapping)\n",
        "\n",
        "    # Check class distribution\n",
        "    class_counts = train_data['attack_cat'].value_counts().sort_index()\n",
        "    print(\"Class distribution after encoding:\")\n",
        "    for class_id, count in class_counts.items():\n",
        "        print(f\"  Class {class_id}: {count} samples ({100*count/len(train_data):.2f}%)\")\n",
        "\n",
        "    # 7. Handle outliers with improved approach\n",
        "    print(\"\\n🔍 Handling outliers...\")\n",
        "    # Save original indices before filtering\n",
        "    train_indices = train_data.index\n",
        "    test_indices = test_data.index\n",
        "\n",
        "    # Get only numerical features for z-score calculation\n",
        "    numerical_features = train_data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "    numerical_features = [col for col in numerical_features if col not in ['id', 'label', 'attack_cat']]\n",
        "\n",
        "    # Calculate z-scores based on training data\n",
        "    train_zscore_params = {}\n",
        "    for col in numerical_features:\n",
        "        mean_val = train_data[col].mean()\n",
        "        std_val = train_data[col].std()\n",
        "        if std_val == 0:  # Handle zero std\n",
        "            std_val = 1e-6\n",
        "        train_zscore_params[col] = (mean_val, std_val)\n",
        "\n",
        "    # Apply z-score filtering consistently using training parameters\n",
        "    threshold = 3\n",
        "    outlier_mask_train = pd.DataFrame(index=train_data.index)\n",
        "    outlier_mask_test = pd.DataFrame(index=test_data.index)\n",
        "\n",
        "    for col in numerical_features:\n",
        "        mean_val, std_val = train_zscore_params[col]\n",
        "        # Calculate z-scores using training parameters\n",
        "        outlier_mask_train[col] = np.abs((train_data[col] - mean_val) / std_val) > threshold\n",
        "        outlier_mask_test[col] = np.abs((test_data[col] - mean_val) / std_val) > threshold\n",
        "\n",
        "    # Filter rows with outliers in any numerical column\n",
        "    train_data = train_data[~outlier_mask_train.any(axis=1)]\n",
        "    test_data = test_data[~outlier_mask_test.any(axis=1)]\n",
        "\n",
        "    print(f\"After outlier removal - Train: {train_data.shape}, Test: {test_data.shape}\")\n",
        "\n",
        "    # 8. Apply advanced feature transformation for skewed features\n",
        "    print(\"\\n🔍 Applying transformations for skewed numerical features...\")\n",
        "    transform_params = {}  # Store parameters for consistent application\n",
        "\n",
        "    for col in numerical_features:\n",
        "        if col in train_data.columns:  # Check if column still exists after outlier removal\n",
        "            # Check if data is significantly skewed\n",
        "            skewness = train_data[col].skew()\n",
        "            if abs(skewness) > 1.5:  # Threshold for transformation\n",
        "                # Calculate parameters for log transformation\n",
        "                train_min = train_data[col].min()\n",
        "                offset = 1 - min(0, train_min)  # Ensure all values positive\n",
        "                transform_params[col] = offset\n",
        "\n",
        "                # Apply transformation consistently\n",
        "                train_data[col] = np.log1p(train_data[col] + offset)\n",
        "\n",
        "                # Apply same transformation to test data\n",
        "                if col in test_data.columns:\n",
        "                    test_data[col] = np.log1p(test_data[col] + offset)\n",
        "\n",
        "                print(f\"  Applied log transform to {col} (skewness: {skewness:.2f})\")\n",
        "\n",
        "    # 9. Encoding categorical features\n",
        "    print(\"\\n🔍 Encoding categorical features...\")\n",
        "    # Get existing categorical columns that remain\n",
        "    categorical_cols = [col for col in train_data.select_dtypes(include=['object']).columns\n",
        "                       if col != 'attack_cat' and col in train_data.columns]\n",
        "\n",
        "    if categorical_cols:\n",
        "        encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "\n",
        "        # Fit on training data only\n",
        "        encoded_train = encoder.fit_transform(train_data[categorical_cols])\n",
        "        encoded_feature_names = encoder.get_feature_names_out(categorical_cols)\n",
        "        encoded_train_df = pd.DataFrame(encoded_train,\n",
        "                                       columns=encoded_feature_names,\n",
        "                                       index=train_data.index)\n",
        "\n",
        "        # Apply to test data using fitted encoder\n",
        "        if all(col in test_data.columns for col in categorical_cols):\n",
        "            encoded_test = encoder.transform(test_data[categorical_cols])\n",
        "            encoded_test_df = pd.DataFrame(encoded_test,\n",
        "                                          columns=encoded_feature_names,\n",
        "                                          index=test_data.index)\n",
        "        else:\n",
        "            # Handle missing columns in test data\n",
        "            missing_cols = [col for col in categorical_cols if col not in test_data.columns]\n",
        "            print(f\"  Warning: Test data missing categorical columns: {missing_cols}\")\n",
        "            # Create empty dataframe with correct columns\n",
        "            encoded_test_df = pd.DataFrame(0,\n",
        "                                         index=test_data.index,\n",
        "                                         columns=encoded_feature_names)\n",
        "\n",
        "        # Drop original categorical columns and join encoded ones\n",
        "        train_data = train_data.drop(columns=categorical_cols)\n",
        "        test_data = test_data.drop(columns=[col for col in categorical_cols if col in test_data.columns])\n",
        "\n",
        "        # Join encoded features\n",
        "        train_data = pd.concat([train_data, encoded_train_df], axis=1)\n",
        "        test_data = pd.concat([test_data, encoded_test_df], axis=1)\n",
        "\n",
        "        print(f\"  Encoded {len(categorical_cols)} categorical features into {encoded_train_df.shape[1]} binary features\")\n",
        "\n",
        "    # 10. Feature selection using RandomForest - BEFORE scaling\n",
        "    print(\"\\n🔍 Performing feature selection with RandomForest...\")\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "    # Separate features and target\n",
        "    X_train_features = train_data.drop(columns=['id', 'label', 'attack_cat'], errors='ignore')\n",
        "    y_train_features = train_data['attack_cat']\n",
        "\n",
        "    # Fit RandomForest for feature importance\n",
        "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    rf_model.fit(X_train_features, y_train_features)\n",
        "\n",
        "    # Get feature importance\n",
        "    feature_importances = rf_model.feature_importances_\n",
        "    feature_names = X_train_features.columns\n",
        "\n",
        "    # Create DataFrame for visualization\n",
        "    importances_df = pd.DataFrame({\n",
        "        'feature': feature_names,\n",
        "        'importance': feature_importances\n",
        "    }).sort_values('importance', ascending=False)\n",
        "\n",
        "    # Select features based on importance threshold\n",
        "    threshold = 0.01\n",
        "    selected_features = importances_df[importances_df['importance'] > threshold]['feature'].tolist()\n",
        "\n",
        "    print(f\"  Selected {len(selected_features)} features after feature selection\")\n",
        "    if len(selected_features) == 0:\n",
        "        # If no features selected, take top 10\n",
        "        selected_features = importances_df.nlargest(10, 'importance')['feature'].tolist()\n",
        "        print(f\"  No features met threshold. Using top 10 features instead.\")\n",
        "\n",
        "    # Keep only selected features plus ID and target columns\n",
        "    keep_columns = selected_features + ['id', 'label', 'attack_cat']\n",
        "\n",
        "    # Filter columns that actually exist in both datasets\n",
        "    train_keep_cols = [col for col in keep_columns if col in train_data.columns]\n",
        "    test_keep_cols = [col for col in keep_columns if col in test_data.columns]\n",
        "\n",
        "    train_data = train_data[train_keep_cols]\n",
        "    test_data = test_data[test_keep_cols]\n",
        "\n",
        "    # Update numerical columns after feature selection - very important!\n",
        "    numerical_cols = [col for col in selected_features\n",
        "                     if col in train_data.columns\n",
        "                     and train_data[col].dtype in ['int64', 'float64']]\n",
        "\n",
        "    print(f\"  Remaining numerical columns after feature selection: {len(numerical_cols)}\")\n",
        "\n",
        "    # 11. Apply scaling to numerical features\n",
        "    print(\"\\n🔍 Applying scaling to numerical features...\")\n",
        "\n",
        "    # Verify numerical columns exist in both datasets\n",
        "    valid_num_cols = [col for col in numerical_cols if col in train_data.columns and col in test_data.columns]\n",
        "\n",
        "    if valid_num_cols:\n",
        "        # Use StandardScaler\n",
        "        scaler = StandardScaler()\n",
        "\n",
        "        # Fit on training data\n",
        "        train_data[valid_num_cols] = scaler.fit_transform(train_data[valid_num_cols])\n",
        "\n",
        "        # Apply same transformation to test data\n",
        "        test_data[valid_num_cols] = scaler.transform(test_data[valid_num_cols])\n",
        "        print(f\"  Scaled {len(valid_num_cols)} numerical features\")\n",
        "    else:\n",
        "        print(\"  No valid numerical columns remaining for scaling\")\n",
        "\n",
        "    # 12. Prepare for resampling\n",
        "    print(\"\\n🔍 Preparing for resampling...\")\n",
        "    # Extract features and target\n",
        "    X_train = train_data.drop(columns=['id', 'label', 'attack_cat'], errors='ignore')\n",
        "    y_train = train_data['attack_cat']\n",
        "\n",
        "    # Print class distribution before resampling\n",
        "    print(\"  Class distribution before resampling:\")\n",
        "    for class_id, count in y_train.value_counts().sort_index().items():\n",
        "        print(f\"    Class {class_id}: {count} samples ({100*count/len(y_train):.2f}%)\")\n",
        "\n",
        "    # 13. Enhanced resampling with error handling\n",
        "    print(\"\\n🔍 Applying resampling...\")\n",
        "    # Calculate target samples per class - balanced with limits\n",
        "    max_samples = min(8000, int(len(y_train) * 0.15))\n",
        "\n",
        "    # Create sampling strategy dictionary\n",
        "    sampling_strategy = {}\n",
        "    for class_id, count in y_train.value_counts().items():\n",
        "        if class_id == y_train.value_counts().idxmax():  # Majority class\n",
        "            sampling_strategy[class_id] = min(int(len(y_train) * 0.3), count)\n",
        "        else:  # Minority classes\n",
        "            sampling_strategy[class_id] = min(max_samples, count)\n",
        "\n",
        "    print(\"  Target sample counts per class:\")\n",
        "    for class_id, target_count in sorted(sampling_strategy.items()):\n",
        "        print(f\"    Class {class_id}: {target_count} samples\")\n",
        "\n",
        "    # Apply resampling with error handling\n",
        "    try:\n",
        "        # First try ADASYN which requires multiple samples per class\n",
        "        resampler = Pipeline([\n",
        "            ('over', ADASYN(sampling_strategy='minority', random_state=42, n_neighbors=5)),\n",
        "            ('under', RandomUnderSampler(sampling_strategy=sampling_strategy, random_state=42))\n",
        "        ])\n",
        "        X_train_resampled, y_train_resampled = resampler.fit_resample(X_train, y_train)\n",
        "    except ValueError as e:\n",
        "        print(f\"  ADASYN failed: {e}. Falling back to SMOTE.\")\n",
        "        # Fall back to SMOTE with different neighbor settings\n",
        "        from imblearn.over_sampling import SMOTE\n",
        "        try:\n",
        "            resampler = Pipeline([\n",
        "                ('over', SMOTE(sampling_strategy='minority', random_state=42, k_neighbors=3)),\n",
        "                ('under', RandomUnderSampler(sampling_strategy=sampling_strategy, random_state=42))\n",
        "            ])\n",
        "            X_train_resampled, y_train_resampled = resampler.fit_resample(X_train, y_train)\n",
        "        except ValueError as e2:\n",
        "            print(f\"  SMOTE failed: {e2}. Using RandomOverSampler instead.\")\n",
        "            # Fall back to RandomOverSampler which doesn't require nearest neighbors\n",
        "            from imblearn.over_sampling import RandomOverSampler\n",
        "            resampler = Pipeline([\n",
        "                ('over', RandomOverSampler(sampling_strategy='minority', random_state=42)),\n",
        "                ('under', RandomUnderSampler(sampling_strategy=sampling_strategy, random_state=42))\n",
        "            ])\n",
        "            X_train_resampled, y_train_resampled = resampler.fit_resample(X_train, y_train)\n",
        "\n",
        "    # Print class distribution after resampling\n",
        "    print(\"  Class distribution after resampling:\")\n",
        "    for class_id, count in pd.Series(y_train_resampled).value_counts().sort_index().items():\n",
        "        print(f\"    Class {class_id}: {count} samples ({100*count/len(y_train_resampled):.2f}%)\")\n",
        "\n",
        "    # Create new DataFrame with resampled data\n",
        "    train_data_resampled = pd.DataFrame(X_train_resampled, columns=X_train.columns)\n",
        "    train_data_resampled['attack_cat'] = y_train_resampled\n",
        "    train_data_resampled['id'] = range(len(train_data_resampled))\n",
        "    train_data_resampled['label'] = train_data_resampled['attack_cat'] != 0  # Binary label (0=Normal)\n",
        "\n",
        "    # 14. Final data quality check\n",
        "    print(\"\\n🔍 Final data quality check...\")\n",
        "    # Check for infinities\n",
        "    train_data_resampled = train_data_resampled.replace([np.inf, -np.inf], np.nan)\n",
        "    test_data = test_data.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "    # Check for NaN and fill if any\n",
        "    if train_data_resampled.isnull().sum().sum() > 0:\n",
        "        print(f\"  Found {train_data_resampled.isnull().sum().sum()} NaN values in training data. Filling with column medians.\")\n",
        "        for col in train_data_resampled.columns:\n",
        "            if train_data_resampled[col].isnull().sum() > 0:\n",
        "                if train_data_resampled[col].dtype in ['int64', 'float64']:\n",
        "                    train_data_resampled[col] = train_data_resampled[col].fillna(train_data_resampled[col].median())\n",
        "\n",
        "    if test_data.isnull().sum().sum() > 0:\n",
        "        print(f\"  Found {test_data.isnull().sum().sum()} NaN values in test data. Filling with column medians.\")\n",
        "        for col in test_data.columns:\n",
        "            if test_data[col].isnull().sum() > 0:\n",
        "                if test_data[col].dtype in ['int64', 'float64']:\n",
        "                    test_data[col] = test_data[col].fillna(test_data[col].median())\n",
        "\n",
        "    # 15. Create validation set from training data\n",
        "    print(\"\\n🔍 Creating train/validation split...\")\n",
        "    # Extract features and targets\n",
        "    X_train_final = train_data_resampled.drop(columns=['id', 'label', 'attack_cat'], errors='ignore')\n",
        "    y_train_final = train_data_resampled['attack_cat']\n",
        "    X_test_final = test_data.drop(columns=['id', 'label', 'attack_cat'], errors='ignore')\n",
        "    y_test_final = test_data['attack_cat']\n",
        "\n",
        "    # Split into training and validation sets with stratification\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
        "        X_train_final, y_train_final, test_size=0.2, random_state=42, stratify=y_train_final\n",
        "    )\n",
        "\n",
        "    print(f\"  Training data: {X_train_split.shape[0]} samples\")\n",
        "    print(f\"  Validation data: {X_val_split.shape[0]} samples\")\n",
        "    print(f\"  Test data: {X_test_final.shape[0]} samples\")\n",
        "\n",
        "    # 16. Convert to tensors with error handling\n",
        "    print(\"\\n🔍 Converting to PyTorch tensors...\")\n",
        "    try:\n",
        "        X_train_tensor = torch.tensor(X_train_split.values, dtype=torch.float32)\n",
        "        y_train_tensor = torch.tensor(y_train_split.values, dtype=torch.long)\n",
        "\n",
        "        X_val_tensor = torch.tensor(X_val_split.values, dtype=torch.float32)\n",
        "        y_val_tensor = torch.tensor(y_val_split.values, dtype=torch.long)\n",
        "\n",
        "        X_test_tensor = torch.tensor(X_test_final.values, dtype=torch.float32)\n",
        "        y_test_tensor = torch.tensor(y_test_final.values, dtype=torch.long)\n",
        "    except Exception as e:\n",
        "        print(f\"  Error converting to tensors: {e}\")\n",
        "        print(\"  Attempting to fix tensor conversion issues...\")\n",
        "\n",
        "        # Try to identify and fix numeric issues\n",
        "        for df in [X_train_split, X_val_split, X_test_final]:\n",
        "            for col in df.columns:\n",
        "                if df[col].dtype not in ['int64', 'float64']:\n",
        "                    print(f\"  Converting non-numeric column {col} to float\")\n",
        "                    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "        # Try conversion again\n",
        "        X_train_tensor = torch.tensor(X_train_split.values, dtype=torch.float32)\n",
        "        y_train_tensor = torch.tensor(y_train_split.values, dtype=torch.long)\n",
        "        X_val_tensor = torch.tensor(X_val_split.values, dtype=torch.float32)\n",
        "        y_val_tensor = torch.tensor(y_val_split.values, dtype=torch.long)\n",
        "        X_test_tensor = torch.tensor(X_test_final.values, dtype=torch.float32)\n",
        "        y_test_tensor = torch.tensor(y_test_final.values, dtype=torch.long)\n",
        "\n",
        "    # 17. Create datasets\n",
        "    print(\"\\n🔍 Creating PyTorch datasets...\")\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "    # 18. Create data loaders with optimized parameters\n",
        "    print(\"\\n🔍 Creating data loaders...\")\n",
        "    batch_size = 64  # Adjust based on available memory\n",
        "\n",
        "    # Determine optimal num_workers based on system\n",
        "    import os\n",
        "    try:\n",
        "        num_workers = min(4, os.cpu_count() or 1)\n",
        "    except:\n",
        "        num_workers = 2\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,\n",
        "        drop_last=False\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,\n",
        "        drop_last=False\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,\n",
        "        drop_last=False\n",
        "    )\n",
        "\n",
        "    print(f\"\\n✅ Preprocessing complete!\")\n",
        "    print(f\"  Final training set: {len(train_dataset)} samples with {X_train_split.shape[1]} features\")\n",
        "    print(f\"  Final validation set: {len(val_dataset)} samples\")\n",
        "    print(f\"  Final test set: {len(test_dataset)} samples with {X_test_final.shape[1]} features\")\n",
        "\n",
        "    # 19. Calculate class weights for potential use in loss function\n",
        "    num_classes = len(np.unique(y_train_final))\n",
        "    class_counts = np.bincount(y_train_split)\n",
        "\n",
        "    # Handle potential missing classes in the bincount\n",
        "    if len(class_counts) < num_classes:\n",
        "        temp_counts = np.zeros(num_classes)\n",
        "        temp_counts[:len(class_counts)] = class_counts\n",
        "        class_counts = temp_counts\n",
        "\n",
        "    # Calculate inverse frequency class weights, bounded to prevent extreme values\n",
        "    class_weights = np.ones(num_classes)\n",
        "    non_zero_counts = class_counts[class_counts > 0]\n",
        "    if len(non_zero_counts) > 0:\n",
        "        class_weights[class_counts > 0] = 1.0 / class_counts[class_counts > 0]\n",
        "        # Normalize weights to sum to num_classes\n",
        "        class_weights = class_weights * (num_classes / class_weights.sum())\n",
        "        # Bound weights to reasonable range\n",
        "        class_weights = np.clip(class_weights, 0.1, 10.0)\n",
        "\n",
        "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32)\n",
        "\n",
        "    # Return processed data and related objects\n",
        "    return {\n",
        "        'train_data': train_data_resampled,\n",
        "        'test_data': test_data,\n",
        "        'X_train': X_train_split,\n",
        "        'y_train': y_train_split,\n",
        "        'X_val': X_val_split,\n",
        "        'y_val': y_val_split,\n",
        "        'X_test': X_test_final,\n",
        "        'y_test': y_test_final,\n",
        "        'train_loader': train_loader,\n",
        "        'val_loader': val_loader,\n",
        "        'test_loader': test_loader,\n",
        "        'input_dim': X_train_split.shape[1],\n",
        "        'num_classes': num_classes,\n",
        "        'class_weights': class_weights_tensor\n",
        "    }"
      ],
      "metadata": {
        "id": "IO1imYY0NmLh"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bagian 2: Model CNN + DBN Ensemble"
      ],
      "metadata": {
        "id": "2a1DL2D79R8r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1"
      ],
      "metadata": {
        "id": "Q0EeM5iSUJhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# import torch.nn.functional as F\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# from torch.utils.data import DataLoader, TensorDataset\n",
        "# from sklearn.metrics import confusion_matrix, classification_report\n",
        "# import time\n",
        "# import os\n",
        "# import seaborn as sns\n",
        "# from tqdm import tqdm\n",
        "\n",
        "# # Improved 2D CNN for tabular data\n",
        "# class RevisedCNN(nn.Module):\n",
        "#     # def __init__(self, input_size, num_classes):\n",
        "#     #     super(RevisedCNN, self).__init__()\n",
        "\n",
        "#     #     # Convolutional Layers\n",
        "#     #     self.conv1 = nn.Conv1d(in_channels=1, out_channels=128, kernel_size=3, padding=1)\n",
        "#     #     self.bn1 = nn.BatchNorm1d(128)\n",
        "#     #     self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "#     #     self.conv2 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
        "#     #     self.bn2 = nn.BatchNorm1d(256)\n",
        "#     #     self.pool2 = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "#     #     self.conv3 = nn.Conv1d(in_channels=256, out_channels=512, kernel_size=3, padding=1)\n",
        "#     #     self.bn3 = nn.BatchNorm1d(512)\n",
        "#     #     self.pool3 = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "#     #     # LSTM Layer\n",
        "#     #     self.lstm = nn.LSTM(input_size=512, hidden_size=256, num_layers=2, batch_first=True, dropout=0.3)\n",
        "\n",
        "#     #     # Fully Connected Layers\n",
        "#     #     self.fc1 = nn.Linear(256, 128)\n",
        "#     #     self.dropout1 = nn.Dropout(0.5)\n",
        "#     #     self.fc2 = nn.Linear(128, 64)\n",
        "#     #     self.dropout2 = nn.Dropout(0.5)\n",
        "#     #     self.fc3 = nn.Linear(64, num_classes)\n",
        "\n",
        "#     # def forward(self, x):\n",
        "#     #     # Add channel dimension (batch_size, 1, input_size)\n",
        "#     #     x = x.unsqueeze(1)\n",
        "\n",
        "#     #     # Convolutional Layers\n",
        "#     #     x = self.pool1(nn.functional.relu(self.bn1(self.conv1(x))))\n",
        "#     #     x = self.pool2(nn.functional.relu(self.bn2(self.conv2(x))))\n",
        "#     #     x = self.pool3(nn.functional.relu(self.bn3(self.conv3(x))))\n",
        "\n",
        "#     #     # Prepare for LSTM (batch_size, seq_len, features)\n",
        "#     #     x = x.permute(0, 2, 1)\n",
        "\n",
        "#     #     # LSTM Layer\n",
        "#     #     x, _ = self.lstm(x)\n",
        "\n",
        "#     #     # Use the last LSTM output\n",
        "#     #     x = x[:, -1, :]\n",
        "\n",
        "#     #     # Fully Connected Layers\n",
        "#     #     x = self.dropout1(nn.functional.relu(self.fc1(x)))\n",
        "#     #     x = self.dropout2(nn.functional.relu(self.fc2(x)))\n",
        "#     #     x = self.fc3(x)\n",
        "\n",
        "#     #     return x\n",
        "#     def __init__(self, input_dim, output_dim, dropout_rate=0.3):\n",
        "#         super(RevisedCNN, self).__init__()\n",
        "\n",
        "#         # Reshape input to 2D representation\n",
        "#         self.input_dim = input_dim\n",
        "#         self.reshape_dim = int(np.sqrt(input_dim)) + 1\n",
        "#         self.pad_size = self.reshape_dim**2 - input_dim\n",
        "\n",
        "#         # Convolutional layers with larger kernel and better stride\n",
        "#         self.conv1 = nn.Conv2d(1, 64, kernel_size=5, stride=1, padding=2)\n",
        "#         self.bn1 = nn.BatchNorm2d(64)\n",
        "#         self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "#         # Compute output size after first conv+pool\n",
        "#         conv1_out_size = self.reshape_dim // 2\n",
        "\n",
        "#         self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "#         self.bn2 = nn.BatchNorm2d(128)\n",
        "#         self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "#         # Compute output size after second conv+pool\n",
        "#         conv2_out_size = conv1_out_size // 2\n",
        "\n",
        "#         # Calculate flattened size\n",
        "#         self.flattened_size = 128 * max(1, conv2_out_size) * max(1, conv2_out_size)\n",
        "\n",
        "#         # Fully connected layers with proper sizes\n",
        "#         self.fc1 = nn.Linear(self.flattened_size, 512)\n",
        "#         self.bn_fc1 = nn.BatchNorm1d(512)\n",
        "#         self.dropout_fc1 = nn.Dropout(dropout_rate)\n",
        "\n",
        "#         self.fc2 = nn.Linear(512, 256)\n",
        "#         self.bn_fc2 = nn.BatchNorm1d(256)\n",
        "#         self.dropout_fc2 = nn.Dropout(dropout_rate)\n",
        "\n",
        "#         self.fc3 = nn.Linear(256, output_dim)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         batch_size = x.size(0)\n",
        "\n",
        "#         # Reshape input to 2D (square-like format for better convolution)\n",
        "#         x_padded = F.pad(x, (0, self.pad_size))\n",
        "#         x = x_padded.view(batch_size, 1, self.reshape_dim, self.reshape_dim)\n",
        "\n",
        "#         # Apply convolutional layers\n",
        "#         x = self.conv1(x)\n",
        "#         x = self.bn1(x)\n",
        "#         x = F.leaky_relu(x, negative_slope=0.1)\n",
        "#         x = self.pool1(x)\n",
        "\n",
        "#         x = self.conv2(x)\n",
        "#         x = self.bn2(x)\n",
        "#         x = F.leaky_relu(x, negative_slope=0.1)\n",
        "#         x = self.pool2(x)\n",
        "\n",
        "#         # Flatten\n",
        "#         x = x.view(batch_size, -1)\n",
        "\n",
        "#         # Fully connected layers\n",
        "#         x = self.fc1(x)\n",
        "#         x = self.bn_fc1(x)\n",
        "#         x = F.leaky_relu(x, negative_slope=0.1)\n",
        "#         x = self.dropout_fc1(x)\n",
        "\n",
        "#         x = self.fc2(x)\n",
        "#         x = self.bn_fc2(x)\n",
        "#         x = F.leaky_relu(x, negative_slope=0.1)\n",
        "#         x = self.dropout_fc2(x)\n",
        "\n",
        "#         x = self.fc3(x)\n",
        "\n",
        "#         return x\n",
        "\n",
        "\n",
        "# # Improved RBM with proper pretraining abilities\n",
        "# class ImprovedRBM(nn.Module):\n",
        "#     def __init__(self, input_size, hidden_size):\n",
        "#         super(ImprovedRBM, self).__init__()\n",
        "\n",
        "#         # Initialize weights with small values for stability\n",
        "#         self.W = nn.Parameter(torch.randn(input_size, hidden_size) * 0.01)\n",
        "#         self.b = nn.Parameter(torch.zeros(hidden_size))\n",
        "#         self.c = nn.Parameter(torch.zeros(input_size))\n",
        "#         self.input_size = input_size\n",
        "#         self.hidden_size = hidden_size\n",
        "\n",
        "#         # Xavier/Glorot initialization for better convergence\n",
        "#         nn.init.xavier_uniform_(self.W)\n",
        "\n",
        "#     def forward(self, v):\n",
        "#         \"\"\"Visible to hidden layer probabilities\"\"\"\n",
        "#         batch_size = v.size(0)\n",
        "\n",
        "#         # Reshape input to 2D if needed\n",
        "#         if v.dim() > 2:\n",
        "#             v = v.view(batch_size, -1)\n",
        "\n",
        "#         # Handle dimension mismatch gracefully\n",
        "#         if v.size(1) != self.input_size:\n",
        "#             v_resized = torch.zeros(batch_size, self.input_size, device=v.device)\n",
        "#             min_size = min(v.size(1), self.input_size)\n",
        "#             v_resized[:, :min_size] = v[:, :min_size]\n",
        "#             v = v_resized\n",
        "\n",
        "#         # Apply L2 normalization for better stability\n",
        "#         v = F.normalize(v, p=2, dim=1)\n",
        "\n",
        "#         # Compute hidden activations\n",
        "#         h_activation = torch.matmul(v, self.W) + self.b\n",
        "#         h_probs = torch.sigmoid(h_activation)\n",
        "\n",
        "#         return h_probs\n",
        "\n",
        "#     def sample_h(self, v):\n",
        "#         \"\"\"Sample from the hidden layer given visible state\"\"\"\n",
        "#         h_probs = self.forward(v)\n",
        "#         h_samples = torch.bernoulli(h_probs)\n",
        "#         return h_samples, h_probs\n",
        "\n",
        "#     def sample_v(self, h):\n",
        "#         \"\"\"Sample from the visible layer given hidden state\"\"\"\n",
        "#         v_activation = torch.matmul(h, self.W.t()) + self.c\n",
        "#         v_probs = torch.sigmoid(v_activation)\n",
        "#         v_samples = torch.bernoulli(v_probs)\n",
        "#         return v_samples, v_probs\n",
        "\n",
        "#     def cd_k(self, v_data, k=1):\n",
        "#         \"\"\"Contrastive Divergence with k steps\"\"\"\n",
        "#         h_data, h_data_probs = self.sample_h(v_data)\n",
        "\n",
        "#         # Initialize the chain with data samples\n",
        "#         h_model = h_data\n",
        "\n",
        "#         # Gibbs sampling\n",
        "#         for _ in range(k):\n",
        "#             v_model, v_model_probs = self.sample_v(h_model)\n",
        "#             h_model, h_model_probs = self.sample_h(v_model)\n",
        "\n",
        "#         return v_data, h_data_probs, v_model_probs, h_model_probs\n",
        "\n",
        "#     def free_energy(self, v):\n",
        "#         \"\"\"Calculate free energy\"\"\"\n",
        "#         wx_b = torch.matmul(v, self.W) + self.b\n",
        "#         hidden_term = torch.sum(F.softplus(wx_b), dim=1)\n",
        "#         vbias_term = torch.matmul(v, self.c)\n",
        "#         return -hidden_term - vbias_term\n",
        "\n",
        "\n",
        "# # Function to pretrain an RBM\n",
        "# def pretrain_rbm(rbm, dataloader, device, epochs=5, lr=0.001):\n",
        "#     \"\"\"Pretrain RBM using Contrastive Divergence\"\"\"\n",
        "#     optimizer = torch.optim.Adam(rbm.parameters(), lr=lr)\n",
        "#     rbm.train()\n",
        "\n",
        "#     print(f\"Pretraining RBM {rbm.input_size} -> {rbm.hidden_size}...\")\n",
        "\n",
        "#     for epoch in range(epochs):\n",
        "#         mean_loss = 0\n",
        "#         num_batches = 0\n",
        "\n",
        "#         for batch_idx, (data, _) in enumerate(dataloader):\n",
        "#             data = data.to(device)\n",
        "\n",
        "#             # Run k-step Contrastive Divergence\n",
        "#             v_data, h_data, v_model, h_model = rbm.cd_k(data, k=1)\n",
        "\n",
        "#             # Compute gradients using CD loss\n",
        "#             # Positive phase - negative phase\n",
        "#             pos_associations = torch.matmul(v_data.t(), h_data)\n",
        "#             neg_associations = torch.matmul(v_model.t(), h_model)\n",
        "\n",
        "#             # Update weights and biases\n",
        "#             optimizer.zero_grad()\n",
        "\n",
        "#             # Manually set gradients\n",
        "#             rbm.W.grad = -(pos_associations - neg_associations) / data.size(0)\n",
        "#             rbm.b.grad = -(h_data - h_model).mean(0)\n",
        "#             rbm.c.grad = -(v_data - v_model).mean(0)\n",
        "\n",
        "#             optimizer.step()\n",
        "\n",
        "#             # Compute reconstruction error\n",
        "#             recon_error = F.mse_loss(v_model, v_data)\n",
        "#             mean_loss += recon_error.item()\n",
        "#             num_batches += 1\n",
        "\n",
        "#         # Print epoch stats\n",
        "#         mean_loss /= num_batches\n",
        "#         print(f\"  Epoch {epoch+1}/{epochs}, Reconstruction Error: {mean_loss:.6f}\")\n",
        "\n",
        "#     return rbm\n",
        "\n",
        "\n",
        "# # Improved DBN with proper pretraining\n",
        "# class ImprovedDBN(nn.Module):\n",
        "#     def __init__(self, input_dim, hidden_dims, output_dim, dropout_rate=0.3):\n",
        "#         super(ImprovedDBN, self).__init__()\n",
        "#         self.input_dim = input_dim\n",
        "#         self.hidden_dims = hidden_dims\n",
        "#         self.output_dim = output_dim\n",
        "\n",
        "#         # Stack of RBM layers\n",
        "#         self.rbm_layers = nn.ModuleList()\n",
        "\n",
        "#         # First RBM\n",
        "#         self.rbm_layers.append(ImprovedRBM(input_dim, hidden_dims[0]))\n",
        "\n",
        "#         # Additional RBM layers\n",
        "#         for i in range(1, len(hidden_dims)):\n",
        "#             self.rbm_layers.append(ImprovedRBM(hidden_dims[i-1], hidden_dims[i]))\n",
        "\n",
        "#         # Fully connected layers after RBMs\n",
        "#         self.fc_layers = nn.ModuleList()\n",
        "\n",
        "#         # First FC layer from last RBM's output\n",
        "#         self.fc_layers.append(nn.Sequential(\n",
        "#             nn.Linear(hidden_dims[-1], 256),\n",
        "#             nn.BatchNorm1d(256),\n",
        "#             nn.LeakyReLU(0.1),\n",
        "#             nn.Dropout(dropout_rate)\n",
        "#         ))\n",
        "\n",
        "#         # Second FC layer\n",
        "#         self.fc_layers.append(nn.Sequential(\n",
        "#             nn.Linear(256, 128),\n",
        "#             nn.BatchNorm1d(128),\n",
        "#             nn.LeakyReLU(0.1),\n",
        "#             nn.Dropout(dropout_rate)\n",
        "#         ))\n",
        "\n",
        "#         # Output layer\n",
        "#         self.output_layer = nn.Linear(128, output_dim)\n",
        "\n",
        "#         # Better initialization for linear layers\n",
        "#         for m in self.modules():\n",
        "#             if isinstance(m, nn.Linear):\n",
        "#                 nn.init.xavier_uniform_(m.weight)\n",
        "#                 nn.init.constant_(m.bias, 0)\n",
        "\n",
        "#     def pretrain(self, dataloader, device, epochs=5):\n",
        "#         \"\"\"Pretrain the DBN layer by layer\"\"\"\n",
        "#         print(\"Pretraining DBN layers...\")\n",
        "\n",
        "#         # Train first RBM with input data\n",
        "#         self.rbm_layers[0] = pretrain_rbm(\n",
        "#             self.rbm_layers[0], dataloader, device, epochs)\n",
        "\n",
        "#         # For each subsequent layer, train with features from previous layer\n",
        "#         for i in range(1, len(self.rbm_layers)):\n",
        "#             # Extract features from previous layer\n",
        "#             prev_layer_features = []\n",
        "#             with torch.no_grad():\n",
        "#                 for data, _ in dataloader:\n",
        "#                     data = data.to(device)\n",
        "\n",
        "#                     # Forward pass through previous layers\n",
        "#                     for j in range(i):\n",
        "#                         data = self.rbm_layers[j](data)\n",
        "\n",
        "#                     prev_layer_features.append(data.cpu())\n",
        "\n",
        "#             # Create dataset with extracted features\n",
        "#             prev_features = torch.cat(prev_layer_features, dim=0)\n",
        "#             feature_dataset = TensorDataset(prev_features, torch.zeros(prev_features.size(0)))\n",
        "#             feature_loader = DataLoader(\n",
        "#                 feature_dataset,\n",
        "#                 batch_size=dataloader.batch_size,\n",
        "#                 shuffle=True,\n",
        "#                 num_workers=0\n",
        "#             )\n",
        "\n",
        "#             # Pretrain this RBM with extracted features\n",
        "#             self.rbm_layers[i] = pretrain_rbm(\n",
        "#                 self.rbm_layers[i], feature_loader, device, epochs)\n",
        "\n",
        "#         print(\"DBN pretraining complete.\")\n",
        "#         return self\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         \"\"\"Forward pass through the DBN\"\"\"\n",
        "#         batch_size = x.size(0)\n",
        "\n",
        "#         # Flatten input if not already flat\n",
        "#         if x.dim() > 2:\n",
        "#             x = x.view(batch_size, -1)\n",
        "\n",
        "#         # Forward through RBM layers\n",
        "#         for rbm in self.rbm_layers:\n",
        "#             x = rbm(x)\n",
        "\n",
        "#         # Forward through FC layers\n",
        "#         for fc in self.fc_layers:\n",
        "#             x = fc(x)\n",
        "\n",
        "#         # Output layer\n",
        "#         x = self.output_layer(x)\n",
        "\n",
        "#         return x\n",
        "\n",
        "\n",
        "# # Improved dynamic ensemble model\n",
        "# class ImprovedDynamicEnsembleModel(nn.Module):\n",
        "#     def __init__(self, cnn_model, dbn_model, output_dim):\n",
        "#         super(ImprovedDynamicEnsembleModel, self).__init__()\n",
        "#         self.cnn_model = cnn_model\n",
        "#         self.dbn_model = dbn_model\n",
        "#         self.output_dim = output_dim\n",
        "\n",
        "#         # Class-wise attention - one attention weight per class\n",
        "#         self.class_attention = nn.ModuleList([\n",
        "#             nn.Sequential(\n",
        "#                 nn.Linear(2, 16),\n",
        "#                 nn.LeakyReLU(0.1),\n",
        "#                 nn.Linear(16, 2),\n",
        "#                 nn.Softmax(dim=1)\n",
        "#             ) for _ in range(output_dim)\n",
        "#         ])\n",
        "\n",
        "#         # Gating network to decide contribution from each model\n",
        "#         self.gate_network = nn.Sequential(\n",
        "#             nn.Linear(output_dim * 2, 64),\n",
        "#             nn.LeakyReLU(0.1),\n",
        "#             nn.Linear(64, 1),\n",
        "#             nn.Sigmoid()\n",
        "#         )\n",
        "\n",
        "#         # Combiner network\n",
        "#         self.combiner = nn.Sequential(\n",
        "#             nn.Linear(output_dim * 2, 256),\n",
        "#             nn.BatchNorm1d(256),\n",
        "#             nn.LeakyReLU(0.1),\n",
        "#             nn.Dropout(0.3),\n",
        "#             nn.Linear(256, output_dim)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # Save original input for DBN\n",
        "#         x_original = x.clone()\n",
        "\n",
        "#         # Forward pass through CNN\n",
        "#         cnn_logits = self.cnn_model(x_original)\n",
        "\n",
        "#         # Forward pass through DBN\n",
        "#         dbn_logits = self.dbn_model(x_original)\n",
        "\n",
        "#         # Get probabilities\n",
        "#         cnn_probs = F.softmax(cnn_logits, dim=1)\n",
        "#         dbn_probs = F.softmax(dbn_logits, dim=1)\n",
        "\n",
        "#         # Confidence values\n",
        "#         cnn_conf = torch.max(cnn_probs, dim=1, keepdim=True)[0]\n",
        "#         dbn_conf = torch.max(dbn_probs, dim=1, keepdim=True)[0]\n",
        "\n",
        "#         # Calculate gating value to balance models\n",
        "#         model_confs = torch.cat([cnn_logits, dbn_logits], dim=1)\n",
        "#         gate_value = self.gate_network(model_confs)\n",
        "\n",
        "#         # Class-wise weighting\n",
        "#         weighted_outputs = []\n",
        "#         for i in range(self.output_dim):\n",
        "#             # Get logits for this class\n",
        "#             class_values = torch.cat([\n",
        "#                 cnn_logits[:, i:i+1],\n",
        "#                 dbn_logits[:, i:i+1]\n",
        "#             ], dim=1)\n",
        "\n",
        "#             # Get weights for this class\n",
        "#             weights = self.class_attention[i](class_values)\n",
        "\n",
        "#             # Weight the outputs\n",
        "#             weighted_class = (weights[:, 0:1] * cnn_logits[:, i:i+1] +\n",
        "#                              weights[:, 1:2] * dbn_logits[:, i:i+1])\n",
        "#             weighted_outputs.append(weighted_class)\n",
        "\n",
        "#         # Stack all class outputs\n",
        "#         class_weighted_output = torch.cat(weighted_outputs, dim=1)\n",
        "\n",
        "#         # Combine with original outputs\n",
        "#         combined = torch.cat([cnn_logits, dbn_logits], dim=1)\n",
        "\n",
        "#         # Dynamic final output\n",
        "#         final_output = gate_value * self.combiner(combined) + (1 - gate_value) * class_weighted_output\n",
        "\n",
        "#         return final_output\n",
        "\n",
        "\n",
        "# # Focal Loss implementation for handling class imbalance\n",
        "# class FocalLoss(nn.Module):\n",
        "#     def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
        "#         super(FocalLoss, self).__init__()\n",
        "#         self.alpha = alpha  # Class weights\n",
        "#         self.gamma = gamma  # Focusing parameter\n",
        "#         self.reduction = reduction\n",
        "\n",
        "#     def forward(self, inputs, targets):\n",
        "#         # Standard cross entropy\n",
        "#         ce_loss = F.cross_entropy(\n",
        "#             inputs, targets, weight=self.alpha,\n",
        "#             reduction='none'\n",
        "#         )\n",
        "\n",
        "#         # Get probabilities\n",
        "#         pt = torch.exp(-ce_loss)\n",
        "\n",
        "#         # Calculate focal term\n",
        "#         focal_term = (1 - pt) ** self.gamma\n",
        "\n",
        "#         # Apply focal term to CE loss\n",
        "#         loss = focal_term * ce_loss\n",
        "\n",
        "#         # Apply reduction\n",
        "#         if self.reduction == 'mean':\n",
        "#             return loss.mean()\n",
        "#         elif self.reduction == 'sum':\n",
        "#             return loss.sum()\n",
        "#         else:\n",
        "#             return loss\n",
        "\n",
        "\n",
        "# # Mixup data augmentation function\n",
        "# def mixup_data(x, y, alpha=0.2, device='cuda'):\n",
        "#     \"\"\"Mixup data augmentation function\"\"\"\n",
        "#     if alpha > 0:\n",
        "#         lam = np.random.beta(alpha, alpha)\n",
        "#     else:\n",
        "#         lam = 1\n",
        "\n",
        "#     batch_size = x.size(0)\n",
        "#     index = torch.randperm(batch_size).to(device)\n",
        "\n",
        "#     mixed_x = lam * x + (1 - lam) * x[index]\n",
        "#     y_a, y_b = y, y[index]\n",
        "\n",
        "#     return mixed_x, y_a, y_b, lam\n",
        "\n",
        "\n",
        "# # Mixup criterion function\n",
        "# def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "#     \"\"\"Criterion for mixup data augmentation\"\"\"\n",
        "#     return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "\n",
        "# # Training function with advanced techniques\n",
        "# def train_ensemble_model(ensemble_model, train_loader, val_loader, test_loader,\n",
        "#                         device, class_weights=None, epochs=30, lr=0.001,\n",
        "#                         focal_gamma=2.0, mixup_alpha=0.2, save_dir='.'):\n",
        "#     print(\"Starting advanced training process...\")\n",
        "\n",
        "#     # Create save directory if it doesn't exist\n",
        "#     os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "#     # Setup optimizer with weight decay\n",
        "#     optimizer = optim.AdamW(ensemble_model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "\n",
        "#     # Learning rate scheduler\n",
        "#     scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "#         optimizer, mode='min', factor=0.5, patience=3, verbose=True, min_lr=1e-6\n",
        "#     )\n",
        "\n",
        "#     # Loss function - use focal loss for imbalanced classes\n",
        "#     if class_weights is not None:\n",
        "#         class_weights = class_weights.to(device)\n",
        "\n",
        "#     criterion = FocalLoss(alpha=class_weights, gamma=focal_gamma)\n",
        "\n",
        "#     # Tracking metrics\n",
        "#     train_losses = []\n",
        "#     val_losses = []\n",
        "#     train_accs = []\n",
        "#     val_accs = []\n",
        "#     best_val_acc = 0.0\n",
        "#     best_val_loss = float('inf')\n",
        "#     patience_counter = 0\n",
        "\n",
        "#     # Training loop with early stopping\n",
        "#     for epoch in range(epochs):\n",
        "#         # Training phase\n",
        "#         start_time = time.time()\n",
        "#         ensemble_model.train()\n",
        "#         running_loss = 0.0\n",
        "#         correct = 0\n",
        "#         total = 0\n",
        "\n",
        "#         # Progress bar for training\n",
        "#         train_iterator = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n",
        "\n",
        "#         for inputs, labels in train_iterator:\n",
        "#             inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "#             # Apply mixup augmentation randomly with 50% probability\n",
        "#             use_mixup = np.random.random() < 0.5 and mixup_alpha > 0\n",
        "\n",
        "#             if use_mixup:\n",
        "#                 inputs, labels_a, labels_b, lam = mixup_data(inputs, labels, mixup_alpha, device)\n",
        "#                 optimizer.zero_grad()\n",
        "#                 outputs = ensemble_model(inputs)\n",
        "#                 loss = mixup_criterion(criterion, outputs, labels_a, labels_b, lam)\n",
        "#             else:\n",
        "#                 optimizer.zero_grad()\n",
        "#                 outputs = ensemble_model(inputs)\n",
        "#                 loss = criterion(outputs, labels)\n",
        "\n",
        "#             # Backward pass\n",
        "#             loss.backward()\n",
        "\n",
        "#             # Gradient clipping to avoid exploding gradients\n",
        "#             torch.nn.utils.clip_grad_norm_(ensemble_model.parameters(), max_norm=1.0)\n",
        "\n",
        "#             optimizer.step()\n",
        "\n",
        "#             # Update statistics\n",
        "#             running_loss += loss.item()\n",
        "\n",
        "#             # Calculate accuracy only for non-mixup batches\n",
        "#             if not use_mixup:\n",
        "#                 _, predicted = torch.max(outputs.data, 1)\n",
        "#                 total += labels.size(0)\n",
        "#                 correct += (predicted == labels).sum().item()\n",
        "\n",
        "#         # Calculate epoch metrics\n",
        "#         epoch_train_loss = running_loss / len(train_loader)\n",
        "\n",
        "#         # If we used mixup on all batches, run a separate evaluation pass\n",
        "#         if total == 0:\n",
        "#             ensemble_model.eval()\n",
        "#             correct = 0\n",
        "#             total = 0\n",
        "#             with torch.no_grad():\n",
        "#                 for inputs, labels in train_loader:\n",
        "#                     inputs, labels = inputs.to(device), labels.to(device)\n",
        "#                     outputs = ensemble_model(inputs)\n",
        "#                     _, predicted = torch.max(outputs.data, 1)\n",
        "#                     total += labels.size(0)\n",
        "#                     correct += (predicted == labels).sum().item()\n",
        "\n",
        "#         epoch_train_acc = 100.0 * correct / total\n",
        "#         train_losses.append(epoch_train_loss)\n",
        "#         train_accs.append(epoch_train_acc)\n",
        "\n",
        "#         # Validation phase\n",
        "#         ensemble_model.eval()\n",
        "#         val_loss = 0.0\n",
        "#         val_correct = 0\n",
        "#         val_total = 0\n",
        "#         all_preds = []\n",
        "#         all_targets = []\n",
        "\n",
        "#         # Progress bar for validation\n",
        "#         val_iterator = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\")\n",
        "\n",
        "#         with torch.no_grad():\n",
        "#             for inputs, labels in val_iterator:\n",
        "#                 inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "#                 # Forward pass\n",
        "#                 outputs = ensemble_model(inputs)\n",
        "\n",
        "#                 # Calculate loss\n",
        "#                 loss = criterion(outputs, labels)\n",
        "#                 val_loss += loss.item()\n",
        "\n",
        "#                 # Calculate accuracy\n",
        "#                 _, predicted = torch.max(outputs.data, 1)\n",
        "#                 val_total += labels.size(0)\n",
        "#                 val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "#                 # Save predictions and targets for confusion matrix\n",
        "#                 all_preds.extend(predicted.cpu().numpy())\n",
        "#                 all_targets.extend(labels.cpu().numpy())\n",
        "\n",
        "#         # Calculate validation metrics\n",
        "#         epoch_val_loss = val_loss / len(val_loader)\n",
        "#         epoch_val_acc = 100.0 * val_correct / val_total\n",
        "#         val_losses.append(epoch_val_loss)\n",
        "#         val_accs.append(epoch_val_acc)\n",
        "\n",
        "#         # Update learning rate based on validation loss\n",
        "#         scheduler.step(epoch_val_loss)\n",
        "\n",
        "#         # Calculate epoch duration\n",
        "#         epoch_time = time.time() - start_time\n",
        "\n",
        "#         # Print epoch results\n",
        "#         print(f\"Epoch {epoch+1}/{epochs} | Time: {epoch_time:.1f}s | \"\n",
        "#               f\"Train Loss: {epoch_train_loss:.4f} | Train Acc: {epoch_train_acc:.2f}% | \"\n",
        "#               f\"Val Loss: {epoch_val_loss:.4f} | Val Acc: {epoch_val_acc:.2f}%\")\n",
        "\n",
        "#         # Check if this is the best model so far\n",
        "#         if epoch_val_acc > best_val_acc:\n",
        "#             best_val_acc = epoch_val_acc\n",
        "#             best_val_loss = epoch_val_loss\n",
        "#             patience_counter = 0\n",
        "\n",
        "#             # Save the best model\n",
        "#             torch.save(ensemble_model.state_dict(), os.path.join(save_dir, 'best_model.pt'))\n",
        "#             print(f\"✅ New best model saved with Val Acc: {epoch_val_acc:.2f}%\")\n",
        "\n",
        "#             # Generate confusion matrix for best model\n",
        "#             try:\n",
        "#                 cm = confusion_matrix(all_targets, all_preds)\n",
        "#                 plt.figure(figsize=(12, 10))\n",
        "#                 sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "#                 plt.title(f'Validation Confusion Matrix - Epoch {epoch+1}')\n",
        "#                 plt.xlabel('Predicted')\n",
        "#                 plt.ylabel('True')\n",
        "#                 plt.savefig(os.path.join(save_dir, f'confusion_matrix_epoch_{epoch+1}.png'))\n",
        "#                 plt.close()\n",
        "\n",
        "#                 # Print detailed classification report\n",
        "#                 print(\"\\nClassification Report:\")\n",
        "#                 print(classification_report(all_targets, all_preds))\n",
        "#             except Exception as e:\n",
        "#                 print(f\"Could not generate confusion matrix: {e}\")\n",
        "#         else:\n",
        "#             patience_counter += 1\n",
        "\n",
        "#         # Early stopping check\n",
        "#         if patience_counter >= 7:  # Stop if no improvement for 7 epochs\n",
        "#             print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
        "#             break\n",
        "\n",
        "#     # Final evaluation on test set\n",
        "#     print(\"\\nEvaluating final model on test set...\")\n",
        "#     ensemble_model.load_state_dict(torch.load(os.path.join(save_dir, 'best_model.pt')))\n",
        "#     ensemble_model.eval()\n",
        "#     test_correct = 0\n",
        "#     test_total = 0\n",
        "#     all_test_preds = []\n",
        "#     all_test_targets = []\n",
        "#     all_test_scores = []\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         for inputs, labels in test_loader:\n",
        "#             inputs, labels = inputs.to(device), labels.to(device)\n",
        "#             outputs = ensemble_model(inputs)\n",
        "#             _, predicted = torch.max(outputs.data, 1)\n",
        "#             test_total += labels.size(0)\n",
        "#             test_correct += (predicted == labels).sum().item()\n",
        "#             all_test_preds.extend(predicted.cpu().numpy())\n",
        "#             all_test_targets.extend(labels.cpu().numpy())\n",
        "#             all_test_scores.extend(F.softmax(outputs, dim=1).cpu().numpy())\n",
        "\n",
        "#     test_acc = 100.0 * test_correct / test_total\n",
        "#     print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "#     # Calculate metrics per class\n",
        "#     from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
        "\n",
        "#     # Convert to 1D arrays\n",
        "#     y_true = np.array(all_test_targets)\n",
        "#     y_pred = np.array(all_test_preds)\n",
        "#     y_score = np.array(all_test_scores)\n",
        "\n",
        "#     # Calculate precision, recall, and F1-score per class\n",
        "#     precision, recall, f1, support = precision_recall_fscore_support(\n",
        "#         y_true, y_pred, average=None)\n",
        "\n",
        "#     # Calculate metrics\n",
        "#     weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(\n",
        "#         y_true, y_pred, average='weighted')\n",
        "\n",
        "#     # Attempt to calculate ROC AUC score\n",
        "#     try:\n",
        "#         roc_auc = roc_auc_score(y_true, y_score, multi_class='ovr', average='weighted')\n",
        "#         print(f\"ROC AUC Score (weighted): {roc_auc:.4f}\")\n",
        "#     except Exception as e:\n",
        "#         print(f\"Could not calculate ROC AUC: {e}\")\n",
        "\n",
        "#     print(f\"Weighted Precision: {weighted_precision:.4f}\")\n",
        "#     print(f\"Weighted Recall: {weighted_recall:.4f}\")\n",
        "#     print(f\"Weighted F1 Score: {weighted_f1:.4f}\")\n",
        "\n",
        "#     # Generate final confusion matrix and report\n",
        "#     try:\n",
        "#         cm = confusion_matrix(all_test_targets, all_test_preds)\n",
        "#         plt.figure(figsize=(14, 12))\n",
        "#         sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "#         plt.title('Final Test Confusion Matrix')\n",
        "#         plt.xlabel('Predicted')\n",
        "#         plt.ylabel('True')\n",
        "#         plt.savefig(os.path.join(save_dir, 'final_test_confusion_matrix.png'))\n",
        "#         plt.close()\n",
        "\n",
        "#         # Per-class metrics visualization\n",
        "#         plt.figure(figsize=(14, 8))\n",
        "#         indices = np.arange(len(precision))\n",
        "#         width = 0.25\n",
        "\n",
        "#         plt.bar(indices - width, precision, width, label='Precision')\n",
        "#         plt.bar(indices, recall, width, label='Recall')\n",
        "#         plt.bar(indices + width, f1, width, label='F1-Score')\n",
        "\n",
        "#         plt.xlabel('Class')\n",
        "#         plt.ylabel('Score')\n",
        "#         plt.title('Per-Class Classification Metrics')\n",
        "#         plt.xticks(indices)\n",
        "#         plt.legend()\n",
        "#         plt.tight_layout()\n",
        "#         plt.savefig(os.path.join(save_dir, 'per_class_metrics.png'))\n",
        "#         plt.close()\n",
        "\n",
        "#         print(\"\\nFinal Classification Report:\")\n",
        "#         print(classification_report(all_test_targets, all_test_preds))\n",
        "#     except Exception as e:\n",
        "#         print(f\"Could not generate final confusion matrix: {e}\")\n",
        "\n",
        "#     # Plot training history\n",
        "#     plt.figure(figsize=(15, 6))\n",
        "\n",
        "#     plt.subplot(1, 2, 1)\n",
        "#     plt.plot(train_losses, label='Training Loss')\n",
        "#     plt.plot(val_losses, label='Validation Loss')\n",
        "#     plt.title('Loss over Epochs')\n",
        "#     plt.xlabel('Epoch')\n",
        "#     plt.ylabel('Loss')\n",
        "#     plt.legend()\n",
        "#     plt.grid(True)\n",
        "\n",
        "#     plt.subplot(1, 2, 2)\n",
        "#     plt.plot(train_accs, label='Training Accuracy')\n",
        "#     plt.plot(val_accs, label='Validation Accuracy')\n",
        "#     plt.title('Accuracy over Epochs')\n",
        "#     plt.xlabel('Epoch')\n",
        "#     plt.ylabel('Accuracy (%)')\n",
        "#     plt.legend()\n",
        "#     plt.grid(True)\n",
        "\n",
        "#     plt.tight_layout()\n",
        "#     plt.savefig(os.path.join(save_dir, 'training_history.png'))\n",
        "#     plt.close()\n",
        "\n",
        "#     return ensemble_model, test_acc\n"
      ],
      "metadata": {
        "id": "71EORVoQ9Y9N"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2"
      ],
      "metadata": {
        "id": "1RQaJYUiUPUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# import torch.nn.functional as F\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# from torch.utils.data import DataLoader, TensorDataset\n",
        "# from sklearn.metrics import confusion_matrix, classification_report\n",
        "# import time\n",
        "# import os\n",
        "# import seaborn as sns\n",
        "# from tqdm import tqdm\n",
        "\n",
        "# # Improved 2D CNN for tabular data with batch normalization fix\n",
        "# class RevisedCNN(nn.Module):\n",
        "#     def __init__(self, input_dim, output_dim, dropout_rate=0.3):\n",
        "#         super(RevisedCNN, self).__init__()\n",
        "\n",
        "#         # Reshape input to 2D representation\n",
        "#         self.input_dim = input_dim\n",
        "#         self.reshape_dim = int(np.sqrt(input_dim)) + 1\n",
        "#         self.pad_size = self.reshape_dim**2 - input_dim\n",
        "\n",
        "#         # Convolutional layers with larger kernel and better stride\n",
        "#         self.conv1 = nn.Conv2d(1, 64, kernel_size=5, stride=1, padding=2)\n",
        "#         self.bn1 = nn.BatchNorm2d(64, track_running_stats=True, momentum=0.1, eps=1e-5)\n",
        "#         self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "#         # Compute output size after first conv+pool\n",
        "#         conv1_out_size = self.reshape_dim // 2\n",
        "\n",
        "#         self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "#         self.bn2 = nn.BatchNorm2d(128, track_running_stats=True, momentum=0.1, eps=1e-5)\n",
        "#         self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "#         # Compute output size after second conv+pool\n",
        "#         conv2_out_size = conv1_out_size // 2\n",
        "\n",
        "#         # Calculate flattened size\n",
        "#         self.flattened_size = 128 * max(1, conv2_out_size) * max(1, conv2_out_size)\n",
        "\n",
        "#         # Fully connected layers with proper sizes\n",
        "#         self.fc1 = nn.Linear(self.flattened_size, 512)\n",
        "#         self.bn_fc1 = nn.BatchNorm1d(512, track_running_stats=True, momentum=0.1, eps=1e-5)\n",
        "#         self.dropout_fc1 = nn.Dropout(dropout_rate)\n",
        "\n",
        "#         self.fc2 = nn.Linear(512, 256)\n",
        "#         self.bn_fc2 = nn.BatchNorm1d(256, track_running_stats=True, momentum=0.1, eps=1e-5)\n",
        "#         self.dropout_fc2 = nn.Dropout(dropout_rate)\n",
        "\n",
        "#         self.fc3 = nn.Linear(256, output_dim)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         batch_size = x.size(0)\n",
        "\n",
        "#         # Reshape input to 2D (square-like format for better convolution)\n",
        "#         x_padded = F.pad(x, (0, self.pad_size))\n",
        "#         x = x_padded.view(batch_size, 1, self.reshape_dim, self.reshape_dim)\n",
        "\n",
        "#         # Apply convolutional layers\n",
        "#         x = self.conv1(x)\n",
        "\n",
        "#         # Apply batch norm only if batch size > 1\n",
        "#         if x.size(0) > 1:\n",
        "#             x = self.bn1(x)\n",
        "\n",
        "#         x = F.leaky_relu(x, negative_slope=0.1)\n",
        "#         x = self.pool1(x)\n",
        "\n",
        "#         x = self.conv2(x)\n",
        "\n",
        "#         # Apply batch norm only if batch size > 1\n",
        "#         if x.size(0) > 1:\n",
        "#             x = self.bn2(x)\n",
        "\n",
        "#         x = F.leaky_relu(x, negative_slope=0.1)\n",
        "#         x = self.pool2(x)\n",
        "\n",
        "#         # Flatten\n",
        "#         x = x.view(batch_size, -1)\n",
        "\n",
        "#         # Fully connected layers\n",
        "#         x = self.fc1(x)\n",
        "\n",
        "#         # Apply batch norm only if batch size > 1\n",
        "#         if x.size(0) > 1:\n",
        "#             x = self.bn_fc1(x)\n",
        "\n",
        "#         x = F.leaky_relu(x, negative_slope=0.1)\n",
        "#         x = self.dropout_fc1(x)\n",
        "\n",
        "#         x = self.fc2(x)\n",
        "\n",
        "#         # Apply batch norm only if batch size > 1\n",
        "#         if x.size(0) > 1:\n",
        "#             x = self.bn_fc2(x)\n",
        "\n",
        "#         x = F.leaky_relu(x, negative_slope=0.1)\n",
        "#         x = self.dropout_fc2(x)\n",
        "\n",
        "#         x = self.fc3(x)\n",
        "\n",
        "#         return x\n",
        "\n",
        "\n",
        "# # Improved RBM with proper pretraining abilities\n",
        "# class ImprovedRBM(nn.Module):\n",
        "#     def __init__(self, input_size, hidden_size):\n",
        "#         super(ImprovedRBM, self).__init__()\n",
        "\n",
        "#         # Initialize weights with small values for stability\n",
        "#         self.W = nn.Parameter(torch.randn(input_size, hidden_size) * 0.01)\n",
        "#         self.b = nn.Parameter(torch.zeros(hidden_size))\n",
        "#         self.c = nn.Parameter(torch.zeros(input_size))\n",
        "#         self.input_size = input_size\n",
        "#         self.hidden_size = hidden_size\n",
        "\n",
        "#         # Xavier/Glorot initialization for better convergence\n",
        "#         nn.init.xavier_uniform_(self.W)\n",
        "\n",
        "#     def forward(self, v):\n",
        "#         \"\"\"Visible to hidden layer probabilities\"\"\"\n",
        "#         batch_size = v.size(0)\n",
        "\n",
        "#         # Reshape input to 2D if needed\n",
        "#         if v.dim() > 2:\n",
        "#             v = v.view(batch_size, -1)\n",
        "\n",
        "#         # Handle dimension mismatch gracefully\n",
        "#         if v.size(1) != self.input_size:\n",
        "#             v_resized = torch.zeros(batch_size, self.input_size, device=v.device)\n",
        "#             min_size = min(v.size(1), self.input_size)\n",
        "#             v_resized[:, :min_size] = v[:, :min_size]\n",
        "#             v = v_resized\n",
        "\n",
        "#         # Apply L2 normalization for better stability\n",
        "#         v = F.normalize(v, p=2, dim=1)\n",
        "\n",
        "#         # Compute hidden activations\n",
        "#         h_activation = torch.matmul(v, self.W) + self.b\n",
        "#         h_probs = torch.sigmoid(h_activation)\n",
        "\n",
        "#         return h_probs\n",
        "\n",
        "#     def sample_h(self, v):\n",
        "#         \"\"\"Sample from the hidden layer given visible state\"\"\"\n",
        "#         h_probs = self.forward(v)\n",
        "#         h_samples = torch.bernoulli(h_probs)\n",
        "#         return h_samples, h_probs\n",
        "\n",
        "#     def sample_v(self, h):\n",
        "#         \"\"\"Sample from the visible layer given hidden state\"\"\"\n",
        "#         v_activation = torch.matmul(h, self.W.t()) + self.c\n",
        "#         v_probs = torch.sigmoid(v_activation)\n",
        "#         v_samples = torch.bernoulli(v_probs)\n",
        "#         return v_samples, v_probs\n",
        "\n",
        "#     def cd_k(self, v_data, k=1):\n",
        "#         \"\"\"Contrastive Divergence with k steps\"\"\"\n",
        "#         h_data, h_data_probs = self.sample_h(v_data)\n",
        "\n",
        "#         # Initialize the chain with data samples\n",
        "#         h_model = h_data\n",
        "\n",
        "#         # Gibbs sampling\n",
        "#         for _ in range(k):\n",
        "#             v_model, v_model_probs = self.sample_v(h_model)\n",
        "#             h_model, h_model_probs = self.sample_h(v_model)\n",
        "\n",
        "#         return v_data, h_data_probs, v_model_probs, h_model_probs\n",
        "\n",
        "#     def free_energy(self, v):\n",
        "#         \"\"\"Calculate free energy\"\"\"\n",
        "#         wx_b = torch.matmul(v, self.W) + self.b\n",
        "#         hidden_term = torch.sum(F.softplus(wx_b), dim=1)\n",
        "#         vbias_term = torch.matmul(v, self.c)\n",
        "#         return -hidden_term - vbias_term\n",
        "\n",
        "\n",
        "# # Function to pretrain an RBM\n",
        "# def pretrain_rbm(rbm, dataloader, device, epochs=5, lr=0.001):\n",
        "#     \"\"\"Pretrain RBM using Contrastive Divergence\"\"\"\n",
        "#     optimizer = torch.optim.Adam(rbm.parameters(), lr=lr)\n",
        "#     rbm.train()\n",
        "\n",
        "#     print(f\"Pretraining RBM {rbm.input_size} -> {rbm.hidden_size}...\")\n",
        "\n",
        "#     for epoch in range(epochs):\n",
        "#         mean_loss = 0\n",
        "#         num_batches = 0\n",
        "\n",
        "#         for batch_idx, (data, _) in enumerate(dataloader):\n",
        "#             data = data.to(device)\n",
        "\n",
        "#             # Skip batches with only one sample\n",
        "#             if data.size(0) <= 1:\n",
        "#                 continue\n",
        "\n",
        "#             # Run k-step Contrastive Divergence\n",
        "#             v_data, h_data, v_model, h_model = rbm.cd_k(data, k=1)\n",
        "\n",
        "#             # Compute gradients using CD loss\n",
        "#             # Positive phase - negative phase\n",
        "#             pos_associations = torch.matmul(v_data.t(), h_data)\n",
        "#             neg_associations = torch.matmul(v_model.t(), h_model)\n",
        "\n",
        "#             # Update weights and biases\n",
        "#             optimizer.zero_grad()\n",
        "\n",
        "#             # Manually set gradients\n",
        "#             rbm.W.grad = -(pos_associations - neg_associations) / data.size(0)\n",
        "#             rbm.b.grad = -(h_data - h_model).mean(0)\n",
        "#             rbm.c.grad = -(v_data - v_model).mean(0)\n",
        "\n",
        "#             optimizer.step()\n",
        "\n",
        "#             # Compute reconstruction error\n",
        "#             recon_error = F.mse_loss(v_model, v_data)\n",
        "#             mean_loss += recon_error.item()\n",
        "#             num_batches += 1\n",
        "\n",
        "#         # Print epoch stats\n",
        "#         if num_batches > 0:  # Avoid division by zero\n",
        "#             mean_loss /= num_batches\n",
        "#         print(f\"  Epoch {epoch+1}/{epochs}, Reconstruction Error: {mean_loss:.6f}\")\n",
        "\n",
        "#     return rbm\n",
        "\n",
        "\n",
        "# # Improved DBN with proper pretraining\n",
        "# class ImprovedDBN(nn.Module):\n",
        "#     def __init__(self, input_dim, hidden_dims, output_dim, dropout_rate=0.3):\n",
        "#         super(ImprovedDBN, self).__init__()\n",
        "#         self.input_dim = input_dim\n",
        "#         self.hidden_dims = hidden_dims\n",
        "#         self.output_dim = output_dim\n",
        "\n",
        "#         # Stack of RBM layers\n",
        "#         self.rbm_layers = nn.ModuleList()\n",
        "\n",
        "#         # First RBM\n",
        "#         self.rbm_layers.append(ImprovedRBM(input_dim, hidden_dims[0]))\n",
        "\n",
        "#         # Additional RBM layers\n",
        "#         for i in range(1, len(hidden_dims)):\n",
        "#             self.rbm_layers.append(ImprovedRBM(hidden_dims[i-1], hidden_dims[i]))\n",
        "\n",
        "#         # Fully connected layers after RBMs\n",
        "#         self.fc_layers = nn.ModuleList()\n",
        "\n",
        "#         # First FC layer from last RBM's output\n",
        "#         self.fc_layers.append(nn.Sequential(\n",
        "#             nn.Linear(hidden_dims[-1], 256),\n",
        "#             nn.BatchNorm1d(256, track_running_stats=True, momentum=0.1, eps=1e-5),\n",
        "#             nn.LeakyReLU(0.1),\n",
        "#             nn.Dropout(dropout_rate)\n",
        "#         ))\n",
        "\n",
        "#         # Second FC layer\n",
        "#         self.fc_layers.append(nn.Sequential(\n",
        "#             nn.Linear(256, 128),\n",
        "#             nn.BatchNorm1d(128, track_running_stats=True, momentum=0.1, eps=1e-5),\n",
        "#             nn.LeakyReLU(0.1),\n",
        "#             nn.Dropout(dropout_rate)\n",
        "#         ))\n",
        "\n",
        "#         # Output layer\n",
        "#         self.output_layer = nn.Linear(128, output_dim)\n",
        "\n",
        "#         # Better initialization for linear layers\n",
        "#         for m in self.modules():\n",
        "#             if isinstance(m, nn.Linear):\n",
        "#                 nn.init.xavier_uniform_(m.weight)\n",
        "#                 nn.init.constant_(m.bias, 0)\n",
        "\n",
        "#     def pretrain(self, dataloader, device, epochs=5):\n",
        "#         \"\"\"Pretrain the DBN layer by layer\"\"\"\n",
        "#         print(\"Pretraining DBN layers...\")\n",
        "\n",
        "#         # Train first RBM with input data\n",
        "#         self.rbm_layers[0] = pretrain_rbm(\n",
        "#             self.rbm_layers[0], dataloader, device, epochs)\n",
        "\n",
        "#         # For each subsequent layer, train with features from previous layer\n",
        "#         for i in range(1, len(self.rbm_layers)):\n",
        "#             # Extract features from previous layer\n",
        "#             prev_layer_features = []\n",
        "#             with torch.no_grad():\n",
        "#                 for data, _ in dataloader:\n",
        "#                     # Skip batches with only one sample\n",
        "#                     if data.size(0) <= 1:\n",
        "#                         continue\n",
        "\n",
        "#                     data = data.to(device)\n",
        "\n",
        "#                     # Forward pass through previous layers\n",
        "#                     for j in range(i):\n",
        "#                         data = self.rbm_layers[j](data)\n",
        "\n",
        "#                     prev_layer_features.append(data.cpu())\n",
        "\n",
        "#             # Create dataset with extracted features\n",
        "#             if len(prev_layer_features) > 0:  # Make sure we have features\n",
        "#                 prev_features = torch.cat(prev_layer_features, dim=0)\n",
        "#                 feature_dataset = TensorDataset(prev_features, torch.zeros(prev_features.size(0)))\n",
        "#                 feature_loader = DataLoader(\n",
        "#                     feature_dataset,\n",
        "#                     batch_size=dataloader.batch_size,\n",
        "#                     shuffle=True,\n",
        "#                     num_workers=0\n",
        "#                 )\n",
        "\n",
        "#                 # Pretrain this RBM with extracted features\n",
        "#                 self.rbm_layers[i] = pretrain_rbm(\n",
        "#                     self.rbm_layers[i], feature_loader, device, epochs)\n",
        "#             else:\n",
        "#                 print(f\"Warning: No features extracted for layer {i}. Skipping pretraining.\")\n",
        "\n",
        "#         print(\"DBN pretraining complete.\")\n",
        "#         return self\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         \"\"\"Forward pass through the DBN\"\"\"\n",
        "#         batch_size = x.size(0)\n",
        "\n",
        "#         # Flatten input if not already flat\n",
        "#         if x.dim() > 2:\n",
        "#             x = x.view(batch_size, -1)\n",
        "\n",
        "#         # Forward through RBM layers\n",
        "#         for rbm in self.rbm_layers:\n",
        "#             x = rbm(x)\n",
        "\n",
        "#         # Forward through FC layers with batch size check\n",
        "#         for fc in self.fc_layers:\n",
        "#             # Handle batch normalization in the sequential module\n",
        "#             if batch_size > 1:\n",
        "#                 x = fc(x)\n",
        "#             else:\n",
        "#                 # Apply each component manually, skipping batch norm if necessary\n",
        "#                 for layer in fc:\n",
        "#                     if isinstance(layer, nn.BatchNorm1d):\n",
        "#                         continue\n",
        "#                     x = layer(x)\n",
        "\n",
        "#         # Output layer\n",
        "#         x = self.output_layer(x)\n",
        "\n",
        "#         return x\n",
        "\n",
        "\n",
        "# # Improved dynamic ensemble model\n",
        "# class ImprovedDynamicEnsembleModel(nn.Module):\n",
        "#     def __init__(self, cnn_model, dbn_model, output_dim):\n",
        "#         super(ImprovedDynamicEnsembleModel, self).__init__()\n",
        "#         self.cnn_model = cnn_model\n",
        "#         self.dbn_model = dbn_model\n",
        "#         self.output_dim = output_dim\n",
        "\n",
        "#         # Class-wise attention - one attention weight per class\n",
        "#         self.class_attention = nn.ModuleList([\n",
        "#             nn.Sequential(\n",
        "#                 nn.Linear(2, 16),\n",
        "#                 nn.LeakyReLU(0.1),\n",
        "#                 nn.Linear(16, 2),\n",
        "#                 nn.Softmax(dim=1)\n",
        "#             ) for _ in range(output_dim)\n",
        "#         ])\n",
        "\n",
        "#         # Gating network to decide contribution from each model\n",
        "#         self.gate_network = nn.Sequential(\n",
        "#             nn.Linear(output_dim * 2, 64),\n",
        "#             nn.LeakyReLU(0.1),\n",
        "#             nn.Linear(64, 1),\n",
        "#             nn.Sigmoid()\n",
        "#         )\n",
        "\n",
        "#         # Combiner network\n",
        "#         self.combiner = nn.Sequential(\n",
        "#             nn.Linear(output_dim * 2, 256),\n",
        "#             nn.LeakyReLU(0.1),  # Removed BatchNorm to avoid issues with small batches\n",
        "#             nn.Dropout(0.3),\n",
        "#             nn.Linear(256, output_dim)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # Save original input for DBN\n",
        "#         x_original = x.clone()\n",
        "#         batch_size = x.size(0)\n",
        "\n",
        "#         # Forward pass through CNN\n",
        "#         cnn_logits = self.cnn_model(x_original)\n",
        "\n",
        "#         # Forward pass through DBN\n",
        "#         dbn_logits = self.dbn_model(x_original)\n",
        "\n",
        "#         # Get probabilities\n",
        "#         cnn_probs = F.softmax(cnn_logits, dim=1)\n",
        "#         dbn_probs = F.softmax(dbn_logits, dim=1)\n",
        "\n",
        "#         # Confidence values\n",
        "#         cnn_conf = torch.max(cnn_probs, dim=1, keepdim=True)[0]\n",
        "#         dbn_conf = torch.max(dbn_probs, dim=1, keepdim=True)[0]\n",
        "\n",
        "#         # Calculate gating value to balance models\n",
        "#         model_confs = torch.cat([cnn_logits, dbn_logits], dim=1)\n",
        "#         gate_value = self.gate_network(model_confs)\n",
        "\n",
        "#         # Class-wise weighting\n",
        "#         weighted_outputs = []\n",
        "#         for i in range(self.output_dim):\n",
        "#             # Get logits for this class\n",
        "#             class_values = torch.cat([\n",
        "#                 cnn_logits[:, i:i+1],\n",
        "#                 dbn_logits[:, i:i+1]\n",
        "#             ], dim=1)\n",
        "\n",
        "#             # Get weights for this class\n",
        "#             weights = self.class_attention[i](class_values)\n",
        "\n",
        "#             # Weight the outputs\n",
        "#             weighted_class = (weights[:, 0:1] * cnn_logits[:, i:i+1] +\n",
        "#                              weights[:, 1:2] * dbn_logits[:, i:i+1])\n",
        "#             weighted_outputs.append(weighted_class)\n",
        "\n",
        "#         # Stack all class outputs\n",
        "#         class_weighted_output = torch.cat(weighted_outputs, dim=1)\n",
        "\n",
        "#         # Combine with original outputs\n",
        "#         combined = torch.cat([cnn_logits, dbn_logits], dim=1)\n",
        "\n",
        "#         # Dynamic final output\n",
        "#         final_output = gate_value * self.combiner(combined) + (1 - gate_value) * class_weighted_output\n",
        "\n",
        "#         return final_output\n",
        "\n",
        "\n",
        "# # Focal Loss implementation for handling class imbalance\n",
        "# class FocalLoss(nn.Module):\n",
        "#     def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
        "#         super(FocalLoss, self).__init__()\n",
        "#         self.alpha = alpha  # Class weights\n",
        "#         self.gamma = gamma  # Focusing parameter\n",
        "#         self.reduction = reduction\n",
        "\n",
        "#     def forward(self, inputs, targets):\n",
        "#         # Standard cross entropy\n",
        "#         ce_loss = F.cross_entropy(\n",
        "#             inputs, targets, weight=self.alpha,\n",
        "#             reduction='none'\n",
        "#         )\n",
        "\n",
        "#         # Get probabilities\n",
        "#         pt = torch.exp(-ce_loss)\n",
        "\n",
        "#         # Calculate focal term\n",
        "#         focal_term = (1 - pt) ** self.gamma\n",
        "\n",
        "#         # Apply focal term to CE loss\n",
        "#         loss = focal_term * ce_loss\n",
        "\n",
        "#         # Apply reduction\n",
        "#         if self.reduction == 'mean':\n",
        "#             return loss.mean()\n",
        "#         elif self.reduction == 'sum':\n",
        "#             return loss.sum()\n",
        "#         else:\n",
        "#             return loss\n",
        "\n",
        "\n",
        "# # Mixup data augmentation function\n",
        "# def mixup_data(x, y, alpha=0.2, device='cuda'):\n",
        "#     \"\"\"Mixup data augmentation function\"\"\"\n",
        "#     if alpha > 0:\n",
        "#         lam = np.random.beta(alpha, alpha)\n",
        "#     else:\n",
        "#         lam = 1\n",
        "\n",
        "#     batch_size = x.size(0)\n",
        "\n",
        "#     # Handle small batch sizes\n",
        "#     if batch_size <= 1:\n",
        "#         return x, y, y, 1.0  # No mixup for single sample\n",
        "\n",
        "#     index = torch.randperm(batch_size).to(device)\n",
        "\n",
        "#     mixed_x = lam * x + (1 - lam) * x[index]\n",
        "#     y_a, y_b = y, y[index]\n",
        "\n",
        "#     return mixed_x, y_a, y_b, lam\n",
        "\n",
        "\n",
        "# # Mixup criterion function\n",
        "# def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "#     \"\"\"Criterion for mixup data augmentation\"\"\"\n",
        "#     return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "\n",
        "# # Training function with advanced techniques\n",
        "# def train_ensemble_model(ensemble_model, train_loader, val_loader, test_loader,\n",
        "#                         device, class_weights=None, epochs=30, lr=0.001,\n",
        "#                         focal_gamma=2.0, mixup_alpha=0.2, save_dir='.'):\n",
        "#     print(\"Starting advanced training process...\")\n",
        "\n",
        "#     # Create save directory if it doesn't exist\n",
        "#     os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "#     # Setup optimizer with weight decay\n",
        "#     optimizer = optim.AdamW(ensemble_model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "\n",
        "#     # Learning rate scheduler\n",
        "#     scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "#         optimizer, mode='min', factor=0.5, patience=3, verbose=True, min_lr=1e-6\n",
        "#     )\n",
        "\n",
        "#     # Loss function - use focal loss for imbalanced classes\n",
        "#     if class_weights is not None:\n",
        "#         class_weights = class_weights.to(device)\n",
        "\n",
        "#     criterion = FocalLoss(alpha=class_weights, gamma=focal_gamma)\n",
        "\n",
        "#     # Tracking metrics\n",
        "#     train_losses = []\n",
        "#     val_losses = []\n",
        "#     train_accs = []\n",
        "#     val_accs = []\n",
        "#     best_val_acc = 0.0\n",
        "#     best_val_loss = float('inf')\n",
        "#     patience_counter = 0\n",
        "\n",
        "#     # Training loop with early stopping\n",
        "#     for epoch in range(epochs):\n",
        "#         # Training phase\n",
        "#         start_time = time.time()\n",
        "#         ensemble_model.train()\n",
        "#         running_loss = 0.0\n",
        "#         correct = 0\n",
        "#         total = 0\n",
        "\n",
        "#         # Track processed batches to avoid division by zero\n",
        "#         valid_batches = 0\n",
        "\n",
        "#         # Progress bar for training\n",
        "#         train_iterator = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n",
        "\n",
        "#         for inputs, labels in train_iterator:\n",
        "#             inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "#             # Skip single-sample batches during training\n",
        "#             if inputs.size(0) <= 1:\n",
        "#                 print(f\"Warning: Skipping batch with size {inputs.size(0)}\")\n",
        "#                 continue\n",
        "\n",
        "#             valid_batches += 1\n",
        "\n",
        "#             # Apply mixup augmentation randomly with 50% probability\n",
        "#             use_mixup = np.random.random() < 0.5 and mixup_alpha > 0 and inputs.size(0) > 1\n",
        "\n",
        "#             if use_mixup:\n",
        "#                 inputs, labels_a, labels_b, lam = mixup_data(inputs, labels, mixup_alpha, device)\n",
        "#                 optimizer.zero_grad()\n",
        "#                 outputs = ensemble_model(inputs)\n",
        "#                 loss = mixup_criterion(criterion, outputs, labels_a, labels_b, lam)\n",
        "#             else:\n",
        "#                 optimizer.zero_grad()\n",
        "#                 outputs = ensemble_model(inputs)\n",
        "#                 loss = criterion(outputs, labels)\n",
        "\n",
        "#             # Backward pass\n",
        "#             loss.backward()\n",
        "\n",
        "#             # Gradient clipping to avoid exploding gradients\n",
        "#             torch.nn.utils.clip_grad_norm_(ensemble_model.parameters(), max_norm=1.0)\n",
        "\n",
        "#             optimizer.step()\n",
        "\n",
        "#             # Update statistics\n",
        "#             running_loss += loss.item()\n",
        "\n",
        "#             # Calculate accuracy only for non-mixup batches\n",
        "#             if not use_mixup:\n",
        "#                 _, predicted = torch.max(outputs.data, 1)\n",
        "#                 total += labels.size(0)\n",
        "#                 correct += (predicted == labels).sum().item()\n",
        "\n",
        "#         # Calculate epoch metrics (avoid division by zero)\n",
        "#         if valid_batches > 0:\n",
        "#             epoch_train_loss = running_loss / valid_batches\n",
        "#         else:\n",
        "#             epoch_train_loss = float('inf')\n",
        "#             print(\"Warning: No valid batches processed in training epoch\")\n",
        "\n",
        "#         # If we used mixup on all batches or have no valid batches, run a separate evaluation pass\n",
        "#         if total == 0:\n",
        "#             ensemble_model.eval()\n",
        "#             correct = 0\n",
        "#             total = 0\n",
        "#             with torch.no_grad():\n",
        "#                 for inputs, labels in train_loader:\n",
        "#                     # Skip single sample batches\n",
        "#                     if inputs.size(0) <= 1:\n",
        "#                         continue\n",
        "\n",
        "#                     inputs, labels = inputs.to(device), labels.to(device)\n",
        "#                     outputs = ensemble_model(inputs)\n",
        "#                     _, predicted = torch.max(outputs.data, 1)\n",
        "#                     total += labels.size(0)\n",
        "#                     correct += (predicted == labels).sum().item()\n",
        "\n",
        "#         # Calculate training accuracy (avoid division by zero)\n",
        "#         epoch_train_acc = 100.0 * correct / max(1, total)\n",
        "#         train_losses.append(epoch_train_loss)\n",
        "#         train_accs.append(epoch_train_acc)\n",
        "\n",
        "#         # Validation phase\n",
        "#         ensemble_model.eval()\n",
        "#         val_loss = 0.0\n",
        "#         val_correct = 0\n",
        "#         val_total = 0\n",
        "#         all_preds = []\n",
        "#         all_targets = []\n",
        "#         valid_val_batches = 0\n",
        "\n",
        "#         # Progress bar for validation\n",
        "#         val_iterator = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\")\n",
        "\n",
        "#         with torch.no_grad():\n",
        "#             for inputs, labels in val_iterator:\n",
        "#                 # Skip single sample batches\n",
        "#                 if inputs.size(0) <= 1:\n",
        "#                     continue\n",
        "\n",
        "#                 inputs, labels = inputs.to(device), labels.to(device)\n",
        "#                 valid_val_batches += 1\n",
        "\n",
        "#                 # Forward pass\n",
        "#                 outputs = ensemble_model(inputs)\n",
        "\n",
        "#                 # Calculate loss\n",
        "#                 loss = criterion(outputs, labels)\n",
        "#                 val_loss += loss.item()\n",
        "\n",
        "#                 # Calculate accuracy\n",
        "#                 _, predicted = torch.max(outputs.data, 1)\n",
        "#                 val_total += labels.size(0)\n",
        "#                 val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "#                 # Save predictions and targets for confusion matrix\n",
        "#                 all_preds.extend(predicted.cpu().numpy())\n",
        "#                 all_targets.extend(labels.cpu().numpy())\n",
        "\n",
        "#         # Calculate validation metrics (avoid division by zero)\n",
        "#         if valid_val_batches > 0:\n",
        "#             epoch_val_loss = val_loss / valid_val_batches\n",
        "#         else:\n",
        "#             epoch_val_loss = float('inf')\n",
        "#             print(\"Warning: No valid batches processed in validation\")\n",
        "\n",
        "#         # Avoid division by zero\n",
        "#         epoch_val_acc = 100.0 * val_correct / max(1, val_total)\n",
        "#         val_losses.append(epoch_val_loss)\n",
        "#         val_accs.append(epoch_val_acc)\n",
        "\n",
        "#         # Update learning rate based on validation loss\n",
        "#         scheduler.step(epoch_val_loss)\n",
        "\n",
        "#         # Calculate epoch duration\n",
        "#         epoch_time = time.time() - start_time\n",
        "\n",
        "#         # Print epoch results\n",
        "#         print(f\"Epoch {epoch+1}/{epochs} | Time: {epoch_time:.1f}s | \"\n",
        "#               f\"Train Loss: {epoch_train_loss:.4f} | Train Acc: {epoch_train_acc:.2f}% | \"\n",
        "#               f\"Val Loss: {epoch_val_loss:.4f} | Val Acc: {epoch_val_acc:.2f}%\")\n",
        "\n",
        "#         # Check if this is the best model so far\n",
        "#         if epoch_val_acc > best_val_acc:\n",
        "#             best_val_acc = epoch_val_acc\n",
        "#             best_val_loss = epoch_val_loss\n",
        "#             patience_counter = 0\n",
        "\n",
        "#             # Save the best model\n",
        "#             torch.save(ensemble_model.state_dict(), os.path.join(save_dir, 'best_model.pt'))\n",
        "#             print(f\"✅ New best model saved with Val Acc: {epoch_val_acc:.2f}%\")\n",
        "\n",
        "#             # Generate confusion matrix for best model\n",
        "#             if len(all_preds) > 0 and len(all_targets) > 0:\n",
        "#                 try:\n",
        "#                     cm = confusion_matrix(all_targets, all_preds)\n",
        "#                     plt.figure(figsize=(12, 10))\n",
        "#                     sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "#                     plt.title(f'Validation Confusion Matrix - Epoch {epoch+1}')\n",
        "#                     plt.xlabel('Predicted')\n",
        "#                     plt.ylabel('True')\n",
        "#                     plt.savefig(os.path.join(save_dir, f'confusion_matrix_epoch_{epoch+1}.png'))\n",
        "#                     plt.close()\n",
        "\n",
        "#                     # Print detailed classification report\n",
        "#                     print(\"\\nClassification Report:\")\n",
        "#                     print(classification_report(all_targets, all_preds))\n",
        "#                 except Exception as e:\n",
        "#                     print(f\"Could not generate confusion matrix: {e}\")\n",
        "#         else:\n",
        "#             patience_counter += 1\n",
        "\n",
        "#         # Early stopping check\n",
        "#         if patience_counter >= 7:  # Stop if no improvement for 7 epochs\n",
        "#             print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
        "#             break\n",
        "\n",
        "#     # Final evaluation on test set\n",
        "#     print(\"\\nEvaluating final model on test set...\")\n",
        "#     # Load best model\n",
        "#     try:\n",
        "#         ensemble_model.load_state_dict(torch.load(os.path.join(save_dir, 'best_model.pt')))\n",
        "#         print(\"Loaded best model successfully\")\n",
        "#     except Exception as e:\n",
        "#         print(f\"Could not load best model: {e}. Using current model state.\")\n",
        "\n",
        "#     ensemble_model.eval()\n",
        "#     test_correct = 0\n",
        "#     test_total = 0\n",
        "#     all_test_preds = []\n",
        "#     all_test_targets = []\n",
        "#     all_test_scores = []\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         for inputs, labels in test_loader:\n",
        "#             # Handle test batches appropriately - we evaluate all test samples\n",
        "#             inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "#             # Use batch size check in forward pass to handle single samples\n",
        "#             outputs = ensemble_model(inputs)\n",
        "\n",
        "#             _, predicted = torch.max(outputs.data, 1)\n",
        "#             test_total += labels.size(0)\n",
        "#             test_correct += (predicted == labels).sum().item()\n",
        "#             all_test_preds.extend(predicted.cpu().numpy())\n",
        "#             all_test_targets.extend(labels.cpu().numpy())\n",
        "#             all_test_scores.extend(F.softmax(outputs, dim=1).cpu().numpy())\n",
        "\n",
        "#     test_acc = 100.0 * test_correct / max(1, test_total)\n",
        "#     print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "#     # Calculate metrics per class (if we have predictions)\n",
        "#     if len(all_test_preds) > 0 and len(all_test_targets) > 0:\n",
        "#         from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
        "\n",
        "#         # Convert to 1D arrays\n",
        "#         y_true = np.array(all_test_targets)\n",
        "#         y_pred = np.array(all_test_preds)\n",
        "#         y_score = np.array(all_test_scores)\n",
        "\n",
        "#         # Calculate precision, recall, and F1-score per class\n",
        "#         precision, recall, f1, support = precision_recall_fscore_support(\n",
        "#             y_true, y_pred, average=None)\n",
        "\n",
        "#         # Calculate metrics\n",
        "#         weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(\n",
        "#             y_true, y_pred, average='weighted')\n",
        "\n",
        "#         # Attempt to calculate ROC AUC score\n",
        "#         try:\n",
        "#             roc_auc = roc_auc_score(y_true, y_score, multi_class='ovr', average='weighted')\n",
        "#             print(f\"ROC AUC Score (weighted): {roc_auc:.4f}\")\n",
        "#         except Exception as e:\n",
        "#             print(f\"Could not calculate ROC AUC: {e}\")\n",
        "\n",
        "#         print(f\"Weighted Precision: {weighted_precision:.4f}\")\n",
        "#         print(f\"Weighted Recall: {weighted_recall:.4f}\")\n",
        "#         print(f\"Weighted F1 Score: {weighted_f1:.4f}\")\n",
        "\n",
        "#         # Generate final confusion matrix and report\n",
        "#         try:\n",
        "#             cm = confusion_matrix(all_test_targets, all_test_preds)\n",
        "#             plt.figure(figsize=(14, 12))\n",
        "#             sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "#             plt.title('Final Test Confusion Matrix')\n",
        "#             plt.xlabel('Predicted')\n",
        "#             plt.ylabel('True')\n",
        "#             plt.savefig(os.path.join(save_dir, 'final_test_confusion_matrix.png'))\n",
        "#             plt.close()\n",
        "\n",
        "#             # Per-class metrics visualization\n",
        "#             plt.figure(figsize=(14, 8))\n",
        "#             indices = np.arange(len(precision))\n",
        "#             width = 0.25\n",
        "\n",
        "#             plt.bar(indices - width, precision, width, label='Precision')\n",
        "#             plt.bar(indices, recall, width, label='Recall')\n",
        "#             plt.bar(indices + width, f1, width, label='F1-Score')\n",
        "\n",
        "#             plt.xlabel('Class')\n",
        "#             plt.ylabel('Score')\n",
        "#             plt.title('Per-Class Classification Metrics')\n",
        "#             plt.xticks(indices)\n",
        "#             plt.legend()\n",
        "#             plt.tight_layout()\n",
        "#             plt.savefig(os.path.join(save_dir, 'per_class_metrics.png'))\n",
        "#             plt.close()\n",
        "\n",
        "#             print(\"\\nFinal Classification Report:\")\n",
        "#             print(classification_report(all_test_targets, all_test_preds))\n",
        "#         except Exception as e:\n",
        "#             print(f\"Could not generate final confusion matrix: {e}\")\n",
        "\n",
        "#     # Plot training history\n",
        "#     plt.figure(figsize=(15, 6))\n",
        "\n",
        "#     plt.subplot(1, 2, 1)\n",
        "#     plt.plot(train_losses, label='Training Loss')\n",
        "#     plt.plot(val_losses, label='Validation Loss')\n",
        "#     plt.title('Loss over Epochs')\n",
        "#     plt.xlabel('Epoch')\n",
        "#     plt.ylabel('Loss')\n",
        "#     plt.legend()\n",
        "#     plt.grid(True)\n",
        "\n",
        "#     plt.subplot(1, 2, 2)\n",
        "#     plt.plot(train_accs, label='Training Accuracy')\n",
        "#     plt.plot(val_accs, label='Validation Accuracy')\n",
        "#     plt.title('Accuracy over Epochs')\n",
        "#     plt.xlabel('Epoch')\n",
        "#     plt.ylabel('Accuracy (%)')\n",
        "#     plt.legend()\n",
        "#     plt.grid(True)\n",
        "\n",
        "#     plt.tight_layout()\n",
        "#     plt.savefig(os.path.join(save_dir, 'training_history.png'))\n",
        "#     plt.close()\n",
        "\n",
        "#     return ensemble_model, test_acc"
      ],
      "metadata": {
        "id": "am19RisuP37j"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3"
      ],
      "metadata": {
        "id": "wMYVhpk4USxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# import torch.nn.functional as F\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# from torch.utils.data import DataLoader, TensorDataset\n",
        "# from sklearn.metrics import confusion_matrix, classification_report\n",
        "# import time\n",
        "# import os\n",
        "# import seaborn as sns\n",
        "# from tqdm import tqdm\n",
        "# from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "# # Set device (use GPU if available, otherwise use CPU)\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# print(f\"🚀 Using Device: {device}\")\n",
        "\n",
        "# # Define the CNN-LSTM model with batch normalization fix\n",
        "# class CNN_LSTM(nn.Module):\n",
        "#     def __init__(self, input_size, num_classes):\n",
        "#         super(CNN_LSTM, self).__init__()\n",
        "\n",
        "#         # Convolutional Layers\n",
        "#         self.conv1 = nn.Conv1d(in_channels=1, out_channels=128, kernel_size=3, padding=1)\n",
        "#         self.bn1 = nn.BatchNorm1d(128, track_running_stats=True)\n",
        "#         self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "#         self.conv2 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
        "#         self.bn2 = nn.BatchNorm1d(256, track_running_stats=True)\n",
        "#         self.pool2 = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "#         self.conv3 = nn.Conv1d(in_channels=256, out_channels=512, kernel_size=3, padding=1)\n",
        "#         self.bn3 = nn.BatchNorm1d(512, track_running_stats=True)\n",
        "#         self.pool3 = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "#         # LSTM Layer\n",
        "#         self.lstm = nn.LSTM(input_size=512, hidden_size=256, num_layers=2, batch_first=True, dropout=0.3)\n",
        "\n",
        "#         # Fully Connected Layers\n",
        "#         self.fc1 = nn.Linear(256, 128)\n",
        "#         self.dropout1 = nn.Dropout(0.5)\n",
        "#         self.fc2 = nn.Linear(128, 64)\n",
        "#         self.dropout2 = nn.Dropout(0.5)\n",
        "#         self.fc3 = nn.Linear(64, num_classes)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         batch_size = x.size(0)\n",
        "#         # Add channel dimension (batch_size, 1, input_size)\n",
        "#         x = x.unsqueeze(1)\n",
        "\n",
        "#         # Convolutional Layers with batch size check\n",
        "#         x = self.conv1(x)\n",
        "#         if x.size(0) > 1:  # Apply batch norm only if batch size > 1\n",
        "#             x = self.bn1(x)\n",
        "#         x = F.relu(x)\n",
        "#         x = self.pool1(x)\n",
        "\n",
        "#         x = self.conv2(x)\n",
        "#         if x.size(0) > 1:  # Apply batch norm only if batch size > 1\n",
        "#             x = self.bn2(x)\n",
        "#         x = F.relu(x)\n",
        "#         x = self.pool2(x)\n",
        "\n",
        "#         x = self.conv3(x)\n",
        "#         if x.size(0) > 1:  # Apply batch norm only if batch size > 1\n",
        "#             x = self.bn3(x)\n",
        "#         x = F.relu(x)\n",
        "#         x = self.pool3(x)\n",
        "\n",
        "#         # Prepare for LSTM (batch_size, seq_len, features)\n",
        "#         x = x.permute(0, 2, 1)\n",
        "\n",
        "#         # LSTM Layer\n",
        "#         x, _ = self.lstm(x)\n",
        "\n",
        "#         # Use the last LSTM output\n",
        "#         x = x[:, -1, :]\n",
        "\n",
        "#         # Fully Connected Layers\n",
        "#         x = F.relu(self.fc1(x))\n",
        "#         x = self.dropout1(x)\n",
        "#         x = F.relu(self.fc2(x))\n",
        "#         x = self.dropout2(x)\n",
        "#         x = self.fc3(x)\n",
        "\n",
        "#         return x\n",
        "\n",
        "\n",
        "# # Improved 2D CNN for tabular data with batch normalization fix\n",
        "# class RevisedCNN(nn.Module):\n",
        "#     def __init__(self, input_dim, output_dim, dropout_rate=0.3):\n",
        "#         super(RevisedCNN, self).__init__()\n",
        "\n",
        "#         # Reshape input to 2D representation\n",
        "#         self.input_dim = input_dim\n",
        "#         self.reshape_dim = int(np.sqrt(input_dim)) + 1\n",
        "#         self.pad_size = self.reshape_dim**2 - input_dim\n",
        "\n",
        "#         # Convolutional layers with larger kernel and better stride\n",
        "#         self.conv1 = nn.Conv2d(1, 64, kernel_size=5, stride=1, padding=2)\n",
        "#         self.bn1 = nn.BatchNorm2d(64, track_running_stats=True)\n",
        "#         self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "#         # Compute output size after first conv+pool\n",
        "#         conv1_out_size = self.reshape_dim // 2\n",
        "\n",
        "#         self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "#         self.bn2 = nn.BatchNorm2d(128, track_running_stats=True)\n",
        "#         self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "#         # Compute output size after second conv+pool\n",
        "#         conv2_out_size = conv1_out_size // 2\n",
        "\n",
        "#         # Calculate flattened size\n",
        "#         self.flattened_size = 128 * max(1, conv2_out_size) * max(1, conv2_out_size)\n",
        "\n",
        "#         # Fully connected layers with proper sizes\n",
        "#         self.fc1 = nn.Linear(self.flattened_size, 512)\n",
        "#         self.bn_fc1 = nn.BatchNorm1d(512, track_running_stats=True)\n",
        "#         self.dropout_fc1 = nn.Dropout(dropout_rate)\n",
        "\n",
        "#         self.fc2 = nn.Linear(512, 256)\n",
        "#         self.bn_fc2 = nn.BatchNorm1d(256, track_running_stats=True)\n",
        "#         self.dropout_fc2 = nn.Dropout(dropout_rate)\n",
        "\n",
        "#         self.fc3 = nn.Linear(256, output_dim)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         batch_size = x.size(0)\n",
        "\n",
        "#         # Reshape input to 2D (square-like format for better convolution)\n",
        "#         x_padded = F.pad(x, (0, self.pad_size))\n",
        "#         x = x_padded.view(batch_size, 1, self.reshape_dim, self.reshape_dim)\n",
        "\n",
        "#         # Apply convolutional layers\n",
        "#         x = self.conv1(x)\n",
        "#         if x.size(0) > 1:  # Apply batch norm only if batch size > 1\n",
        "#             x = self.bn1(x)\n",
        "#         x = F.leaky_relu(x, negative_slope=0.1)\n",
        "#         x = self.pool1(x)\n",
        "\n",
        "#         x = self.conv2(x)\n",
        "#         if x.size(0) > 1:  # Apply batch norm only if batch size > 1\n",
        "#             x = self.bn2(x)\n",
        "#         x = F.leaky_relu(x, negative_slope=0.1)\n",
        "#         x = self.pool2(x)\n",
        "\n",
        "#         # Flatten\n",
        "#         x = x.view(batch_size, -1)\n",
        "\n",
        "#         # Fully connected layers\n",
        "#         x = self.fc1(x)\n",
        "#         if x.size(0) > 1:  # Apply batch norm only if batch size > 1\n",
        "#             x = self.bn_fc1(x)\n",
        "#         x = F.leaky_relu(x, negative_slope=0.1)\n",
        "#         x = self.dropout_fc1(x)\n",
        "\n",
        "#         x = self.fc2(x)\n",
        "#         if x.size(0) > 1:  # Apply batch norm only if batch size > 1\n",
        "#             x = self.bn_fc2(x)\n",
        "#         x = F.leaky_relu(x, negative_slope=0.1)\n",
        "#         x = self.dropout_fc2(x)\n",
        "\n",
        "#         x = self.fc3(x)\n",
        "\n",
        "#         return x\n",
        "\n",
        "\n",
        "# # Focal Loss implementation for handling class imbalance\n",
        "# class FocalLoss(nn.Module):\n",
        "#     def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
        "#         super(FocalLoss, self).__init__()\n",
        "#         self.alpha = alpha  # Class weights\n",
        "#         self.gamma = gamma  # Focusing parameter\n",
        "#         self.reduction = reduction\n",
        "\n",
        "#     def forward(self, inputs, targets):\n",
        "#         # Standard cross entropy\n",
        "#         ce_loss = F.cross_entropy(\n",
        "#             inputs, targets, weight=self.alpha,\n",
        "#             reduction='none'\n",
        "#         )\n",
        "\n",
        "#         # Get probabilities\n",
        "#         pt = torch.exp(-ce_loss)\n",
        "\n",
        "#         # Calculate focal term\n",
        "#         focal_term = (1 - pt) ** self.gamma\n",
        "\n",
        "#         # Apply focal term to CE loss\n",
        "#         loss = focal_term * ce_loss\n",
        "\n",
        "#         # Apply reduction\n",
        "#         if self.reduction == 'mean':\n",
        "#             return loss.mean()\n",
        "#         elif self.reduction == 'sum':\n",
        "#             return loss.sum()\n",
        "#         else:\n",
        "#             return loss\n",
        "\n",
        "\n",
        "# # Advanced Ensemble combining CNN-LSTM and 2D CNN models\n",
        "# class AdvancedEnsembleModel(nn.Module):\n",
        "#     def __init__(self, input_dim, num_classes):\n",
        "#         super(AdvancedEnsembleModel, self).__init__()\n",
        "\n",
        "#         # Create component models\n",
        "#         self.cnn_lstm = CNN_LSTM(input_dim, num_classes)\n",
        "#         self.revised_cnn = RevisedCNN(input_dim, num_classes)\n",
        "\n",
        "#         # Gating network to decide contribution from each model\n",
        "#         self.gate_network = nn.Sequential(\n",
        "#             nn.Linear(num_classes * 2, 64),\n",
        "#             nn.LeakyReLU(0.1),\n",
        "#             nn.Linear(64, 1),\n",
        "#             nn.Sigmoid()\n",
        "#         )\n",
        "\n",
        "#         # Combiner network\n",
        "#         self.combiner = nn.Sequential(\n",
        "#             nn.Linear(num_classes * 2, 128),\n",
        "#             nn.LeakyReLU(0.1),\n",
        "#             nn.Dropout(0.3),\n",
        "#             nn.Linear(128, num_classes)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # Get predictions from both models\n",
        "#         cnn_lstm_out = self.cnn_lstm(x)\n",
        "#         revised_cnn_out = self.revised_cnn(x)\n",
        "\n",
        "#         # Calculate gating value to balance models\n",
        "#         model_confs = torch.cat([cnn_lstm_out, revised_cnn_out], dim=1)\n",
        "#         gate_value = self.gate_network(model_confs)\n",
        "\n",
        "#         # Combine predictions\n",
        "#         combined = torch.cat([cnn_lstm_out, revised_cnn_out], dim=1)\n",
        "#         combined_out = self.combiner(combined)\n",
        "\n",
        "#         # Final output is weighted sum of combined output and individual outputs\n",
        "#         final_output = gate_value * combined_out + (1 - gate_value) * (\n",
        "#             0.5 * cnn_lstm_out + 0.5 * revised_cnn_out\n",
        "#         )\n",
        "\n",
        "#         return final_output\n",
        "\n",
        "\n",
        "# # Function for model training with advanced features\n",
        "# def train_advanced_model(model, train_loader, val_loader, test_loader,\n",
        "#                         device, class_weights=None, num_epochs=30,\n",
        "#                         save_dir='model_checkpoints'):\n",
        "#     # Create save directory if it doesn't exist\n",
        "#     os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "#     print(\"🔥 Starting advanced training process...\")\n",
        "\n",
        "#     # Setup optimizer with weight decay\n",
        "#     optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
        "\n",
        "#     # Use Focal Loss if class_weights provided, otherwise CrossEntropy\n",
        "#     if class_weights is not None:\n",
        "#         class_weights = class_weights.to(device)\n",
        "#         criterion = FocalLoss(alpha=class_weights, gamma=2.0)\n",
        "#         print(\"📊 Using Focal Loss with class weights\")\n",
        "#     else:\n",
        "#         criterion = nn.CrossEntropyLoss()\n",
        "#         print(\"📊 Using standard CrossEntropyLoss\")\n",
        "\n",
        "#     # Learning rate scheduler\n",
        "#     scheduler = ReduceLROnPlateau(\n",
        "#         optimizer, mode='min', factor=0.5, patience=3, verbose=True, min_lr=1e-6\n",
        "#     )\n",
        "\n",
        "#     # Tracking metrics\n",
        "#     best_val_acc = 0.0\n",
        "#     patience = 5  # Early stopping patience\n",
        "#     trigger_times = 0  # Counter for early stopping\n",
        "\n",
        "#     # For plotting\n",
        "#     train_losses = []\n",
        "#     train_accs = []\n",
        "#     val_losses = []\n",
        "#     val_accs = []\n",
        "\n",
        "#     # Training loop\n",
        "#     for epoch in range(num_epochs):\n",
        "#         start_time = time.time()\n",
        "#         model.train()\n",
        "#         running_loss = 0.0\n",
        "#         correct = 0\n",
        "#         total = 0\n",
        "\n",
        "#         # Wrap train_loader with tqdm for a progress bar\n",
        "#         train_loader_tqdm = tqdm(\n",
        "#             train_loader,\n",
        "#             desc=f\"Epoch {epoch+1}/{num_epochs}\",\n",
        "#             unit=\"batch\",\n",
        "#             bar_format=\"{l_bar}{bar:20}{r_bar}\",  # Customize the bar format\n",
        "#             ascii=True  # Use ASCII characters for the bar\n",
        "#         )\n",
        "\n",
        "#         for batch_idx, (inputs, labels) in enumerate(train_loader_tqdm):\n",
        "#             # Skip batches with only one sample\n",
        "#             if inputs.size(0) <= 1:\n",
        "#                 continue\n",
        "\n",
        "#             # Move data to device\n",
        "#             inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "#             # Zero the gradients\n",
        "#             optimizer.zero_grad()\n",
        "\n",
        "#             # Forward pass\n",
        "#             outputs = model(inputs)\n",
        "#             loss = criterion(outputs, labels)\n",
        "\n",
        "#             # Backward pass and optimization\n",
        "#             loss.backward()\n",
        "\n",
        "#             # Gradient clipping to avoid exploding gradients\n",
        "#             torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "#             optimizer.step()\n",
        "\n",
        "#             # Update running loss and accuracy\n",
        "#             running_loss += loss.item()\n",
        "#             _, predicted = torch.max(outputs.data, 1)\n",
        "#             total += labels.size(0)\n",
        "#             correct += (predicted == labels).sum().item()\n",
        "\n",
        "#             # Update progress bar description\n",
        "#             train_loader_tqdm.set_postfix(\n",
        "#                 loss=loss.item(),\n",
        "#                 accuracy=f\"{(correct / total) * 100:.2f}%\"\n",
        "#             )\n",
        "\n",
        "#         # Calculate training metrics\n",
        "#         epoch_train_loss = running_loss / max(1, len(train_loader))\n",
        "#         epoch_train_acc = 100.0 * correct / max(1, total)\n",
        "#         train_losses.append(epoch_train_loss)\n",
        "#         train_accs.append(epoch_train_acc)\n",
        "\n",
        "#         # Print training statistics\n",
        "#         print(f\"\\n✅ Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_train_loss:.4f}, Accuracy: {epoch_train_acc:.2f}%\")\n",
        "\n",
        "#         # Validation phase\n",
        "#         model.eval()\n",
        "#         val_loss = 0.0\n",
        "#         val_correct = 0\n",
        "#         val_total = 0\n",
        "#         all_preds = []\n",
        "#         all_targets = []\n",
        "\n",
        "#         with torch.no_grad():\n",
        "#             # Wrap val_loader with tqdm for a progress bar\n",
        "#             val_loader_tqdm = tqdm(\n",
        "#                 val_loader,\n",
        "#                 desc=f\"Validating\",\n",
        "#                 unit=\"batch\",\n",
        "#                 bar_format=\"{l_bar}{bar:20}{r_bar}\",\n",
        "#                 ascii=True\n",
        "#             )\n",
        "\n",
        "#             for inputs, labels in val_loader_tqdm:\n",
        "#                 # Skip batches with only one sample\n",
        "#                 if inputs.size(0) <= 1:\n",
        "#                     continue\n",
        "\n",
        "#                 # Move data to device\n",
        "#                 inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "#                 # Forward pass\n",
        "#                 outputs = model(inputs)\n",
        "#                 loss = criterion(outputs, labels)\n",
        "\n",
        "#                 # Update validation statistics\n",
        "#                 val_loss += loss.item()\n",
        "#                 _, predicted = torch.max(outputs.data, 1)\n",
        "#                 val_total += labels.size(0)\n",
        "#                 val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "#                 # Save predictions for confusion matrix\n",
        "#                 all_preds.extend(predicted.cpu().numpy())\n",
        "#                 all_targets.extend(labels.cpu().numpy())\n",
        "\n",
        "#                 # Update progress bar\n",
        "#                 val_loader_tqdm.set_postfix(\n",
        "#                     loss=loss.item(),\n",
        "#                     accuracy=f\"{(val_correct / val_total) * 100:.2f}%\"\n",
        "#                 )\n",
        "\n",
        "#         # Calculate validation metrics\n",
        "#         epoch_val_loss = val_loss / max(1, len(val_loader))\n",
        "#         epoch_val_acc = 100.0 * val_correct / max(1, val_total)\n",
        "#         val_losses.append(epoch_val_loss)\n",
        "#         val_accs.append(epoch_val_acc)\n",
        "\n",
        "#         # Print validation statistics\n",
        "#         print(f\"🔹 Validation Loss: {epoch_val_loss:.4f}, Accuracy: {epoch_val_acc:.2f}%\")\n",
        "\n",
        "#         # Update learning rate scheduler\n",
        "#         scheduler.step(epoch_val_loss)\n",
        "\n",
        "#         # Check if this is the best model so far\n",
        "#         if epoch_val_acc > best_val_acc:\n",
        "#             best_val_acc = epoch_val_acc\n",
        "\n",
        "#             # Save the best model\n",
        "#             torch.save({\n",
        "#                 'epoch': epoch + 1,\n",
        "#                 'model_state_dict': model.state_dict(),\n",
        "#                 'optimizer_state_dict': optimizer.state_dict(),\n",
        "#                 'loss': epoch_val_loss,\n",
        "#                 'accuracy': epoch_val_acc\n",
        "#             }, os.path.join(save_dir, \"best_model.pth\"))\n",
        "\n",
        "#             print(f\"💾 Best Model Saved: best_model.pth (Accuracy: {epoch_val_acc:.2f}%)\")\n",
        "\n",
        "#             # Reset early stopping counter\n",
        "#             trigger_times = 0\n",
        "\n",
        "#             # Generate confusion matrix for best model\n",
        "#             if len(all_preds) > 0 and len(all_targets) > 0:\n",
        "#                 try:\n",
        "#                     cm = confusion_matrix(all_targets, all_preds)\n",
        "#                     plt.figure(figsize=(12, 10))\n",
        "#                     sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "#                     plt.title(f'Validation Confusion Matrix - Epoch {epoch+1}')\n",
        "#                     plt.xlabel('Predicted')\n",
        "#                     plt.ylabel('True')\n",
        "#                     plt.savefig(os.path.join(save_dir, f'confusion_matrix_epoch_{epoch+1}.png'))\n",
        "#                     plt.close()\n",
        "\n",
        "#                     # Print classification report\n",
        "#                     print(\"\\n📋 Classification Report:\")\n",
        "#                     print(classification_report(all_targets, all_preds))\n",
        "#                 except Exception as e:\n",
        "#                     print(f\"⚠️ Could not generate confusion matrix: {e}\")\n",
        "#         else:\n",
        "#             # Increment early stopping counter\n",
        "#             trigger_times += 1\n",
        "#             if trigger_times >= patience:\n",
        "#                 print(\"⛔ Early Stopping Triggered!\")\n",
        "#                 break\n",
        "\n",
        "#     # Final evaluation on test set\n",
        "#     print(\"\\n🧪 Evaluating final model on test set...\")\n",
        "\n",
        "#     # Load best model\n",
        "#     try:\n",
        "#         checkpoint = torch.load(os.path.join(save_dir, \"best_model.pth\"))\n",
        "#         model.load_state_dict(checkpoint['model_state_dict'])\n",
        "#         print(f\"📂 Loaded best model from epoch {checkpoint['epoch']}\")\n",
        "#     except Exception as e:\n",
        "#         print(f\"⚠️ Could not load best model: {e}. Using current model state.\")\n",
        "\n",
        "#     model.eval()\n",
        "#     test_loss = 0.0\n",
        "#     test_correct = 0\n",
        "#     test_total = 0\n",
        "#     all_test_preds = []\n",
        "#     all_test_targets = []\n",
        "#     all_test_scores = []\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         test_loader_tqdm = tqdm(\n",
        "#             test_loader,\n",
        "#             desc=\"Testing\",\n",
        "#             unit=\"batch\",\n",
        "#             bar_format=\"{l_bar}{bar:20}{r_bar}\",\n",
        "#             ascii=True\n",
        "#         )\n",
        "\n",
        "#         for inputs, labels in test_loader_tqdm:\n",
        "#             # Move data to device\n",
        "#             inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "#             # Forward pass\n",
        "#             outputs = model(inputs)\n",
        "#             loss = criterion(outputs, labels)\n",
        "\n",
        "#             # Update test statistics\n",
        "#             test_loss += loss.item()\n",
        "#             _, predicted = torch.max(outputs.data, 1)\n",
        "#             test_total += labels.size(0)\n",
        "#             test_correct += (predicted == labels).sum().item()\n",
        "\n",
        "#             # Save predictions for metrics\n",
        "#             all_test_preds.extend(predicted.cpu().numpy())\n",
        "#             all_test_targets.extend(labels.cpu().numpy())\n",
        "#             all_test_scores.extend(F.softmax(outputs, dim=1).cpu().numpy())\n",
        "\n",
        "#             # Update progress bar\n",
        "#             test_loader_tqdm.set_postfix(\n",
        "#                 loss=loss.item(),\n",
        "#                 accuracy=f\"{(test_correct / test_total) * 100:.2f}%\"\n",
        "#             )\n",
        "\n",
        "#     # Calculate test accuracy\n",
        "#     test_acc = 100.0 * test_correct / max(1, test_total)\n",
        "#     print(f\"🏆 Test Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "#     # Calculate detailed metrics\n",
        "#     from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
        "\n",
        "#     if len(all_test_preds) > 0 and len(all_test_targets) > 0:\n",
        "#         # Convert to arrays\n",
        "#         y_true = np.array(all_test_targets)\n",
        "#         y_pred = np.array(all_test_preds)\n",
        "#         y_score = np.array(all_test_scores)\n",
        "\n",
        "#         # Calculate precision, recall, and F1-score\n",
        "#         precision, recall, f1, support = precision_recall_fscore_support(\n",
        "#             y_true, y_pred, average=None)\n",
        "\n",
        "#         # Calculate weighted metrics\n",
        "#         weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(\n",
        "#             y_true, y_pred, average='weighted')\n",
        "\n",
        "#         # Try to calculate ROC AUC score\n",
        "#         try:\n",
        "#             roc_auc = roc_auc_score(y_true, y_score, multi_class='ovr', average='weighted')\n",
        "#             print(f\"📈 ROC AUC Score (weighted): {roc_auc:.4f}\")\n",
        "#         except Exception as e:\n",
        "#             print(f\"⚠️ Could not calculate ROC AUC: {e}\")\n",
        "\n",
        "#         print(f\"📏 Weighted Precision: {weighted_precision:.4f}\")\n",
        "#         print(f\"📏 Weighted Recall: {weighted_recall:.4f}\")\n",
        "#         print(f\"📏 Weighted F1 Score: {weighted_f1:.4f}\")\n",
        "\n",
        "#         # Generate final confusion matrix\n",
        "#         try:\n",
        "#             cm = confusion_matrix(all_test_targets, all_test_preds)\n",
        "#             plt.figure(figsize=(14, 12))\n",
        "#             sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "#             plt.title('Final Test Confusion Matrix')\n",
        "#             plt.xlabel('Predicted')\n",
        "#             plt.ylabel('True')\n",
        "#             plt.savefig(os.path.join(save_dir, 'final_test_confusion_matrix.png'))\n",
        "#             plt.close()\n",
        "\n",
        "#             # Per-class metrics visualization\n",
        "#             plt.figure(figsize=(14, 8))\n",
        "#             indices = np.arange(len(precision))\n",
        "#             width = 0.25\n",
        "\n",
        "#             plt.bar(indices - width, precision, width, label='Precision')\n",
        "#             plt.bar(indices, recall, width, label='Recall')\n",
        "#             plt.bar(indices + width, f1, width, label='F1-Score')\n",
        "\n",
        "#             plt.xlabel('Class')\n",
        "#             plt.ylabel('Score')\n",
        "#             plt.title('Per-Class Classification Metrics')\n",
        "#             plt.xticks(indices)\n",
        "#             plt.legend()\n",
        "#             plt.tight_layout()\n",
        "#             plt.savefig(os.path.join(save_dir, 'per_class_metrics.png'))\n",
        "#             plt.close()\n",
        "\n",
        "#             # Print final classification report\n",
        "#             print(\"\\n📋 Final Classification Report:\")\n",
        "#             print(classification_report(all_test_targets, all_test_preds))\n",
        "#         except Exception as e:\n",
        "#             print(f\"⚠️ Could not generate final confusion matrix: {e}\")\n",
        "\n",
        "#     # Plot training history\n",
        "#     plt.figure(figsize=(15, 6))\n",
        "\n",
        "#     plt.subplot(1, 2, 1)\n",
        "#     plt.plot(train_losses, label='Training Loss')\n",
        "#     plt.plot(val_losses, label='Validation Loss')\n",
        "#     plt.title('Loss over Epochs')\n",
        "#     plt.xlabel('Epoch')\n",
        "#     plt.ylabel('Loss')\n",
        "#     plt.legend()\n",
        "#     plt.grid(True)\n",
        "\n",
        "#     plt.subplot(1, 2, 2)\n",
        "#     plt.plot(train_accs, label='Training Accuracy')\n",
        "#     plt.plot(val_accs, label='Validation Accuracy')\n",
        "#     plt.title('Accuracy over Epochs')\n",
        "#     plt.xlabel('Epoch')\n",
        "#     plt.ylabel('Accuracy (%)')\n",
        "#     plt.legend()\n",
        "#     plt.grid(True)\n",
        "\n",
        "#     plt.tight_layout()\n",
        "#     plt.savefig(os.path.join(save_dir, 'training_history.png'))\n",
        "#     plt.close()\n",
        "\n",
        "#     print(\"\\n✅ Training Complete!\")\n",
        "#     return model, test_acc\n",
        "\n",
        "\n",
        "# # Example usage\n",
        "# def main(processed_data, save_dir='model_checkpoints'):\n",
        "#     # Extract data from processed_data dictionary\n",
        "#     train_loader = processed_data['train_loader']\n",
        "#     val_loader = processed_data['val_loader']\n",
        "#     test_loader = processed_data['test_loader']\n",
        "#     input_dim = processed_data['input_dim']\n",
        "#     num_classes = processed_data['num_classes']\n",
        "#     class_weights = processed_data['class_weights']\n",
        "\n",
        "#     # Initialize the advanced ensemble model\n",
        "#     print(f\"🔧 Creating Advanced Ensemble Model with {input_dim} features and {num_classes} classes\")\n",
        "#     model = AdvancedEnsembleModel(input_dim, num_classes).to(device)\n",
        "\n",
        "#     # Train the model\n",
        "#     model, test_acc = train_advanced_model(\n",
        "#         model,\n",
        "#         train_loader,\n",
        "#         val_loader,\n",
        "#         test_loader,\n",
        "#         device,\n",
        "#         class_weights=class_weights,\n",
        "#         num_epochs=50,\n",
        "#         save_dir=save_dir\n",
        "#     )\n",
        "\n",
        "#     return model, test_acc\n",
        "\n",
        "\n",
        "# # To use the code:\n",
        "# # 1. First run the enhanced_preprocessing function from the previous code\n",
        "# # 2. Then run the main function with the processed data\n",
        "# # processed_data = enhanced_preprocessing(train_data, test_data)\n",
        "# # model, test_acc = main(processed_data)"
      ],
      "metadata": {
        "id": "8yP673WsUBc_"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4"
      ],
      "metadata": {
        "id": "XpoDjYTaYBLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import time\n",
        "import os\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "# Set device (use GPU if available, otherwise use CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"🚀 Using Device: {device}\")\n",
        "\n",
        "# CNN-LSTM Model\n",
        "class CNN_LSTM(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(CNN_LSTM, self).__init__()\n",
        "\n",
        "        # Convolutional Layers\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=128, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm1d(128, track_running_stats=True)\n",
        "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm1d(256, track_running_stats=True)\n",
        "        self.pool2 = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "        self.conv3 = nn.Conv1d(in_channels=256, out_channels=512, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm1d(512, track_running_stats=True)\n",
        "        self.pool3 = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "        # LSTM Layer\n",
        "        self.lstm = nn.LSTM(input_size=512, hidden_size=256, num_layers=2, batch_first=True, dropout=0.3)\n",
        "\n",
        "        # Fully Connected Layers\n",
        "        self.fc1 = nn.Linear(256, 128)\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc3 = nn.Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        # Add channel dimension (batch_size, 1, input_size)\n",
        "        x = x.unsqueeze(1)\n",
        "\n",
        "        # Convolutional Layers with batch size check\n",
        "        x = self.conv1(x)\n",
        "        if x.size(0) > 1:  # Apply batch norm only if batch size > 1\n",
        "            x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        if x.size(0) > 1:  # Apply batch norm only if batch size > 1\n",
        "            x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        if x.size(0) > 1:  # Apply batch norm only if batch size > 1\n",
        "            x = self.bn3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool3(x)\n",
        "\n",
        "        # Prepare for LSTM (batch_size, seq_len, features)\n",
        "        x = x.permute(0, 2, 1)\n",
        "\n",
        "        # LSTM Layer\n",
        "        x, _ = self.lstm(x)\n",
        "\n",
        "        # Use the last LSTM output\n",
        "        x = x[:, -1, :]\n",
        "\n",
        "        # Fully Connected Layers\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# 2D CNN model for tabular data\n",
        "class RevisedCNN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, dropout_rate=0.3):\n",
        "        super(RevisedCNN, self).__init__()\n",
        "\n",
        "        # Reshape input to 2D representation\n",
        "        self.input_dim = input_dim\n",
        "        self.reshape_dim = int(np.sqrt(input_dim)) + 1\n",
        "        self.pad_size = self.reshape_dim**2 - input_dim\n",
        "\n",
        "        # Convolutional layers with larger kernel and better stride\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=5, stride=1, padding=2)\n",
        "        self.bn1 = nn.BatchNorm2d(64, track_running_stats=True)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Compute output size after first conv+pool\n",
        "        conv1_out_size = self.reshape_dim // 2\n",
        "\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(128, track_running_stats=True)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Compute output size after second conv+pool\n",
        "        conv2_out_size = conv1_out_size // 2\n",
        "\n",
        "        # Calculate flattened size\n",
        "        self.flattened_size = 128 * max(1, conv2_out_size) * max(1, conv2_out_size)\n",
        "\n",
        "        # Fully connected layers with proper sizes\n",
        "        self.fc1 = nn.Linear(self.flattened_size, 512)\n",
        "        self.bn_fc1 = nn.BatchNorm1d(512, track_running_stats=True)\n",
        "        self.dropout_fc1 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.bn_fc2 = nn.BatchNorm1d(256, track_running_stats=True)\n",
        "        self.dropout_fc2 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        self.fc3 = nn.Linear(256, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Reshape input to 2D (square-like format for better convolution)\n",
        "        x_padded = F.pad(x, (0, self.pad_size))\n",
        "        x = x_padded.view(batch_size, 1, self.reshape_dim, self.reshape_dim)\n",
        "\n",
        "        # Apply convolutional layers\n",
        "        x = self.conv1(x)\n",
        "        if x.size(0) > 1:  # Apply batch norm only if batch size > 1\n",
        "            x = self.bn1(x)\n",
        "        x = F.leaky_relu(x, negative_slope=0.1)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        if x.size(0) > 1:  # Apply batch norm only if batch size > 1\n",
        "            x = self.bn2(x)\n",
        "        x = F.leaky_relu(x, negative_slope=0.1)\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        # Flatten\n",
        "        x = x.view(batch_size, -1)\n",
        "\n",
        "        # Fully connected layers\n",
        "        x = self.fc1(x)\n",
        "        if x.size(0) > 1:  # Apply batch norm only if batch size > 1\n",
        "            x = self.bn_fc1(x)\n",
        "        x = F.leaky_relu(x, negative_slope=0.1)\n",
        "        x = self.dropout_fc1(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        if x.size(0) > 1:  # Apply batch norm only if batch size > 1\n",
        "            x = self.bn_fc2(x)\n",
        "        x = F.leaky_relu(x, negative_slope=0.1)\n",
        "        x = self.dropout_fc2(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Improved RBM with proper pretraining abilities\n",
        "class ImprovedRBM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(ImprovedRBM, self).__init__()\n",
        "\n",
        "        # Initialize weights with small values for stability\n",
        "        self.W = nn.Parameter(torch.randn(input_size, hidden_size) * 0.01)\n",
        "        self.b = nn.Parameter(torch.zeros(hidden_size))\n",
        "        self.c = nn.Parameter(torch.zeros(input_size))\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Xavier/Glorot initialization for better convergence\n",
        "        nn.init.xavier_uniform_(self.W)\n",
        "\n",
        "    def forward(self, v):\n",
        "        \"\"\"Visible to hidden layer probabilities\"\"\"\n",
        "        batch_size = v.size(0)\n",
        "\n",
        "        # Reshape input to 2D if needed\n",
        "        if v.dim() > 2:\n",
        "            v = v.view(batch_size, -1)\n",
        "\n",
        "        # Handle dimension mismatch gracefully\n",
        "        if v.size(1) != self.input_size:\n",
        "            v_resized = torch.zeros(batch_size, self.input_size, device=v.device)\n",
        "            min_size = min(v.size(1), self.input_size)\n",
        "            v_resized[:, :min_size] = v[:, :min_size]\n",
        "            v = v_resized\n",
        "\n",
        "        # Apply L2 normalization for better stability\n",
        "        v = F.normalize(v, p=2, dim=1)\n",
        "\n",
        "        # Compute hidden activations\n",
        "        h_activation = torch.matmul(v, self.W) + self.b\n",
        "        h_probs = torch.sigmoid(h_activation)\n",
        "\n",
        "        return h_probs\n",
        "\n",
        "    def sample_h(self, v):\n",
        "        \"\"\"Sample from the hidden layer given visible state\"\"\"\n",
        "        h_probs = self.forward(v)\n",
        "        h_samples = torch.bernoulli(h_probs)\n",
        "        return h_samples, h_probs\n",
        "\n",
        "    def sample_v(self, h):\n",
        "        \"\"\"Sample from the visible layer given hidden state\"\"\"\n",
        "        v_activation = torch.matmul(h, self.W.t()) + self.c\n",
        "        v_probs = torch.sigmoid(v_activation)\n",
        "        v_samples = torch.bernoulli(v_probs)\n",
        "        return v_samples, v_probs\n",
        "\n",
        "    def cd_k(self, v_data, k=1):\n",
        "        \"\"\"Contrastive Divergence with k steps\"\"\"\n",
        "        h_data, h_data_probs = self.sample_h(v_data)\n",
        "\n",
        "        # Initialize the chain with data samples\n",
        "        h_model = h_data\n",
        "\n",
        "        # Gibbs sampling\n",
        "        for _ in range(k):\n",
        "            v_model, v_model_probs = self.sample_v(h_model)\n",
        "            h_model, h_model_probs = self.sample_h(v_model)\n",
        "\n",
        "        return v_data, h_data_probs, v_model_probs, h_model_probs\n",
        "\n",
        "    def free_energy(self, v):\n",
        "        \"\"\"Calculate free energy\"\"\"\n",
        "        wx_b = torch.matmul(v, self.W) + self.b\n",
        "        hidden_term = torch.sum(F.softplus(wx_b), dim=1)\n",
        "        vbias_term = torch.matmul(v, self.c)\n",
        "        return -hidden_term - vbias_term\n",
        "\n",
        "\n",
        "# Function to pretrain an RBM\n",
        "def pretrain_rbm(rbm, dataloader, device, epochs=5, lr=0.001):\n",
        "    \"\"\"Pretrain RBM using Contrastive Divergence\"\"\"\n",
        "    optimizer = torch.optim.Adam(rbm.parameters(), lr=lr)\n",
        "    rbm.train()\n",
        "\n",
        "    print(f\"Pretraining RBM {rbm.input_size} -> {rbm.hidden_size}...\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        mean_loss = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        for batch_idx, (data, _) in enumerate(dataloader):\n",
        "            # Skip batches with only one sample\n",
        "            if data.size(0) <= 1:\n",
        "                continue\n",
        "\n",
        "            data = data.to(device)\n",
        "\n",
        "            # Run k-step Contrastive Divergence\n",
        "            v_data, h_data, v_model, h_model = rbm.cd_k(data, k=1)\n",
        "\n",
        "            # Compute gradients using CD loss\n",
        "            # Positive phase - negative phase\n",
        "            pos_associations = torch.matmul(v_data.t(), h_data)\n",
        "            neg_associations = torch.matmul(v_model.t(), h_model)\n",
        "\n",
        "            # Update weights and biases\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Manually set gradients\n",
        "            rbm.W.grad = -(pos_associations - neg_associations) / data.size(0)\n",
        "            rbm.b.grad = -(h_data - h_model).mean(0)\n",
        "            rbm.c.grad = -(v_data - v_model).mean(0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            # Compute reconstruction error\n",
        "            recon_error = F.mse_loss(v_model, v_data)\n",
        "            mean_loss += recon_error.item()\n",
        "            num_batches += 1\n",
        "\n",
        "        # Print epoch stats\n",
        "        if num_batches > 0:  # Avoid division by zero\n",
        "            mean_loss /= num_batches\n",
        "        print(f\"  Epoch {epoch+1}/{epochs}, Reconstruction Error: {mean_loss:.6f}\")\n",
        "\n",
        "    return rbm\n",
        "\n",
        "\n",
        "# Improved DBN with proper pretraining\n",
        "class ImprovedDBN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims, output_dim, dropout_rate=0.3):\n",
        "        super(ImprovedDBN, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dims = hidden_dims\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        # Stack of RBM layers\n",
        "        self.rbm_layers = nn.ModuleList()\n",
        "\n",
        "        # First RBM\n",
        "        self.rbm_layers.append(ImprovedRBM(input_dim, hidden_dims[0]))\n",
        "\n",
        "        # Additional RBM layers\n",
        "        for i in range(1, len(hidden_dims)):\n",
        "            self.rbm_layers.append(ImprovedRBM(hidden_dims[i-1], hidden_dims[i]))\n",
        "\n",
        "        # Fully connected layers after RBMs\n",
        "        self.fc_layers = nn.ModuleList()\n",
        "\n",
        "        # First FC layer from last RBM's output\n",
        "        self.fc_layers.append(nn.Sequential(\n",
        "            nn.Linear(hidden_dims[-1], 256),\n",
        "            nn.BatchNorm1d(256, track_running_stats=True),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Dropout(dropout_rate)\n",
        "        ))\n",
        "\n",
        "        # Second FC layer\n",
        "        self.fc_layers.append(nn.Sequential(\n",
        "            nn.Linear(256, 128),\n",
        "            nn.BatchNorm1d(128, track_running_stats=True),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Dropout(dropout_rate)\n",
        "        ))\n",
        "\n",
        "        # Output layer\n",
        "        self.output_layer = nn.Linear(128, output_dim)\n",
        "\n",
        "        # Better initialization for linear layers\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def pretrain(self, dataloader, device, epochs=5):\n",
        "        \"\"\"Pretrain the DBN layer by layer\"\"\"\n",
        "        print(\"Pretraining DBN layers...\")\n",
        "\n",
        "        # Train first RBM with input data\n",
        "        self.rbm_layers[0] = pretrain_rbm(\n",
        "            self.rbm_layers[0], dataloader, device, epochs)\n",
        "\n",
        "        # For each subsequent layer, train with features from previous layer\n",
        "        for i in range(1, len(self.rbm_layers)):\n",
        "            # Extract features from previous layer\n",
        "            prev_layer_features = []\n",
        "            with torch.no_grad():\n",
        "                for data, _ in dataloader:\n",
        "                    # Skip batches with only one sample\n",
        "                    if data.size(0) <= 1:\n",
        "                        continue\n",
        "\n",
        "                    data = data.to(device)\n",
        "\n",
        "                    # Forward pass through previous layers\n",
        "                    for j in range(i):\n",
        "                        data = self.rbm_layers[j](data)\n",
        "\n",
        "                    prev_layer_features.append(data.cpu())\n",
        "\n",
        "            # Create dataset with extracted features\n",
        "            if len(prev_layer_features) > 0:  # Make sure we have features\n",
        "                prev_features = torch.cat(prev_layer_features, dim=0)\n",
        "                feature_dataset = TensorDataset(prev_features, torch.zeros(prev_features.size(0)))\n",
        "                feature_loader = DataLoader(\n",
        "                    feature_dataset,\n",
        "                    batch_size=dataloader.batch_size,\n",
        "                    shuffle=True,\n",
        "                    num_workers=0\n",
        "                )\n",
        "\n",
        "                # Pretrain this RBM with extracted features\n",
        "                self.rbm_layers[i] = pretrain_rbm(\n",
        "                    self.rbm_layers[i], feature_loader, device, epochs)\n",
        "            else:\n",
        "                print(f\"Warning: No features extracted for layer {i}. Skipping pretraining.\")\n",
        "\n",
        "        print(\"DBN pretraining complete.\")\n",
        "        return self\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass through the DBN\"\"\"\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Flatten input if not already flat\n",
        "        if x.dim() > 2:\n",
        "            x = x.view(batch_size, -1)\n",
        "\n",
        "        # Forward through RBM layers\n",
        "        for rbm in self.rbm_layers:\n",
        "            x = rbm(x)\n",
        "\n",
        "        # Forward through FC layers with batch size check\n",
        "        for fc in self.fc_layers:\n",
        "            # Handle batch normalization in the sequential module\n",
        "            if batch_size > 1:\n",
        "                x = fc(x)\n",
        "            else:\n",
        "                # Apply each component manually, skipping batch norm if necessary\n",
        "                for layer in fc:\n",
        "                    if isinstance(layer, nn.BatchNorm1d):\n",
        "                        continue\n",
        "                    x = layer(x)\n",
        "\n",
        "        # Output layer\n",
        "        x = self.output_layer(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Focal Loss implementation for handling class imbalance\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha  # Class weights\n",
        "        self.gamma = gamma  # Focusing parameter\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        # Standard cross entropy\n",
        "        ce_loss = F.cross_entropy(\n",
        "            inputs, targets, weight=self.alpha,\n",
        "            reduction='none'\n",
        "        )\n",
        "\n",
        "        # Get probabilities\n",
        "        pt = torch.exp(-ce_loss)\n",
        "\n",
        "        # Calculate focal term\n",
        "        focal_term = (1 - pt) ** self.gamma\n",
        "\n",
        "        # Apply focal term to CE loss\n",
        "        loss = focal_term * ce_loss\n",
        "\n",
        "        # Apply reduction\n",
        "        if self.reduction == 'mean':\n",
        "            return loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return loss.sum()\n",
        "        else:\n",
        "            return loss\n",
        "\n",
        "\n",
        "# Super Ensemble Model: Combines CNN-LSTM, 2D CNN, and DBN\n",
        "class SuperEnsembleModel(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, hidden_dims=[512, 384, 256], dropout_rate=0.3):\n",
        "        super(SuperEnsembleModel, self).__init__()\n",
        "\n",
        "        # Create component models\n",
        "        self.cnn_lstm = CNN_LSTM(input_size=input_dim, num_classes=output_dim)\n",
        "        self.revised_cnn = RevisedCNN(input_dim=input_dim, output_dim=output_dim, dropout_rate=dropout_rate)\n",
        "        self.dbn_model = ImprovedDBN(input_dim=input_dim, hidden_dims=hidden_dims, output_dim=output_dim, dropout_rate=dropout_rate)\n",
        "\n",
        "        # Number of base models\n",
        "        self.num_models = 3\n",
        "\n",
        "        # Attention module for model weighting\n",
        "        self.model_attention = nn.Sequential(\n",
        "            nn.Linear(output_dim * self.num_models, 128),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Linear(128, self.num_models),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "        # Class-wise attention - one attention weight per class\n",
        "        self.class_attention = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(self.num_models, 32),\n",
        "                nn.LeakyReLU(0.1),\n",
        "                nn.Linear(32, self.num_models),\n",
        "                nn.Softmax(dim=1)\n",
        "            ) for _ in range(output_dim)\n",
        "        ])\n",
        "\n",
        "        # Meta-learner for final predictions\n",
        "        self.meta_learner = nn.Sequential(\n",
        "            nn.Linear(output_dim * self.num_models, 512),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, output_dim)\n",
        "        )\n",
        "\n",
        "    def pretrain_dbn(self, dataloader, device, epochs=3):\n",
        "        \"\"\"Pretrain the DBN component of the ensemble\"\"\"\n",
        "        return self.dbn_model.pretrain(dataloader, device, epochs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Get predictions from all models\n",
        "        cnn_lstm_out = self.cnn_lstm(x)\n",
        "        cnn_out = self.revised_cnn(x)\n",
        "        dbn_out = self.dbn_model(x)\n",
        "\n",
        "        # Concatenate all outputs for meta-learner\n",
        "        all_outputs = torch.cat([cnn_lstm_out, cnn_out, dbn_out], dim=1)\n",
        "\n",
        "        # Calculate global attention weights\n",
        "        model_weights = self.model_attention(all_outputs)\n",
        "\n",
        "        # Calculate class-wise attention weights\n",
        "        class_outputs = []\n",
        "\n",
        "        for i in range(self.cnn_lstm.fc3.out_features):  # Number of classes\n",
        "            # Get predictions for this class from all models\n",
        "            class_values = torch.cat([\n",
        "                cnn_lstm_out[:, i:i+1],\n",
        "                cnn_out[:, i:i+1],\n",
        "                dbn_out[:, i:i+1]\n",
        "            ], dim=1)\n",
        "\n",
        "            # Calculate class-specific weights\n",
        "            class_weights = self.class_attention[i](class_values)\n",
        "\n",
        "            # Apply weights to get weighted output for this class\n",
        "            weighted_class = torch.sum(class_values * class_weights, dim=1, keepdim=True)\n",
        "            class_outputs.append(weighted_class)\n",
        "\n",
        "        # Combine class outputs\n",
        "        class_weighted_output = torch.cat(class_outputs, dim=1)\n",
        "\n",
        "        # Apply meta-learner to get final predictions\n",
        "        meta_output = self.meta_learner(all_outputs)\n",
        "\n",
        "        # Dynamic combination of class-weighted output and meta-learner output\n",
        "        # The model weights determine how much to rely on each approach\n",
        "        global_weight = model_weights.mean(dim=1, keepdim=True)\n",
        "        final_output = (global_weight * meta_output +\n",
        "                        (1 - global_weight) * class_weighted_output)\n",
        "\n",
        "        return final_output\n",
        "\n",
        "\n",
        "# Advanced training function for the super ensemble\n",
        "def train_super_ensemble(model, train_loader, val_loader, test_loader,\n",
        "                        device, class_weights=None, num_epochs=30,\n",
        "                        save_dir='model_checkpoints'):\n",
        "    # Create save directory if it doesn't exist\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    print(\"🔥 Starting super ensemble training process...\")\n",
        "\n",
        "    # Setup optimizer with weight decay\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
        "\n",
        "    # Use Focal Loss if class_weights provided, otherwise CrossEntropy\n",
        "    if class_weights is not None:\n",
        "        class_weights = class_weights.to(device)\n",
        "        criterion = FocalLoss(alpha=class_weights, gamma=2.0)\n",
        "        print(\"📊 Using Focal Loss with class weights\")\n",
        "    else:\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        print(\"📊 Using standard CrossEntropyLoss\")\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=3, verbose=True, min_lr=1e-6\n",
        "    )\n",
        "\n",
        "    # Tracking metrics\n",
        "    best_val_acc = 0.0\n",
        "    patience = 5  # Early stopping patience\n",
        "    trigger_times = 0  # Counter for early stopping\n",
        "\n",
        "    # For plotting\n",
        "    train_losses = []\n",
        "    train_accs = []\n",
        "    val_losses = []\n",
        "    val_accs = []\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        start_time = time.time()\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        # Wrap train_loader with tqdm for a progress bar\n",
        "        train_loader_tqdm = tqdm(\n",
        "            train_loader,\n",
        "            desc=f\"Epoch {epoch+1}/{num_epochs}\",\n",
        "            unit=\"batch\",\n",
        "            bar_format=\"{l_bar}{bar:20}{r_bar}\",  # Customize the bar format\n",
        "            ascii=True  # Use ASCII characters for the bar\n",
        "        )\n",
        "\n",
        "        for batch_idx, (inputs, labels) in enumerate(train_loader_tqdm):\n",
        "            # Skip batches with only one sample\n",
        "            if inputs.size(0) <= 1:\n",
        "                continue\n",
        "\n",
        "            # Move data to device\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping to avoid exploding gradients\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update running loss and accuracy\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            # Update progress bar description\n",
        "            train_loader_tqdm.set_postfix(\n",
        "                loss=loss.item(),\n",
        "                accuracy=f\"{(correct / total) * 100:.2f}%\"\n",
        "            )\n",
        "\n",
        "        # Calculate training metrics\n",
        "        epoch_train_loss = running_loss / max(1, len(train_loader))\n",
        "        epoch_train_acc = 100.0 * correct / max(1, total)\n",
        "        train_losses.append(epoch_train_loss)\n",
        "        train_accs.append(epoch_train_acc)\n",
        "\n",
        "        # Print training statistics\n",
        "        print(f\"\\n✅ Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_train_loss:.4f}, Accuracy: {epoch_train_acc:.2f}%\")\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        all_preds = []\n",
        "        all_targets = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Wrap val_loader with tqdm for a progress bar\n",
        "            val_loader_tqdm = tqdm(\n",
        "                val_loader,\n",
        "                desc=f\"Validating\",\n",
        "                unit=\"batch\",\n",
        "                bar_format=\"{l_bar}{bar:20}{r_bar}\",\n",
        "                ascii=True\n",
        "            )\n",
        "\n",
        "            for inputs, labels in val_loader_tqdm:\n",
        "                # Skip batches with only one sample\n",
        "                if inputs.size(0) <= 1:\n",
        "                    continue\n",
        "\n",
        "                # Move data to device\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                # Update validation statistics\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "                # Save predictions for confusion matrix\n",
        "                all_preds.extend(predicted.cpu().numpy())\n",
        "                all_targets.extend(labels.cpu().numpy())\n",
        "\n",
        "                # Update progress bar\n",
        "                val_loader_tqdm.set_postfix(\n",
        "                    loss=loss.item(),\n",
        "                    accuracy=f\"{(val_correct / val_total) * 100:.2f}%\"\n",
        "                )\n",
        "\n",
        "        # Calculate validation metrics\n",
        "        epoch_val_loss = val_loss / max(1, len(val_loader))\n",
        "        epoch_val_acc = 100.0 * val_correct / max(1, val_total)\n",
        "        val_losses.append(epoch_val_loss)\n",
        "        val_accs.append(epoch_val_acc)\n",
        "\n",
        "        # Print validation statistics\n",
        "        print(f\"🔹 Validation Loss: {epoch_val_loss:.4f}, Accuracy: {epoch_val_acc:.2f}%\")\n",
        "\n",
        "        # Update learning rate scheduler\n",
        "        scheduler.step(epoch_val_loss)\n",
        "\n",
        "        # Check if this is the best model so far\n",
        "        if epoch_val_acc > best_val_acc:\n",
        "            best_val_acc = epoch_val_acc\n",
        "\n",
        "            # Save the best model\n",
        "            torch.save({\n",
        "                'epoch': epoch + 1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': epoch_val_loss,\n",
        "                'accuracy': epoch_val_acc\n",
        "            }, os.path.join(save_dir, \"best_model.pth\"))\n",
        "\n",
        "            print(f\"💾 Best Model Saved: best_model.pth (Accuracy: {epoch_val_acc:.2f}%)\")\n",
        "\n",
        "            # Reset early stopping counter\n",
        "            trigger_times = 0\n",
        "\n",
        "            # Generate confusion matrix for best model\n",
        "            if len(all_preds) > 0 and len(all_targets) > 0:\n",
        "                try:\n",
        "                    cm = confusion_matrix(all_targets, all_preds)\n",
        "                    plt.figure(figsize=(12, 10))\n",
        "                    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "                    plt.title(f'Validation Confusion Matrix - Epoch {epoch+1}')\n",
        "                    plt.xlabel('Predicted')\n",
        "                    plt.ylabel('True')\n",
        "                    plt.savefig(os.path.join(save_dir, f'confusion_matrix_epoch_{epoch+1}.png'))\n",
        "                    plt.close()\n",
        "\n",
        "                    # Print classification report\n",
        "                    print(\"\\n📋 Classification Report:\")\n",
        "                    print(classification_report(all_targets, all_preds))\n",
        "                except Exception as e:\n",
        "                    print(f\"⚠️ Could not generate confusion matrix: {e}\")\n",
        "        else:\n",
        "            # Increment early stopping counter\n",
        "            trigger_times += 1\n",
        "            if trigger_times >= patience:\n",
        "                print(\"⛔ Early Stopping Triggered!\")\n",
        "                break\n",
        "\n",
        "    # Final evaluation on test set\n",
        "    print(\"\\n🧪 Evaluating final model on test set...\")\n",
        "\n",
        "    # Load best model\n",
        "    try:\n",
        "        checkpoint = torch.load(os.path.join(save_dir, \"best_model.pth\"))\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        print(f\"📂 Loaded best model from epoch {checkpoint['epoch']}\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Could not load best model: {e}. Using current model state.\")\n",
        "\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    test_correct = 0\n",
        "    test_total = 0\n",
        "    all_test_preds = []\n",
        "    all_test_targets = []\n",
        "    all_test_scores = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        test_loader_tqdm = tqdm(\n",
        "            test_loader,\n",
        "            desc=\"Testing\",\n",
        "            unit=\"batch\",\n",
        "            bar_format=\"{l_bar}{bar:20}{r_bar}\",\n",
        "            ascii=True\n",
        "        )\n",
        "\n",
        "        for inputs, labels in test_loader_tqdm:\n",
        "            # Move data to device\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Update test statistics\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            test_total += labels.size(0)\n",
        "            test_correct += (predicted == labels).sum().item()\n",
        "\n",
        "            # Save predictions for metrics\n",
        "            all_test_preds.extend(predicted.cpu().numpy())\n",
        "            all_test_targets.extend(labels.cpu().numpy())\n",
        "            all_test_scores.extend(F.softmax(outputs, dim=1).cpu().numpy())\n",
        "\n",
        "            # Update progress bar\n",
        "            test_loader_tqdm.set_postfix(\n",
        "                loss=loss.item(),\n",
        "                accuracy=f\"{(test_correct / test_total) * 100:.2f}%\"\n",
        "            )\n",
        "\n",
        "    # Calculate test accuracy\n",
        "    test_acc = 100.0 * test_correct / max(1, test_total)\n",
        "    print(f\"🏆 Test Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "    # Calculate detailed metrics\n",
        "    from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
        "\n",
        "    if len(all_test_preds) > 0 and len(all_test_targets) > 0:\n",
        "        # Convert to arrays\n",
        "        y_true = np.array(all_test_targets)\n",
        "        y_pred = np.array(all_test_preds)\n",
        "        y_score = np.array(all_test_scores)\n",
        "\n",
        "        # Calculate precision, recall, and F1-score\n",
        "        precision, recall, f1, support = precision_recall_fscore_support(\n",
        "            y_true, y_pred, average=None)\n",
        "\n",
        "        # Calculate weighted metrics\n",
        "        weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(\n",
        "            y_true, y_pred, average='weighted')\n",
        "\n",
        "        # Try to calculate ROC AUC score\n",
        "        try:\n",
        "            roc_auc = roc_auc_score(y_true, y_score, multi_class='ovr', average='weighted')\n",
        "            print(f\"📈 ROC AUC Score (weighted): {roc_auc:.4f}\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Could not calculate ROC AUC: {e}\")\n",
        "\n",
        "        print(f\"📏 Weighted Precision: {weighted_precision:.4f}\")\n",
        "        print(f\"📏 Weighted Recall: {weighted_recall:.4f}\")\n",
        "        print(f\"📏 Weighted F1 Score: {weighted_f1:.4f}\")\n",
        "\n",
        "        # Generate final confusion matrix\n",
        "        try:\n",
        "            cm = confusion_matrix(all_test_targets, all_test_preds)\n",
        "            plt.figure(figsize=(14, 12))\n",
        "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "            plt.title('Final Test Confusion Matrix')\n",
        "            plt.xlabel('Predicted')\n",
        "            plt.ylabel('True')\n",
        "            plt.savefig(os.path.join(save_dir, 'final_test_confusion_matrix.png'))\n",
        "            plt.close()\n",
        "\n",
        "            # Per-class metrics visualization\n",
        "            plt.figure(figsize=(14, 8))\n",
        "            indices = np.arange(len(precision))\n",
        "            width = 0.25\n",
        "\n",
        "            plt.bar(indices - width, precision, width, label='Precision')\n",
        "            plt.bar(indices, recall, width, label='Recall')\n",
        "            plt.bar(indices + width, f1, width, label='F1-Score')\n",
        "\n",
        "            plt.xlabel('Class')\n",
        "            plt.ylabel('Score')\n",
        "            plt.title('Per-Class Classification Metrics')\n",
        "            plt.xticks(indices)\n",
        "            plt.legend()\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(save_dir, 'per_class_metrics.png'))\n",
        "            plt.close()\n",
        "\n",
        "            # Print final classification report\n",
        "            print(\"\\n📋 Final Classification Report:\")\n",
        "            print(classification_report(all_test_targets, all_test_preds))\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Could not generate final confusion matrix: {e}\")\n",
        "\n",
        "    # Plot training history\n",
        "    plt.figure(figsize=(15, 6))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.title('Loss over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(train_accs, label='Training Accuracy')\n",
        "    plt.plot(val_accs, label='Validation Accuracy')\n",
        "    plt.title('Accuracy over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(save_dir, 'training_history.png'))\n",
        "    plt.close()\n",
        "\n",
        "    print(\"\\n✅ Training Complete!\")\n",
        "    return model, test_acc"
      ],
      "metadata": {
        "id": "OriKRkGhYP-C",
        "outputId": "f9f540bb-34e5-4458-f0ec-daf707f99f4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Using Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running Preprocessing dan CNN+DBN"
      ],
      "metadata": {
        "id": "mXGcyX409kyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Main function to run the whole pipeline\n",
        "# def run_improved_iot_anomaly_detection(train_data, test_data, save_dir='model_results'):\n",
        "#     print(\"\\n🚀 Starting improved IoT network anomaly detection pipeline...\")\n",
        "\n",
        "#     # Step 1: Preprocessing data\n",
        "#     print(\"\\n📊 Running enhanced data preprocessing...\")\n",
        "#     processed_data = enhanced_preprocessing(train_data, test_data)\n",
        "\n",
        "#     # Extract necessary components from processed data\n",
        "#     train_loader = processed_data['train_loader']\n",
        "#     val_loader = processed_data['val_loader']\n",
        "#     test_loader = processed_data['test_loader']\n",
        "#     input_dim = processed_data['input_dim']\n",
        "#     num_classes = processed_data['num_classes']\n",
        "#     class_weights = processed_data['class_weights']\n",
        "#     train_data = processed_data['train_data']  # Get updated train_data\n",
        "#     test_data = processed_data['test_data']\n",
        "\n",
        "#     # Step 2: Set up device for training\n",
        "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#     class_weights = class_weights.to(device)\n",
        "#     print(f\"\\n🔧 Using device: {device}\")\n",
        "\n",
        "#     # Step 3: Create improved CNN model\n",
        "#     print(\"\\n🧠 Creating improved CNN model...\")\n",
        "#     cnn_model = RevisedCNN(\n",
        "#         input_dim=input_dim,\n",
        "#         output_dim=num_classes,\n",
        "#         dropout_rate=0.3\n",
        "#         # input_size = input_dim,\n",
        "#         # num_classes = num_classes\n",
        "#     ).to(device)\n",
        "#     print(f\"CNN model created with {input_dim} input features and {num_classes} output classes\")\n",
        "\n",
        "#     # Step 4: Create improved DBN model with pretraining\n",
        "#     print(\"\\n🧠 Creating improved DBN model...\")\n",
        "#     dbn_model = ImprovedDBN(\n",
        "#         input_dim=input_dim,\n",
        "#         hidden_dims=[512, 384, 256],\n",
        "#         output_dim=num_classes,\n",
        "#         dropout_rate=0.3\n",
        "#     ).to(device)\n",
        "\n",
        "#     # Pretrain DBN layers\n",
        "#     dbn_model.pretrain(train_loader, device, epochs=3)\n",
        "#     print(\"DBN model created and pretrained\")\n",
        "\n",
        "#     # Step 5: Create improved ensemble model\n",
        "#     print(\"\\n🧠 Creating improved dynamic ensemble model...\")\n",
        "#     ensemble_model = ImprovedDynamicEnsembleModel(\n",
        "#         cnn_model=cnn_model,\n",
        "#         dbn_model=dbn_model,\n",
        "#         output_dim=num_classes\n",
        "#     ).to(device)\n",
        "#     print(\"Ensemble model created\")\n",
        "\n",
        "#     # Step 6: Train the ensemble model\n",
        "#     print(\"\\n🏋️‍♀️ Starting advanced training process...\")\n",
        "#     trained_model, test_accuracy = train_ensemble_model(\n",
        "#         ensemble_model=ensemble_model,\n",
        "#         train_loader=train_loader,\n",
        "#         val_loader=val_loader,\n",
        "#         test_loader=test_loader,\n",
        "#         device=device,\n",
        "#         class_weights=class_weights,\n",
        "#         epochs=30,\n",
        "#         lr=0.001,\n",
        "#         focal_gamma=2.0,\n",
        "#         mixup_alpha=0.2,\n",
        "#         save_dir=save_dir\n",
        "#     )\n",
        "\n",
        "#     print(f\"\\n🏁 Final test accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "#     # Return the trained model and results\n",
        "#     results = {\n",
        "#         'model': trained_model,\n",
        "#         'test_accuracy': test_accuracy,\n",
        "#         'input_dim': input_dim,\n",
        "#         'num_classes': num_classes,\n",
        "#         'model_path': os.path.join(save_dir, 'best_model.pt')\n",
        "#     }\n",
        "\n",
        "#     return results\n",
        "\n",
        "\n",
        "# Main function to run the whole pipeline with Super Ensemble\n",
        "def run_improved_iot_anomaly_detection(train_data, test_data, save_dir='super_ensemble_results'):\n",
        "    print(\"\\n🚀 Starting improved IoT network anomaly detection pipeline with Super Ensemble...\")\n",
        "\n",
        "    # Step 1: Preprocessing data\n",
        "    print(\"\\n📊 Running enhanced data preprocessing...\")\n",
        "    processed_data = enhanced_preprocessing(train_data, test_data)\n",
        "\n",
        "    # Extract necessary components from processed data\n",
        "    train_loader = processed_data['train_loader']\n",
        "    val_loader = processed_data['val_loader']\n",
        "    test_loader = processed_data['test_loader']\n",
        "    input_dim = processed_data['input_dim']\n",
        "    num_classes = processed_data['num_classes']\n",
        "    class_weights = processed_data['class_weights']\n",
        "    train_data = processed_data['train_data']\n",
        "    test_data = processed_data['test_data']\n",
        "\n",
        "    # Step 2: Set up device for training\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"\\n🔧 Using device: {device}\")\n",
        "    class_weights = class_weights.to(device)\n",
        "\n",
        "    # Step 3: Create Super Ensemble model (CNN-LSTM + 2D CNN + DBN)\n",
        "    print(\"\\n🧠 Creating Super Ensemble model...\")\n",
        "    super_ensemble = SuperEnsembleModel(\n",
        "        input_dim=input_dim,\n",
        "        output_dim=num_classes,\n",
        "        hidden_dims=[512, 384, 256],\n",
        "        dropout_rate=0.3\n",
        "    ).to(device)\n",
        "\n",
        "    # Step 4: Pretrain DBN component\n",
        "    print(\"\\n🧪 Pretraining DBN component...\")\n",
        "    super_ensemble.pretrain_dbn(train_loader, device, epochs=3)\n",
        "\n",
        "    print(f\"Super Ensemble model created with {input_dim} input features and {num_classes} output classes\")\n",
        "\n",
        "    # Step 5: Train the Super Ensemble model\n",
        "    print(\"\\n🏋️‍♀️ Starting Super Ensemble training...\")\n",
        "    trained_model, test_accuracy = train_super_ensemble(\n",
        "        model=super_ensemble,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        test_loader=test_loader,\n",
        "        device=device,\n",
        "        class_weights=class_weights,\n",
        "        num_epochs=50,\n",
        "        save_dir=save_dir\n",
        "    )\n",
        "\n",
        "    print(f\"\\n🏁 Final test accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "    # Return the trained model and results\n",
        "    results = {\n",
        "        'model': trained_model,\n",
        "        'test_accuracy': test_accuracy,\n",
        "        'input_dim': input_dim,\n",
        "        'num_classes': num_classes,\n",
        "        'model_path': os.path.join(save_dir, 'best_model.pth')\n",
        "    }\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Load dataset UNSW-NB15\n",
        "# Asumsikan Anda memiliki file CSV untuk training dan testing\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/ANOMALI DUNDUN/UNSW_NB15_training-set.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/ANOMALI DUNDUN/UNSW_NB15_testing-set.csv')\n",
        "\n",
        "# 2. Jalankan pipeline pemrosesan dan model\n",
        "results = run_improved_iot_anomaly_detection(train_data, test_data, save_dir='iot_anomaly_results')\n",
        "\n",
        "print(f\"Final model accuracy: {results['test_accuracy']:.2f}%\")\n",
        "print(f\"Model saved to: {results['model_path']}\")"
      ],
      "metadata": {
        "id": "_Az7ztq69efe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b0c048c-64da-410f-de30-c35e81f1b463"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 Starting improved IoT network anomaly detection pipeline with Super Ensemble...\n",
            "\n",
            "📊 Running enhanced data preprocessing...\n",
            "\n",
            "🔍 Starting enhanced preprocessing pipeline...\n",
            "\n",
            "📊 Dataset Overview:\n",
            "Training data shape: (82332, 45)\n",
            "Testing data shape: (175341, 45)\n",
            "Missing values - Training: 0, Testing: 0\n",
            "\n",
            "🔍 Handling missing target values...\n",
            "After dropping rows with missing targets - Train: (82332, 45), Test: (175341, 45)\n",
            "\n",
            "🔍 Handling remaining missing values...\n",
            "\n",
            "🔍 Removing duplicate entries...\n",
            "After removing duplicates - Train: (82332, 45), Test: (175341, 45)\n",
            "\n",
            "🔍 Encoding target variable...\n",
            "Class distribution after encoding:\n",
            "  Class 0: 37000 samples (44.94%)\n",
            "  Class 1: 18871 samples (22.92%)\n",
            "  Class 2: 11132 samples (13.52%)\n",
            "  Class 3: 6062 samples (7.36%)\n",
            "  Class 4: 4089 samples (4.97%)\n",
            "  Class 5: 3496 samples (4.25%)\n",
            "  Class 6: 677 samples (0.82%)\n",
            "  Class 7: 583 samples (0.71%)\n",
            "  Class 8: 378 samples (0.46%)\n",
            "  Class 9: 44 samples (0.05%)\n",
            "\n",
            "🔍 Handling outliers...\n",
            "After outlier removal - Train: (61667, 45), Test: (136484, 45)\n",
            "\n",
            "🔍 Applying transformations for skewed numerical features...\n",
            "  Applied log transform to dur (skewness: 6.17)\n",
            "  Applied log transform to spkts (skewness: 7.51)\n",
            "  Applied log transform to dpkts (skewness: 8.02)\n",
            "  Applied log transform to sbytes (skewness: 6.64)\n",
            "  Applied log transform to dbytes (skewness: 5.94)\n",
            "  Applied log transform to rate (skewness: 1.79)\n",
            "  Applied log transform to sload (skewness: 2.69)\n",
            "  Applied log transform to dload (skewness: 5.78)\n",
            "  Applied log transform to sloss (skewness: 6.46)\n",
            "  Applied log transform to dloss (skewness: 7.66)\n",
            "  Applied log transform to sinpkt (skewness: 20.81)\n",
            "  Applied log transform to dinpkt (skewness: 8.01)\n",
            "  Applied log transform to sjit (skewness: 8.10)\n",
            "  Applied log transform to djit (skewness: 5.31)\n",
            "  Applied log transform to synack (skewness: 1.52)\n",
            "  Applied log transform to smean (skewness: 3.52)\n",
            "  Applied log transform to dmean (skewness: 3.21)\n",
            "  Applied log transform to trans_depth (skewness: 2.74)\n",
            "  Applied log transform to response_body_len (skewness: 6.69)\n",
            "  Applied log transform to ct_srv_src (skewness: 1.91)\n",
            "  Applied log transform to ct_dst_ltm (skewness: 2.44)\n",
            "  Applied log transform to ct_src_dport_ltm (skewness: 2.74)\n",
            "  Applied log transform to ct_dst_sport_ltm (skewness: 2.91)\n",
            "  Applied log transform to ct_dst_src_ltm (skewness: 2.67)\n",
            "  Applied log transform to ct_flw_http_mthd (skewness: 2.75)\n",
            "  Applied log transform to ct_src_ltm (skewness: 1.92)\n",
            "  Applied log transform to ct_srv_dst (skewness: 1.97)\n",
            "\n",
            "🔍 Encoding categorical features...\n",
            "  Encoded 3 categorical features into 150 binary features\n",
            "\n",
            "🔍 Performing feature selection with RandomForest...\n",
            "  Selected 32 features after feature selection\n",
            "  Remaining numerical columns after feature selection: 32\n",
            "\n",
            "🔍 Applying scaling to numerical features...\n",
            "  Scaled 32 numerical features\n",
            "\n",
            "🔍 Preparing for resampling...\n",
            "  Class distribution before resampling:\n",
            "    Class 0: 26641 samples (43.20%)\n",
            "    Class 1: 13012 samples (21.10%)\n",
            "    Class 2: 8432 samples (13.67%)\n",
            "    Class 3: 4924 samples (7.98%)\n",
            "    Class 4: 3656 samples (5.93%)\n",
            "    Class 5: 3395 samples (5.51%)\n",
            "    Class 6: 657 samples (1.07%)\n",
            "    Class 7: 557 samples (0.90%)\n",
            "    Class 8: 358 samples (0.58%)\n",
            "    Class 9: 35 samples (0.06%)\n",
            "\n",
            "🔍 Applying resampling...\n",
            "  Target sample counts per class:\n",
            "    Class 0: 18500 samples\n",
            "    Class 1: 8000 samples\n",
            "    Class 2: 8000 samples\n",
            "    Class 3: 4924 samples\n",
            "    Class 4: 3656 samples\n",
            "    Class 5: 3395 samples\n",
            "    Class 6: 657 samples\n",
            "    Class 7: 557 samples\n",
            "    Class 8: 358 samples\n",
            "    Class 9: 35 samples\n",
            "  Class distribution after resampling:\n",
            "    Class 0: 18500 samples (38.48%)\n",
            "    Class 1: 8000 samples (16.64%)\n",
            "    Class 2: 8000 samples (16.64%)\n",
            "    Class 3: 4924 samples (10.24%)\n",
            "    Class 4: 3656 samples (7.60%)\n",
            "    Class 5: 3395 samples (7.06%)\n",
            "    Class 6: 657 samples (1.37%)\n",
            "    Class 7: 557 samples (1.16%)\n",
            "    Class 8: 358 samples (0.74%)\n",
            "    Class 9: 35 samples (0.07%)\n",
            "\n",
            "🔍 Final data quality check...\n",
            "\n",
            "🔍 Creating train/validation split...\n",
            "  Training data: 38465 samples\n",
            "  Validation data: 9617 samples\n",
            "  Test data: 136484 samples\n",
            "\n",
            "🔍 Converting to PyTorch tensors...\n",
            "\n",
            "🔍 Creating PyTorch datasets...\n",
            "\n",
            "🔍 Creating data loaders...\n",
            "\n",
            "✅ Preprocessing complete!\n",
            "  Final training set: 38465 samples with 32 features\n",
            "  Final validation set: 9617 samples\n",
            "  Final test set: 136484 samples with 32 features\n",
            "\n",
            "🔧 Using device: cuda\n",
            "\n",
            "🧠 Creating Super Ensemble model...\n",
            "\n",
            "🧪 Pretraining DBN component...\n",
            "Pretraining DBN layers...\n",
            "Pretraining RBM 32 -> 512...\n",
            "  Epoch 1/3, Reconstruction Error: 0.900850\n",
            "  Epoch 2/3, Reconstruction Error: 0.749342\n",
            "  Epoch 3/3, Reconstruction Error: 0.691753\n",
            "Pretraining RBM 512 -> 384...\n",
            "  Epoch 1/3, Reconstruction Error: 0.042886\n",
            "  Epoch 2/3, Reconstruction Error: 0.007617\n",
            "  Epoch 3/3, Reconstruction Error: 0.006719\n",
            "Pretraining RBM 384 -> 256...\n",
            "  Epoch 1/3, Reconstruction Error: 0.018009\n",
            "  Epoch 2/3, Reconstruction Error: 0.005883\n",
            "  Epoch 3/3, Reconstruction Error: 0.004279\n",
            "DBN pretraining complete.\n",
            "Super Ensemble model created with 32 input features and 10 output classes\n",
            "\n",
            "🏋️‍♀️ Starting Super Ensemble training...\n",
            "🔥 Starting super ensemble training process...\n",
            "📊 Using Focal Loss with class weights\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/50: 100%|####################| 602/602 [00:16<00:00, 36.82batch/s, accuracy=43.23%, loss=0.01]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Epoch [1/50], Loss: 0.0209, Accuracy: 43.23%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|####################| 151/151 [00:02<00:00, 72.96batch/s, accuracy=51.92%, loss=0.00512]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Validation Loss: 0.0136, Accuracy: 51.92%\n",
            "💾 Best Model Saved: best_model.pth (Accuracy: 51.92%)\n",
            "\n",
            "📋 Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.75      0.82      3700\n",
            "           1       0.99      0.97      0.98      1600\n",
            "           2       0.78      0.26      0.39      1600\n",
            "           3       0.89      0.04      0.08       985\n",
            "           4       0.00      0.00      0.00       731\n",
            "           5       0.00      0.00      0.00       679\n",
            "           6       0.07      0.52      0.12       132\n",
            "           7       0.09      0.59      0.15       111\n",
            "           8       0.04      0.92      0.08        72\n",
            "           9       0.01      0.86      0.01         7\n",
            "\n",
            "    accuracy                           0.52      9617\n",
            "   macro avg       0.38      0.49      0.26      9617\n",
            "weighted avg       0.73      0.52      0.55      9617\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/50: 100%|####################| 602/602 [00:16<00:00, 36.58batch/s, accuracy=55.47%, loss=0.0413]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Epoch [2/50], Loss: 0.0101, Accuracy: 55.47%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|####################| 151/151 [00:01<00:00, 89.94batch/s, accuracy=57.61%, loss=0.00491] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Validation Loss: 0.0143, Accuracy: 57.61%\n",
            "💾 Best Model Saved: best_model.pth (Accuracy: 57.61%)\n",
            "\n",
            "📋 Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.86      0.86      3700\n",
            "           1       0.99      0.97      0.98      1600\n",
            "           2       0.82      0.26      0.40      1600\n",
            "           3       0.92      0.17      0.28       985\n",
            "           4       1.00      0.00      0.01       731\n",
            "           5       0.00      0.00      0.00       679\n",
            "           6       0.05      0.37      0.09       132\n",
            "           7       0.11      0.88      0.19       111\n",
            "           8       0.04      0.92      0.09        72\n",
            "           9       0.01      0.71      0.03         7\n",
            "\n",
            "    accuracy                           0.58      9617\n",
            "   macro avg       0.48      0.51      0.29      9617\n",
            "weighted avg       0.81      0.58      0.59      9617\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/50: 100%|####################| 602/602 [00:17<00:00, 34.78batch/s, accuracy=58.21%, loss=0.00935]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Epoch [3/50], Loss: 0.0087, Accuracy: 58.21%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|####################| 151/151 [00:01<00:00, 81.37batch/s, accuracy=57.53%, loss=0.00485]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Validation Loss: 0.0135, Accuracy: 57.53%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/50: 100%|####################| 602/602 [00:18<00:00, 32.38batch/s, accuracy=59.66%, loss=0.0074]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Epoch [4/50], Loss: 0.0084, Accuracy: 59.66%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|####################| 151/151 [00:01<00:00, 95.71batch/s, accuracy=56.80%, loss=0.00441] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Validation Loss: 0.0141, Accuracy: 56.80%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/50: 100%|####################| 602/602 [00:16<00:00, 36.82batch/s, accuracy=60.95%, loss=0.00764]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Epoch [5/50], Loss: 0.0073, Accuracy: 60.95%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|####################| 151/151 [00:01<00:00, 88.83batch/s, accuracy=59.11%, loss=0.00457] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Validation Loss: 0.0135, Accuracy: 59.11%\n",
            "💾 Best Model Saved: best_model.pth (Accuracy: 59.11%)\n",
            "\n",
            "📋 Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.76      0.84      3700\n",
            "           1       0.99      0.97      0.98      1600\n",
            "           2       0.75      0.44      0.56      1600\n",
            "           3       0.56      0.40      0.46       985\n",
            "           4       0.40      0.02      0.03       731\n",
            "           5       0.46      0.03      0.05       679\n",
            "           6       0.05      0.36      0.09       132\n",
            "           7       0.11      0.83      0.19       111\n",
            "           8       0.05      0.96      0.09        72\n",
            "           9       0.03      0.71      0.06         7\n",
            "\n",
            "    accuracy                           0.59      9617\n",
            "   macro avg       0.43      0.55      0.33      9617\n",
            "weighted avg       0.77      0.59      0.64      9617\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/50: 100%|####################| 602/602 [00:17<00:00, 33.60batch/s, accuracy=61.83%, loss=0.0091]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Epoch [6/50], Loss: 0.0072, Accuracy: 61.83%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|####################| 151/151 [00:01<00:00, 91.26batch/s, accuracy=62.60%, loss=0.00416] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Validation Loss: 0.0121, Accuracy: 62.60%\n",
            "💾 Best Model Saved: best_model.pth (Accuracy: 62.60%)\n",
            "\n",
            "📋 Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.85      0.89      3700\n",
            "           1       0.99      0.97      0.98      1600\n",
            "           2       0.78      0.41      0.54      1600\n",
            "           3       0.77      0.43      0.55       985\n",
            "           4       0.46      0.04      0.07       731\n",
            "           5       0.50      0.00      0.01       679\n",
            "           6       0.06      0.35      0.10       132\n",
            "           7       0.12      0.90      0.20       111\n",
            "           8       0.05      0.97      0.10        72\n",
            "           9       0.03      0.71      0.05         7\n",
            "\n",
            "    accuracy                           0.63      9617\n",
            "   macro avg       0.47      0.56      0.35      9617\n",
            "weighted avg       0.80      0.63      0.66      9617\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/50: 100%|####################| 602/602 [00:17<00:00, 35.36batch/s, accuracy=62.13%, loss=0.00254]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Epoch [7/50], Loss: 0.0067, Accuracy: 62.13%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|####################| 151/151 [00:01<00:00, 90.82batch/s, accuracy=63.93%, loss=0.0045] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Validation Loss: 0.0140, Accuracy: 63.93%\n",
            "💾 Best Model Saved: best_model.pth (Accuracy: 63.93%)\n",
            "\n",
            "📋 Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.84      0.89      3700\n",
            "           1       0.99      0.97      0.98      1600\n",
            "           2       0.76      0.45      0.56      1600\n",
            "           3       0.70      0.51      0.59       985\n",
            "           4       0.44      0.05      0.08       731\n",
            "           5       0.57      0.02      0.04       679\n",
            "           6       0.06      0.36      0.10       132\n",
            "           7       0.12      0.91      0.22       111\n",
            "           8       0.06      0.93      0.10        72\n",
            "           9       0.04      0.71      0.08         7\n",
            "\n",
            "    accuracy                           0.64      9617\n",
            "   macro avg       0.47      0.57      0.36      9617\n",
            "weighted avg       0.80      0.64      0.67      9617\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/50: 100%|####################| 602/602 [00:17<00:00, 34.58batch/s, accuracy=63.20%, loss=0.00615]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Epoch [8/50], Loss: 0.0063, Accuracy: 63.20%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|####################| 151/151 [00:01<00:00, 90.52batch/s, accuracy=65.41%, loss=0.00357] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Validation Loss: 0.0125, Accuracy: 65.41%\n",
            "💾 Best Model Saved: best_model.pth (Accuracy: 65.41%)\n",
            "\n",
            "📋 Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.91      0.91      3700\n",
            "           1       0.99      0.97      0.98      1600\n",
            "           2       0.78      0.41      0.54      1600\n",
            "           3       0.81      0.44      0.57       985\n",
            "           4       0.48      0.07      0.13       731\n",
            "           5       0.54      0.03      0.05       679\n",
            "           6       0.06      0.32      0.10       132\n",
            "           7       0.12      0.91      0.21       111\n",
            "           8       0.06      0.90      0.12        72\n",
            "           9       0.03      0.86      0.05         7\n",
            "\n",
            "    accuracy                           0.65      9617\n",
            "   macro avg       0.48      0.58      0.37      9617\n",
            "weighted avg       0.81      0.65      0.68      9617\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/50: 100%|####################| 602/602 [00:16<00:00, 36.13batch/s, accuracy=63.07%, loss=0.00672]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Epoch [9/50], Loss: 0.0062, Accuracy: 63.07%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|####################| 151/151 [00:01<00:00, 90.03batch/s, accuracy=61.45%, loss=0.00589] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Validation Loss: 0.0139, Accuracy: 61.45%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/50: 100%|####################| 602/602 [00:17<00:00, 33.95batch/s, accuracy=63.70%, loss=0.00713]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Epoch [10/50], Loss: 0.0064, Accuracy: 63.70%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|####################| 151/151 [00:01<00:00, 89.49batch/s, accuracy=61.86%, loss=0.00462] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Validation Loss: 0.0124, Accuracy: 61.86%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/50: 100%|####################| 602/602 [00:17<00:00, 35.04batch/s, accuracy=65.15%, loss=0.00473]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Epoch [11/50], Loss: 0.0053, Accuracy: 65.15%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|####################| 151/151 [00:01<00:00, 92.53batch/s, accuracy=66.20%, loss=0.00397] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Validation Loss: 0.0118, Accuracy: 66.20%\n",
            "💾 Best Model Saved: best_model.pth (Accuracy: 66.20%)\n",
            "\n",
            "📋 Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.88      0.90      3700\n",
            "           1       0.99      0.97      0.98      1600\n",
            "           2       0.77      0.45      0.57      1600\n",
            "           3       0.87      0.42      0.56       985\n",
            "           4       0.47      0.12      0.19       731\n",
            "           5       0.63      0.17      0.27       679\n",
            "           6       0.06      0.33      0.11       132\n",
            "           7       0.12      0.92      0.20       111\n",
            "           8       0.06      0.92      0.11        72\n",
            "           9       0.06      0.86      0.12         7\n",
            "\n",
            "    accuracy                           0.66      9617\n",
            "   macro avg       0.50      0.60      0.40      9617\n",
            "weighted avg       0.82      0.66      0.70      9617\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/50: 100%|####################| 602/602 [00:17<00:00, 35.20batch/s, accuracy=65.84%, loss=0.00391]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Epoch [12/50], Loss: 0.0051, Accuracy: 65.84%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|####################| 151/151 [00:02<00:00, 73.40batch/s, accuracy=67.94%, loss=0.00416]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Validation Loss: 0.0118, Accuracy: 67.94%\n",
            "💾 Best Model Saved: best_model.pth (Accuracy: 67.94%)\n",
            "\n",
            "📋 Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.91      0.92      3700\n",
            "           1       0.99      0.97      0.98      1600\n",
            "           2       0.78      0.45      0.57      1600\n",
            "           3       0.81      0.52      0.63       985\n",
            "           4       0.48      0.10      0.16       731\n",
            "           5       0.58      0.13      0.21       679\n",
            "           6       0.06      0.33      0.10       132\n",
            "           7       0.12      0.90      0.21       111\n",
            "           8       0.07      0.89      0.13        72\n",
            "           9       0.06      0.86      0.11         7\n",
            "\n",
            "    accuracy                           0.68      9617\n",
            "   macro avg       0.49      0.61      0.40      9617\n",
            "weighted avg       0.82      0.68      0.71      9617\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/50: 100%|####################| 602/602 [00:16<00:00, 36.35batch/s, accuracy=65.76%, loss=0.00691]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Epoch [13/50], Loss: 0.0050, Accuracy: 65.76%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|####################| 151/151 [00:01<00:00, 91.53batch/s, accuracy=67.28%, loss=0.00499] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Validation Loss: 0.0131, Accuracy: 67.28%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/50: 100%|####################| 602/602 [00:16<00:00, 35.70batch/s, accuracy=65.84%, loss=0.00907]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Epoch [14/50], Loss: 0.0049, Accuracy: 65.84%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|####################| 151/151 [00:02<00:00, 74.11batch/s, accuracy=67.45%, loss=0.00462]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Validation Loss: 0.0150, Accuracy: 67.45%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/50: 100%|####################| 602/602 [00:17<00:00, 34.20batch/s, accuracy=66.78%, loss=0.00345]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Epoch [15/50], Loss: 0.0049, Accuracy: 66.78%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|####################| 151/151 [00:01<00:00, 90.42batch/s, accuracy=68.27%, loss=0.00571] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Validation Loss: 0.0147, Accuracy: 68.27%\n",
            "💾 Best Model Saved: best_model.pth (Accuracy: 68.27%)\n",
            "\n",
            "📋 Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.93      0.92      3700\n",
            "           1       0.99      0.97      0.98      1600\n",
            "           2       0.79      0.46      0.58      1600\n",
            "           3       0.87      0.46      0.60       985\n",
            "           4       0.50      0.09      0.15       731\n",
            "           5       0.67      0.18      0.28       679\n",
            "           6       0.06      0.33      0.10       132\n",
            "           7       0.12      0.93      0.21       111\n",
            "           8       0.07      0.89      0.13        72\n",
            "           9       0.14      0.71      0.24         7\n",
            "\n",
            "    accuracy                           0.68      9617\n",
            "   macro avg       0.51      0.59      0.42      9617\n",
            "weighted avg       0.82      0.68      0.71      9617\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/50: 100%|####################| 602/602 [00:16<00:00, 35.81batch/s, accuracy=66.74%, loss=0.0052]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Epoch [16/50], Loss: 0.0048, Accuracy: 66.74%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|####################| 151/151 [00:01<00:00, 88.95batch/s, accuracy=67.47%, loss=0.00381]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Validation Loss: 0.0119, Accuracy: 67.47%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/50: 100%|####################| 602/602 [00:17<00:00, 34.25batch/s, accuracy=67.28%, loss=0.00258]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Epoch [17/50], Loss: 0.0044, Accuracy: 67.28%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|####################| 151/151 [00:01<00:00, 91.78batch/s, accuracy=68.68%, loss=0.00394] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Validation Loss: 0.0136, Accuracy: 68.68%\n",
            "💾 Best Model Saved: best_model.pth (Accuracy: 68.68%)\n",
            "\n",
            "📋 Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.93      0.92      3700\n",
            "           1       0.99      0.97      0.98      1600\n",
            "           2       0.77      0.46      0.58      1600\n",
            "           3       0.86      0.48      0.62       985\n",
            "           4       0.49      0.11      0.18       731\n",
            "           5       0.62      0.19      0.29       679\n",
            "           6       0.06      0.33      0.11       132\n",
            "           7       0.12      0.91      0.22       111\n",
            "           8       0.07      0.90      0.13        72\n",
            "           9       0.11      0.71      0.19         7\n",
            "\n",
            "    accuracy                           0.69      9617\n",
            "   macro avg       0.50      0.60      0.42      9617\n",
            "weighted avg       0.82      0.69      0.72      9617\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/50: 100%|####################| 602/602 [00:16<00:00, 35.97batch/s, accuracy=67.39%, loss=0.00232]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Epoch [18/50], Loss: 0.0045, Accuracy: 67.39%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|####################| 151/151 [00:01<00:00, 92.41batch/s, accuracy=68.99%, loss=0.0044] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Validation Loss: 0.0126, Accuracy: 68.99%\n",
            "💾 Best Model Saved: best_model.pth (Accuracy: 68.99%)\n",
            "\n",
            "📋 Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.91      0.92      3700\n",
            "           1       0.99      0.97      0.98      1600\n",
            "           2       0.77      0.48      0.60      1600\n",
            "           3       0.84      0.49      0.62       985\n",
            "           4       0.49      0.13      0.20       731\n",
            "           5       0.71      0.19      0.30       679\n",
            "           6       0.07      0.33      0.11       132\n",
            "           7       0.12      0.92      0.22       111\n",
            "           8       0.07      0.93      0.14        72\n",
            "           9       0.11      0.86      0.20         7\n",
            "\n",
            "    accuracy                           0.69      9617\n",
            "   macro avg       0.51      0.62      0.43      9617\n",
            "weighted avg       0.83      0.69      0.72      9617\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/50: 100%|####################| 602/602 [00:17<00:00, 34.13batch/s, accuracy=67.75%, loss=0.00298]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Epoch [19/50], Loss: 0.0044, Accuracy: 67.75%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|####################| 151/151 [00:01<00:00, 91.33batch/s, accuracy=68.06%, loss=0.00424]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Validation Loss: 0.0128, Accuracy: 68.06%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/50: 100%|####################| 602/602 [00:16<00:00, 36.05batch/s, accuracy=67.72%, loss=0.00443]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Epoch [20/50], Loss: 0.0044, Accuracy: 67.72%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|####################| 151/151 [00:01<00:00, 90.83batch/s, accuracy=68.61%, loss=0.0041] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Validation Loss: 0.0133, Accuracy: 68.61%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21/50: 100%|####################| 602/602 [00:16<00:00, 35.68batch/s, accuracy=68.24%, loss=0.00517]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Epoch [21/50], Loss: 0.0042, Accuracy: 68.24%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|####################| 151/151 [00:02<00:00, 69.55batch/s, accuracy=68.93%, loss=0.00416]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Validation Loss: 0.0146, Accuracy: 68.93%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22/50: 100%|####################| 602/602 [00:16<00:00, 35.73batch/s, accuracy=68.23%, loss=0.00904]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Epoch [22/50], Loss: 0.0043, Accuracy: 68.23%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|####################| 151/151 [00:01<00:00, 91.03batch/s, accuracy=69.34%, loss=0.00396]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Validation Loss: 0.0141, Accuracy: 69.34%\n",
            "💾 Best Model Saved: best_model.pth (Accuracy: 69.34%)\n",
            "\n",
            "📋 Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.94      0.93      3700\n",
            "           1       0.99      0.97      0.98      1600\n",
            "           2       0.80      0.47      0.59      1600\n",
            "           3       0.86      0.48      0.61       985\n",
            "           4       0.47      0.11      0.18       731\n",
            "           5       0.65      0.21      0.31       679\n",
            "           6       0.06      0.33      0.11       132\n",
            "           7       0.12      0.94      0.21       111\n",
            "           8       0.08      0.90      0.14        72\n",
            "           9       0.14      0.71      0.23         7\n",
            "\n",
            "    accuracy                           0.69      9617\n",
            "   macro avg       0.51      0.60      0.43      9617\n",
            "weighted avg       0.82      0.69      0.72      9617\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23/50: 100%|####################| 602/602 [00:16<00:00, 35.81batch/s, accuracy=68.59%, loss=0.003]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Epoch [23/50], Loss: 0.0042, Accuracy: 68.59%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|####################| 151/151 [00:01<00:00, 77.77batch/s, accuracy=69.51%, loss=0.00414]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Validation Loss: 0.0144, Accuracy: 69.51%\n",
            "💾 Best Model Saved: best_model.pth (Accuracy: 69.51%)\n",
            "\n",
            "📋 Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.93      0.93      3700\n",
            "           1       0.99      0.97      0.98      1600\n",
            "           2       0.79      0.47      0.59      1600\n",
            "           3       0.86      0.49      0.62       985\n",
            "           4       0.49      0.12      0.19       731\n",
            "           5       0.63      0.21      0.31       679\n",
            "           6       0.07      0.33      0.11       132\n",
            "           7       0.12      0.92      0.21       111\n",
            "           8       0.08      0.90      0.14        72\n",
            "           9       0.14      0.71      0.23         7\n",
            "\n",
            "    accuracy                           0.70      9617\n",
            "   macro avg       0.51      0.60      0.43      9617\n",
            "weighted avg       0.82      0.70      0.72      9617\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24/50: 100%|####################| 602/602 [00:17<00:00, 35.14batch/s, accuracy=68.62%, loss=0.00533]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Epoch [24/50], Loss: 0.0042, Accuracy: 68.62%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|####################| 151/151 [00:01<00:00, 90.62batch/s, accuracy=69.28%, loss=0.00384] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Validation Loss: 0.0143, Accuracy: 69.28%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25/50: 100%|####################| 602/602 [00:16<00:00, 35.74batch/s, accuracy=68.52%, loss=0.00401]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Epoch [25/50], Loss: 0.0041, Accuracy: 68.52%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|####################| 151/151 [00:01<00:00, 92.23batch/s, accuracy=69.71%, loss=0.00377] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Validation Loss: 0.0140, Accuracy: 69.71%\n",
            "💾 Best Model Saved: best_model.pth (Accuracy: 69.71%)\n",
            "\n",
            "📋 Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.94      0.93      3700\n",
            "           1       0.99      0.97      0.98      1600\n",
            "           2       0.79      0.48      0.60      1600\n",
            "           3       0.86      0.48      0.62       985\n",
            "           4       0.47      0.14      0.22       731\n",
            "           5       0.65      0.19      0.30       679\n",
            "           6       0.07      0.33      0.11       132\n",
            "           7       0.12      0.93      0.22       111\n",
            "           8       0.08      0.90      0.14        72\n",
            "           9       0.15      0.71      0.25         7\n",
            "\n",
            "    accuracy                           0.70      9617\n",
            "   macro avg       0.51      0.61      0.44      9617\n",
            "weighted avg       0.82      0.70      0.73      9617\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26/50: 100%|####################| 602/602 [00:17<00:00, 34.16batch/s, accuracy=68.77%, loss=0.00581]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Epoch [26/50], Loss: 0.0041, Accuracy: 68.77%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|####################| 151/151 [00:01<00:00, 84.17batch/s, accuracy=69.75%, loss=0.00339]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Validation Loss: 0.0139, Accuracy: 69.75%\n",
            "💾 Best Model Saved: best_model.pth (Accuracy: 69.75%)\n",
            "\n",
            "📋 Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.94      0.93      3700\n",
            "           1       0.99      0.97      0.98      1600\n",
            "           2       0.79      0.48      0.60      1600\n",
            "           3       0.86      0.48      0.62       985\n",
            "           4       0.48      0.13      0.21       731\n",
            "           5       0.66      0.20      0.30       679\n",
            "           6       0.07      0.33      0.11       132\n",
            "           7       0.12      0.92      0.22       111\n",
            "           8       0.08      0.90      0.14        72\n",
            "           9       0.14      0.71      0.24         7\n",
            "\n",
            "    accuracy                           0.70      9617\n",
            "   macro avg       0.51      0.61      0.43      9617\n",
            "weighted avg       0.82      0.70      0.73      9617\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27/50: 100%|####################| 602/602 [00:17<00:00, 34.65batch/s, accuracy=68.84%, loss=0.00372]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Epoch [27/50], Loss: 0.0041, Accuracy: 68.84%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|####################| 151/151 [00:02<00:00, 64.81batch/s, accuracy=70.05%, loss=0.00388]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Validation Loss: 0.0137, Accuracy: 70.05%\n",
            "💾 Best Model Saved: best_model.pth (Accuracy: 70.05%)\n",
            "\n",
            "📋 Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.94      0.93      3700\n",
            "           1       0.99      0.97      0.98      1600\n",
            "           2       0.79      0.48      0.60      1600\n",
            "           3       0.86      0.50      0.63       985\n",
            "           4       0.48      0.11      0.18       731\n",
            "           5       0.68      0.21      0.33       679\n",
            "           6       0.07      0.33      0.11       132\n",
            "           7       0.12      0.92      0.21       111\n",
            "           8       0.08      0.89      0.14        72\n",
            "           9       0.14      0.71      0.24         7\n",
            "\n",
            "    accuracy                           0.70      9617\n",
            "   macro avg       0.51      0.61      0.44      9617\n",
            "weighted avg       0.83      0.70      0.73      9617\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28/50: 100%|####################| 602/602 [00:17<00:00, 34.46batch/s, accuracy=68.65%, loss=0.00408]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Epoch [28/50], Loss: 0.0041, Accuracy: 68.65%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|####################| 151/151 [00:01<00:00, 88.59batch/s, accuracy=69.69%, loss=0.00372]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Validation Loss: 0.0137, Accuracy: 69.69%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29/50: 100%|####################| 602/602 [00:16<00:00, 36.15batch/s, accuracy=68.72%, loss=0.00396]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Epoch [29/50], Loss: 0.0040, Accuracy: 68.72%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|####################| 151/151 [00:01<00:00, 90.28batch/s, accuracy=69.89%, loss=0.00381] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Validation Loss: 0.0142, Accuracy: 69.89%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30/50: 100%|####################| 602/602 [00:17<00:00, 33.57batch/s, accuracy=68.96%, loss=0.00294]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Epoch [30/50], Loss: 0.0041, Accuracy: 68.96%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|####################| 151/151 [00:01<00:00, 87.34batch/s, accuracy=69.72%, loss=0.00357]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Validation Loss: 0.0141, Accuracy: 69.72%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 31/50: 100%|####################| 602/602 [00:16<00:00, 35.71batch/s, accuracy=68.79%, loss=0.00131]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Epoch [31/50], Loss: 0.0040, Accuracy: 68.79%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|####################| 151/151 [00:01<00:00, 91.84batch/s, accuracy=70.04%, loss=0.00346] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Validation Loss: 0.0136, Accuracy: 70.04%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 32/50: 100%|####################| 602/602 [00:17<00:00, 33.91batch/s, accuracy=68.81%, loss=0.00331]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Epoch [32/50], Loss: 0.0040, Accuracy: 68.81%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|####################| 151/151 [00:02<00:00, 71.45batch/s, accuracy=69.82%, loss=0.00373]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Validation Loss: 0.0139, Accuracy: 69.82%\n",
            "⛔ Early Stopping Triggered!\n",
            "\n",
            "🧪 Evaluating final model on test set...\n",
            "📂 Loaded best model from epoch 27\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|####################| 2133/2133 [00:22<00:00, 94.42batch/s, accuracy=64.25%, loss=0.00125]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🏆 Test Accuracy: 64.25%\n",
            "📈 ROC AUC Score (weighted): 0.9532\n",
            "📏 Weighted Precision: 0.7400\n",
            "📏 Weighted Recall: 0.6425\n",
            "📏 Weighted F1 Score: 0.6371\n",
            "\n",
            "📋 Final Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.97      0.85     40368\n",
            "           1       0.99      0.98      0.99     32130\n",
            "           2       0.77      0.39      0.52     25141\n",
            "           3       0.60      0.06      0.12     13769\n",
            "           4       0.35      0.17      0.23     10762\n",
            "           5       0.69      0.23      0.34      9980\n",
            "           6       0.05      0.53      0.10      1601\n",
            "           7       0.06      0.21      0.10      1580\n",
            "           8       0.12      0.93      0.21      1060\n",
            "           9       0.07      0.54      0.13        93\n",
            "\n",
            "    accuracy                           0.64    136484\n",
            "   macro avg       0.45      0.50      0.36    136484\n",
            "weighted avg       0.74      0.64      0.64    136484\n",
            "\n",
            "\n",
            "✅ Training Complete!\n",
            "\n",
            "🏁 Final test accuracy: 64.25%\n",
            "Final model accuracy: 64.25%\n",
            "Model saved to: iot_anomaly_results/best_model.pth\n"
          ]
        }
      ]
    }
  ]
}