{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPiVVhT7yze8Nah5H9hkO1N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fadhluibnu/ANOMALY_IOT_NETWORK_DETECTION/blob/main/2_IOT_ANOMALI_DETECTION_GROX_GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bagian 1: Preprocessing"
      ],
      "metadata": {
        "id": "DIqHMFUc9KDB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyTIGlgL8AYW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, RobustScaler\n",
        "from sklearn.feature_selection import mutual_info_classif, SelectKBest, VarianceThreshold\n",
        "from imblearn.over_sampling import ADASYN\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline\n",
        "from scipy.stats import zscore\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def enhanced_preprocessing(train_data, test_data):\n",
        "    \"\"\"\n",
        "    Enhanced preprocessing pipeline for IoT network anomaly detection.\n",
        "    Includes improved feature engineering, selection, and balancing.\n",
        "\n",
        "    Args:\n",
        "        train_data: Original training dataframe\n",
        "        test_data: Original testing dataframe\n",
        "\n",
        "    Returns:\n",
        "        processed_data: Dictionary with all processed datasets and loaders\n",
        "    \"\"\"\n",
        "    print(\"\\nüîç Starting enhanced preprocessing pipeline...\")\n",
        "\n",
        "    # Make copies to avoid modifying originals\n",
        "    train_data = train_data.copy()\n",
        "    test_data = test_data.copy()\n",
        "\n",
        "    # 1. Data Exploration\n",
        "    print(\"\\nüìä Dataset Overview:\")\n",
        "    print(f\"Training data shape: {train_data.shape}\")\n",
        "    print(f\"Testing data shape: {test_data.shape}\")\n",
        "\n",
        "    # Check for missing values\n",
        "    train_missing = train_data.isnull().sum().sum()\n",
        "    test_missing = test_data.isnull().sum().sum()\n",
        "    print(f\"Missing values - Training: {train_missing}, Testing: {test_missing}\")\n",
        "\n",
        "    # 2. Drop rows with missing attack_cat (target variable)\n",
        "    print(\"\\nüîç Handling missing target values...\")\n",
        "    train_data = train_data.dropna(subset=['attack_cat'])\n",
        "    test_data = test_data.dropna(subset=['attack_cat'])\n",
        "    print(f\"After dropping rows with missing targets - Train: {train_data.shape}, Test: {test_data.shape}\")\n",
        "\n",
        "    # 3. Identify numerical and categorical columns\n",
        "    numerical_cols = train_data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "    numerical_cols = [col for col in numerical_cols if col not in ['id', 'label', 'attack_cat']]\n",
        "    categorical_cols = train_data.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "    # 4. Handle remaining missing values\n",
        "    print(\"\\nüîç Handling remaining missing values...\")\n",
        "    for col in numerical_cols:\n",
        "        # Use median for numerical features (more robust than mean)\n",
        "        train_data[col] = train_data[col].fillna(train_data[col].median())\n",
        "        test_data[col] = test_data[col].fillna(train_data[col].median())\n",
        "\n",
        "    for col in categorical_cols:\n",
        "        if col != 'attack_cat':\n",
        "            # Use mode (most frequent) for categorical features\n",
        "            train_data[col] = train_data[col].fillna(train_data[col].mode()[0])\n",
        "            test_data[col] = test_data[col].fillna(train_data[col].mode()[0])\n",
        "\n",
        "    # 5. Remove duplicate rows\n",
        "    print(\"\\nüîç Removing duplicate entries...\")\n",
        "    train_data = train_data.drop_duplicates()\n",
        "    test_data = test_data.drop_duplicates()\n",
        "    print(f\"After removing duplicates - Train: {train_data.shape}, Test: {test_data.shape}\")\n",
        "\n",
        "    # 6. Encode attack_cat (target variable)\n",
        "    print(\"\\nüîç Encoding target variable...\")\n",
        "    attack_mapping = {\n",
        "        'Normal': 0, 'Generic': 1, 'Exploits': 2, 'Fuzzers': 3, 'DoS': 4,\n",
        "        'Reconnaissance': 5, 'Analysis': 6, 'Backdoor': 7, 'Shellcode': 8, 'Worms': 9\n",
        "    }\n",
        "    train_data['attack_cat'] = train_data['attack_cat'].map(attack_mapping)\n",
        "    test_data['attack_cat'] = test_data['attack_cat'].map(attack_mapping)\n",
        "\n",
        "    # Check class distribution\n",
        "    class_counts = train_data['attack_cat'].value_counts().sort_index()\n",
        "    print(\"Class distribution after encoding:\")\n",
        "    for class_id, count in class_counts.items():\n",
        "        print(f\"  Class {class_id}: {count} samples ({100*count/len(train_data):.2f}%)\")\n",
        "\n",
        "    # 7. IMPROVED: Handle outliers first, then transform data (before scaling)\n",
        "    print(\"\\nüîç Handling outliers with improved Winsorization...\")\n",
        "    for col in numerical_cols:\n",
        "        # Use more conservative percentiles for winsorization (0.5% - 99.5%)\n",
        "        lower_bound = train_data[col].quantile(0.005)\n",
        "        upper_bound = train_data[col].quantile(0.995)\n",
        "\n",
        "        # Apply clipping to both train and test data\n",
        "        train_data[col] = train_data[col].clip(lower=lower_bound, upper=upper_bound)\n",
        "        test_data[col] = test_data[col].clip(lower=lower_bound, upper=upper_bound)\n",
        "\n",
        "    # 8. Advanced feature transformation for skewed features\n",
        "    print(\"\\nüîç Applying transformations for skewed numerical features...\")\n",
        "    for col in numerical_cols:\n",
        "        # Check if data is significantly skewed\n",
        "        skewness = train_data[col].skew()\n",
        "        if abs(skewness) > 1.5:  # More aggressive threshold for transformation\n",
        "            # Apply log transformation (adding a constant to handle zeros/negatives)\n",
        "            train_min = train_data[col].min()\n",
        "            offset = 1 - min(0, train_min)  # Ensure all values are positive\n",
        "\n",
        "            train_data[col] = np.log1p(train_data[col] + offset)\n",
        "            test_data[col] = np.log1p(test_data[col] + offset)\n",
        "            print(f\"  Applied log transform to {col} (skewness: {skewness:.2f})\")\n",
        "\n",
        "    # 9. Encoding categorical features with enhanced handling\n",
        "    print(\"\\nüîç Encoding categorical features...\")\n",
        "    # Use OneHotEncoder with improved handling for test data\n",
        "    if len(categorical_cols) > 0:\n",
        "        encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "        encoded_train = encoder.fit_transform(train_data[categorical_cols])\n",
        "        encoded_test = encoder.transform(test_data[categorical_cols])\n",
        "\n",
        "        # Get feature names\n",
        "        feature_names = encoder.get_feature_names_out(categorical_cols)\n",
        "\n",
        "        # Create DataFrames with encoded features\n",
        "        encoded_train_df = pd.DataFrame(encoded_train, columns=feature_names, index=train_data.index)\n",
        "        encoded_test_df = pd.DataFrame(encoded_test, columns=feature_names, index=test_data.index)\n",
        "\n",
        "        # Drop original categorical columns and join encoded ones\n",
        "        train_data = train_data.drop(columns=categorical_cols).reset_index(drop=True)\n",
        "        test_data = test_data.drop(columns=categorical_cols).reset_index(drop=True)\n",
        "\n",
        "        train_data = pd.concat([train_data, encoded_train_df.reset_index(drop=True)], axis=1)\n",
        "        test_data = pd.concat([test_data, encoded_test_df.reset_index(drop=True)], axis=1)\n",
        "\n",
        "        print(f\"  Encoded {len(categorical_cols)} categorical features into {encoded_train.shape[1]} binary features\")\n",
        "\n",
        "    # 10. IMPROVED: Two-stage feature selection using variance and mutual information\n",
        "    print(\"\\nüîç Performing improved two-stage feature selection...\")\n",
        "    # Exclude target and ID columns\n",
        "    X_train = train_data.drop(columns=['id', 'label', 'attack_cat'])\n",
        "    y_train = train_data['attack_cat']\n",
        "\n",
        "    # Stage 1: Remove low variance features\n",
        "    selector_var = VarianceThreshold(threshold=0.01)\n",
        "    selector_var.fit(X_train)\n",
        "    low_var_features = X_train.columns[~selector_var.get_support()].tolist()\n",
        "    if low_var_features:\n",
        "        print(f\"  Removed {len(low_var_features)} low variance features\")\n",
        "        X_train = X_train.drop(columns=low_var_features)\n",
        "\n",
        "    # Stage 2: Select features using mutual information\n",
        "    # Keep more features (95% of remaining or max 150 features)\n",
        "    k = min(150, int(X_train.shape[1] * 0.95))\n",
        "    selector_mi = SelectKBest(mutual_info_classif, k=k)\n",
        "    selector_mi.fit(X_train, y_train)\n",
        "\n",
        "    # Get selected feature names\n",
        "    selected_features = X_train.columns[selector_mi.get_support()].tolist()\n",
        "    print(f\"  Selected {len(selected_features)} features after two-stage selection\")\n",
        "\n",
        "    # Keep only selected features\n",
        "    features_to_keep = ['id', 'label', 'attack_cat'] + selected_features\n",
        "    train_data = train_data[features_to_keep]\n",
        "    test_data = test_data[features_to_keep]\n",
        "\n",
        "    # Update numerical columns list to reflect selected features only\n",
        "    numerical_cols = [col for col in selected_features if col in numerical_cols]\n",
        "\n",
        "    # 11. IMPROVED: Apply scaling after outlier handling\n",
        "    print(\"\\nüîç Applying robust scaling to numerical features...\")\n",
        "    scaler = RobustScaler()  # Still more robust than StandardScaler\n",
        "\n",
        "    # If no numerical columns remain after selection, skip scaling\n",
        "    if numerical_cols:\n",
        "        train_data[numerical_cols] = scaler.fit_transform(train_data[numerical_cols])\n",
        "        test_data[numerical_cols] = scaler.transform(test_data[numerical_cols])\n",
        "\n",
        "    # 12. IMPROVED: Enhanced resampling with ADASYN and undersampling\n",
        "    print(\"\\nüîç Applying enhanced balanced resampling...\")\n",
        "    # Prepare data for resampling\n",
        "    X_train = train_data.drop(columns=['id', 'label', 'attack_cat'])\n",
        "    y_train = train_data['attack_cat']\n",
        "\n",
        "    # Print class distribution before resampling\n",
        "    print(\"  Class distribution before resampling:\")\n",
        "    for class_id, count in y_train.value_counts().sort_index().items():\n",
        "        print(f\"    Class {class_id}: {count} samples ({100*count/len(y_train):.2f}%)\")\n",
        "\n",
        "    # Calculate max samples per class - balanced but with limits\n",
        "    max_samples = min(8000, int(len(y_train) * 0.15))  # Limit max samples per minority class\n",
        "    sampling_strategy = {i: min(max_samples, count) for i, count in y_train.value_counts().items()}\n",
        "\n",
        "    # For majority class, keep more samples but not too many\n",
        "    majority_class = y_train.value_counts().idxmax()\n",
        "    sampling_strategy[majority_class] = min(int(len(y_train) * 0.3), y_train.value_counts()[majority_class])\n",
        "\n",
        "    print(\"  Target sample counts per class:\")\n",
        "    for class_id, target_count in sorted(sampling_strategy.items()):\n",
        "        print(f\"    Class {class_id}: {target_count} samples\")\n",
        "\n",
        "    # Apply the resampling pipeline\n",
        "    try:\n",
        "        # First try ADASYN which requires multiple samples per class\n",
        "        resampler = Pipeline([\n",
        "            ('over', ADASYN(sampling_strategy='minority', random_state=42, n_neighbors=5)),\n",
        "            ('under', RandomUnderSampler(sampling_strategy=sampling_strategy, random_state=42))\n",
        "        ])\n",
        "        X_train_resampled, y_train_resampled = resampler.fit_resample(X_train, y_train)\n",
        "    except ValueError as e:\n",
        "        print(f\"  ADASYN failed: {e}. Falling back to SMOTE.\")\n",
        "        # Fall back to SMOTE with different neighbor settings\n",
        "        from imblearn.over_sampling import SMOTE\n",
        "        resampler = Pipeline([\n",
        "            ('over', SMOTE(sampling_strategy='minority', random_state=42, k_neighbors=3)),\n",
        "            ('under', RandomUnderSampler(sampling_strategy=sampling_strategy, random_state=42))\n",
        "        ])\n",
        "        X_train_resampled, y_train_resampled = resampler.fit_resample(X_train, y_train)\n",
        "\n",
        "    # Print class distribution after resampling\n",
        "    print(\"  Class distribution after resampling:\")\n",
        "    for class_id, count in pd.Series(y_train_resampled).value_counts().sort_index().items():\n",
        "        print(f\"    Class {class_id}: {count} samples ({100*count/len(y_train_resampled):.2f}%)\")\n",
        "\n",
        "    # Create new DataFrame with resampled data\n",
        "    train_data_resampled = pd.DataFrame(X_train_resampled, columns=X_train.columns)\n",
        "    train_data_resampled['attack_cat'] = y_train_resampled\n",
        "    train_data_resampled['id'] = range(len(train_data_resampled))\n",
        "    train_data_resampled['label'] = train_data_resampled['attack_cat'] != 0  # Binary label (0=Normal)\n",
        "\n",
        "    # 13. Check for any remaining issues\n",
        "    print(\"\\nüîç Final data quality check...\")\n",
        "    # Check for infinities\n",
        "    train_data_resampled = train_data_resampled.replace([np.inf, -np.inf], np.nan)\n",
        "    test_data = test_data.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "    # Check for NaN and fill if any\n",
        "    if train_data_resampled.isnull().sum().sum() > 0:\n",
        "        print(f\"  Found {train_data_resampled.isnull().sum().sum()} NaN values in training data. Filling with column medians.\")\n",
        "        for col in train_data_resampled.columns:\n",
        "            if train_data_resampled[col].isnull().sum() > 0:\n",
        "                if train_data_resampled[col].dtype in ['int64', 'float64']:\n",
        "                    train_data_resampled[col] = train_data_resampled[col].fillna(train_data_resampled[col].median())\n",
        "\n",
        "    if test_data.isnull().sum().sum() > 0:\n",
        "        print(f\"  Found {test_data.isnull().sum().sum()} NaN values in test data. Filling with column medians.\")\n",
        "        for col in test_data.columns:\n",
        "            if test_data[col].isnull().sum() > 0:\n",
        "                if test_data[col].dtype in ['int64', 'float64']:\n",
        "                    test_data[col] = test_data[col].fillna(test_data[col].median())\n",
        "\n",
        "    # 14. IMPROVED: Create validation set from training data\n",
        "    print(\"\\nüîç Creating train/validation split...\")\n",
        "    # Extract features and targets\n",
        "    X_train_final = train_data_resampled.drop(columns=['id', 'label', 'attack_cat'])\n",
        "    y_train_final = train_data_resampled['attack_cat']\n",
        "    X_test_final = test_data.drop(columns=['id', 'label', 'attack_cat'])\n",
        "    y_test_final = test_data['attack_cat']\n",
        "\n",
        "    # Split into training and validation sets with stratification\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
        "        X_train_final, y_train_final, test_size=0.15, random_state=42, stratify=y_train_final\n",
        "    )\n",
        "\n",
        "    print(f\"  Training data: {X_train_split.shape[0]} samples\")\n",
        "    print(f\"  Validation data: {X_val_split.shape[0]} samples\")\n",
        "    print(f\"  Test data: {X_test_final.shape[0]} samples\")\n",
        "\n",
        "    # 15. Convert to tensors\n",
        "    X_train_tensor = torch.tensor(X_train_split.values, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train_split.values, dtype=torch.long)\n",
        "\n",
        "    X_val_tensor = torch.tensor(X_val_split.values, dtype=torch.float32)\n",
        "    y_val_tensor = torch.tensor(y_val_split.values, dtype=torch.long)\n",
        "\n",
        "    X_test_tensor = torch.tensor(X_test_final.values, dtype=torch.float32)\n",
        "    y_test_tensor = torch.tensor(y_test_final.values, dtype=torch.long)\n",
        "\n",
        "    # 16. Create datasets\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "    # 17. Create data loaders with optimized parameters\n",
        "    print(\"\\nüîç Creating data loaders...\")\n",
        "    batch_size = 64  # Can adjust based on available memory\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        drop_last=False\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        drop_last=False\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        drop_last=False\n",
        "    )\n",
        "\n",
        "    print(f\"\\n‚úÖ Preprocessing complete!\")\n",
        "    print(f\"  Final training set: {len(train_dataset)} samples with {X_train_split.shape[1]} features\")\n",
        "    print(f\"  Final validation set: {len(val_dataset)} samples\")\n",
        "    print(f\"  Final test set: {len(test_dataset)} samples with {X_test_final.shape[1]} features\")\n",
        "\n",
        "    # 18. Visualize class distribution\n",
        "    try:\n",
        "        plt.figure(figsize=(15, 5))\n",
        "\n",
        "        plt.subplot(1, 3, 1)\n",
        "        train_class_counts = pd.Series(y_train_split).value_counts().sort_index()\n",
        "        plt.bar(train_class_counts.index.astype(str), train_class_counts.values)\n",
        "        plt.title('Training Data Class Distribution')\n",
        "        plt.xlabel('Attack Category')\n",
        "        plt.ylabel('Count')\n",
        "        plt.xticks(rotation=45)\n",
        "\n",
        "        plt.subplot(1, 3, 2)\n",
        "        val_class_counts = pd.Series(y_val_split).value_counts().sort_index()\n",
        "        plt.bar(val_class_counts.index.astype(str), val_class_counts.values)\n",
        "        plt.title('Validation Data Class Distribution')\n",
        "        plt.xlabel('Attack Category')\n",
        "        plt.ylabel('Count')\n",
        "        plt.xticks(rotation=45)\n",
        "\n",
        "        plt.subplot(1, 3, 3)\n",
        "        test_class_counts = pd.Series(y_test_final).value_counts().sort_index()\n",
        "        plt.bar(test_class_counts.index.astype(str), test_class_counts.values)\n",
        "        plt.title('Test Data Class Distribution')\n",
        "        plt.xlabel('Attack Category')\n",
        "        plt.ylabel('Count')\n",
        "        plt.xticks(rotation=45)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('class_distribution.png')\n",
        "        plt.close()\n",
        "    except:\n",
        "        print(\"  Could not create visualization (possibly running in non-graphical environment)\")\n",
        "\n",
        "    # Calculate class weights for later use in loss function\n",
        "    num_classes = len(np.unique(y_train_final))\n",
        "    class_counts = np.bincount(y_train_split)\n",
        "\n",
        "    # Handle potential missing classes in the bincount\n",
        "    if len(class_counts) < num_classes:\n",
        "        temp_counts = np.zeros(num_classes)\n",
        "        temp_counts[:len(class_counts)] = class_counts\n",
        "        class_counts = temp_counts\n",
        "\n",
        "    # Calculate inverse frequency class weights, bounded to prevent extreme values\n",
        "    class_weights = np.ones(num_classes)\n",
        "    non_zero_counts = class_counts[class_counts > 0]\n",
        "    if len(non_zero_counts) > 0:\n",
        "        class_weights[class_counts > 0] = 1.0 / class_counts[class_counts > 0]\n",
        "        # Normalize weights to sum to num_classes\n",
        "        class_weights = class_weights * (num_classes / class_weights.sum())\n",
        "        # Bound weights to reasonable range\n",
        "        class_weights = np.clip(class_weights, 0.1, 10.0)\n",
        "\n",
        "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32)\n",
        "\n",
        "    # Return processed data and related objects\n",
        "    return {\n",
        "        'train_data': train_data_resampled,\n",
        "        'test_data': test_data,\n",
        "        'X_train': X_train_split,\n",
        "        'y_train': y_train_split,\n",
        "        'X_val': X_val_split,\n",
        "        'y_val': y_val_split,\n",
        "        'X_test': X_test_final,\n",
        "        'y_test': y_test_final,\n",
        "        'train_loader': train_loader,\n",
        "        'val_loader': val_loader,\n",
        "        'test_loader': test_loader,\n",
        "        'input_dim': X_train_split.shape[1],\n",
        "        'num_classes': num_classes,\n",
        "        'class_weights': class_weights_tensor\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bagian 2: Model CNN + DBN Ensemble"
      ],
      "metadata": {
        "id": "2a1DL2D79R8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import time\n",
        "import os\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Improved 2D CNN for tabular data\n",
        "class RevisedCNN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, dropout_rate=0.3):\n",
        "        super(RevisedCNN, self).__init__()\n",
        "\n",
        "        # Reshape input to 2D representation\n",
        "        self.input_dim = input_dim\n",
        "        self.reshape_dim = int(np.sqrt(input_dim)) + 1\n",
        "        self.pad_size = self.reshape_dim**2 - input_dim\n",
        "\n",
        "        # Convolutional layers with larger kernel and better stride\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=5, stride=1, padding=2)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Compute output size after first conv+pool\n",
        "        conv1_out_size = self.reshape_dim // 2\n",
        "\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Compute output size after second conv+pool\n",
        "        conv2_out_size = conv1_out_size // 2\n",
        "\n",
        "        # Calculate flattened size\n",
        "        self.flattened_size = 128 * max(1, conv2_out_size) * max(1, conv2_out_size)\n",
        "\n",
        "        # Fully connected layers with proper sizes\n",
        "        self.fc1 = nn.Linear(self.flattened_size, 512)\n",
        "        self.bn_fc1 = nn.BatchNorm1d(512)\n",
        "        self.dropout_fc1 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.bn_fc2 = nn.BatchNorm1d(256)\n",
        "        self.dropout_fc2 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        self.fc3 = nn.Linear(256, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Reshape input to 2D (square-like format for better convolution)\n",
        "        x_padded = F.pad(x, (0, self.pad_size))\n",
        "        x = x_padded.view(batch_size, 1, self.reshape_dim, self.reshape_dim)\n",
        "\n",
        "        # Apply convolutional layers\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = F.leaky_relu(x, negative_slope=0.1)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = F.leaky_relu(x, negative_slope=0.1)\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        # Flatten\n",
        "        x = x.view(batch_size, -1)\n",
        "\n",
        "        # Fully connected layers\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn_fc1(x)\n",
        "        x = F.leaky_relu(x, negative_slope=0.1)\n",
        "        x = self.dropout_fc1(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = self.bn_fc2(x)\n",
        "        x = F.leaky_relu(x, negative_slope=0.1)\n",
        "        x = self.dropout_fc2(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Improved RBM with proper pretraining abilities\n",
        "class ImprovedRBM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(ImprovedRBM, self).__init__()\n",
        "\n",
        "        # Initialize weights with small values for stability\n",
        "        self.W = nn.Parameter(torch.randn(input_size, hidden_size) * 0.01)\n",
        "        self.b = nn.Parameter(torch.zeros(hidden_size))\n",
        "        self.c = nn.Parameter(torch.zeros(input_size))\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Xavier/Glorot initialization for better convergence\n",
        "        nn.init.xavier_uniform_(self.W)\n",
        "\n",
        "    def forward(self, v):\n",
        "        \"\"\"Visible to hidden layer probabilities\"\"\"\n",
        "        batch_size = v.size(0)\n",
        "\n",
        "        # Reshape input to 2D if needed\n",
        "        if v.dim() > 2:\n",
        "            v = v.view(batch_size, -1)\n",
        "\n",
        "        # Handle dimension mismatch gracefully\n",
        "        if v.size(1) != self.input_size:\n",
        "            v_resized = torch.zeros(batch_size, self.input_size, device=v.device)\n",
        "            min_size = min(v.size(1), self.input_size)\n",
        "            v_resized[:, :min_size] = v[:, :min_size]\n",
        "            v = v_resized\n",
        "\n",
        "        # Apply L2 normalization for better stability\n",
        "        v = F.normalize(v, p=2, dim=1)\n",
        "\n",
        "        # Compute hidden activations\n",
        "        h_activation = torch.matmul(v, self.W) + self.b\n",
        "        h_probs = torch.sigmoid(h_activation)\n",
        "\n",
        "        return h_probs\n",
        "\n",
        "    def sample_h(self, v):\n",
        "        \"\"\"Sample from the hidden layer given visible state\"\"\"\n",
        "        h_probs = self.forward(v)\n",
        "        h_samples = torch.bernoulli(h_probs)\n",
        "        return h_samples, h_probs\n",
        "\n",
        "    def sample_v(self, h):\n",
        "        \"\"\"Sample from the visible layer given hidden state\"\"\"\n",
        "        v_activation = torch.matmul(h, self.W.t()) + self.c\n",
        "        v_probs = torch.sigmoid(v_activation)\n",
        "        v_samples = torch.bernoulli(v_probs)\n",
        "        return v_samples, v_probs\n",
        "\n",
        "    def cd_k(self, v_data, k=1):\n",
        "        \"\"\"Contrastive Divergence with k steps\"\"\"\n",
        "        h_data, h_data_probs = self.sample_h(v_data)\n",
        "\n",
        "        # Initialize the chain with data samples\n",
        "        h_model = h_data\n",
        "\n",
        "        # Gibbs sampling\n",
        "        for _ in range(k):\n",
        "            v_model, v_model_probs = self.sample_v(h_model)\n",
        "            h_model, h_model_probs = self.sample_h(v_model)\n",
        "\n",
        "        return v_data, h_data_probs, v_model_probs, h_model_probs\n",
        "\n",
        "    def free_energy(self, v):\n",
        "        \"\"\"Calculate free energy\"\"\"\n",
        "        wx_b = torch.matmul(v, self.W) + self.b\n",
        "        hidden_term = torch.sum(F.softplus(wx_b), dim=1)\n",
        "        vbias_term = torch.matmul(v, self.c)\n",
        "        return -hidden_term - vbias_term\n",
        "\n",
        "\n",
        "# Function to pretrain an RBM\n",
        "def pretrain_rbm(rbm, dataloader, device, epochs=5, lr=0.001):\n",
        "    \"\"\"Pretrain RBM using Contrastive Divergence\"\"\"\n",
        "    optimizer = torch.optim.Adam(rbm.parameters(), lr=lr)\n",
        "    rbm.train()\n",
        "\n",
        "    print(f\"Pretraining RBM {rbm.input_size} -> {rbm.hidden_size}...\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        mean_loss = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        for batch_idx, (data, _) in enumerate(dataloader):\n",
        "            data = data.to(device)\n",
        "\n",
        "            # Run k-step Contrastive Divergence\n",
        "            v_data, h_data, v_model, h_model = rbm.cd_k(data, k=1)\n",
        "\n",
        "            # Compute gradients using CD loss\n",
        "            # Positive phase - negative phase\n",
        "            pos_associations = torch.matmul(v_data.t(), h_data)\n",
        "            neg_associations = torch.matmul(v_model.t(), h_model)\n",
        "\n",
        "            # Update weights and biases\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Manually set gradients\n",
        "            rbm.W.grad = -(pos_associations - neg_associations) / data.size(0)\n",
        "            rbm.b.grad = -(h_data - h_model).mean(0)\n",
        "            rbm.c.grad = -(v_data - v_model).mean(0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            # Compute reconstruction error\n",
        "            recon_error = F.mse_loss(v_model, v_data)\n",
        "            mean_loss += recon_error.item()\n",
        "            num_batches += 1\n",
        "\n",
        "        # Print epoch stats\n",
        "        mean_loss /= num_batches\n",
        "        print(f\"  Epoch {epoch+1}/{epochs}, Reconstruction Error: {mean_loss:.6f}\")\n",
        "\n",
        "    return rbm\n",
        "\n",
        "\n",
        "# Improved DBN with proper pretraining\n",
        "class ImprovedDBN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims, output_dim, dropout_rate=0.3):\n",
        "        super(ImprovedDBN, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dims = hidden_dims\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        # Stack of RBM layers\n",
        "        self.rbm_layers = nn.ModuleList()\n",
        "\n",
        "        # First RBM\n",
        "        self.rbm_layers.append(ImprovedRBM(input_dim, hidden_dims[0]))\n",
        "\n",
        "        # Additional RBM layers\n",
        "        for i in range(1, len(hidden_dims)):\n",
        "            self.rbm_layers.append(ImprovedRBM(hidden_dims[i-1], hidden_dims[i]))\n",
        "\n",
        "        # Fully connected layers after RBMs\n",
        "        self.fc_layers = nn.ModuleList()\n",
        "\n",
        "        # First FC layer from last RBM's output\n",
        "        self.fc_layers.append(nn.Sequential(\n",
        "            nn.Linear(hidden_dims[-1], 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Dropout(dropout_rate)\n",
        "        ))\n",
        "\n",
        "        # Second FC layer\n",
        "        self.fc_layers.append(nn.Sequential(\n",
        "            nn.Linear(256, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Dropout(dropout_rate)\n",
        "        ))\n",
        "\n",
        "        # Output layer\n",
        "        self.output_layer = nn.Linear(128, output_dim)\n",
        "\n",
        "        # Better initialization for linear layers\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def pretrain(self, dataloader, device, epochs=5):\n",
        "        \"\"\"Pretrain the DBN layer by layer\"\"\"\n",
        "        print(\"Pretraining DBN layers...\")\n",
        "\n",
        "        # Train first RBM with input data\n",
        "        self.rbm_layers[0] = pretrain_rbm(\n",
        "            self.rbm_layers[0], dataloader, device, epochs)\n",
        "\n",
        "        # For each subsequent layer, train with features from previous layer\n",
        "        for i in range(1, len(self.rbm_layers)):\n",
        "            # Extract features from previous layer\n",
        "            prev_layer_features = []\n",
        "            with torch.no_grad():\n",
        "                for data, _ in dataloader:\n",
        "                    data = data.to(device)\n",
        "\n",
        "                    # Forward pass through previous layers\n",
        "                    for j in range(i):\n",
        "                        data = self.rbm_layers[j](data)\n",
        "\n",
        "                    prev_layer_features.append(data.cpu())\n",
        "\n",
        "            # Create dataset with extracted features\n",
        "            prev_features = torch.cat(prev_layer_features, dim=0)\n",
        "            feature_dataset = TensorDataset(prev_features, torch.zeros(prev_features.size(0)))\n",
        "            feature_loader = DataLoader(\n",
        "                feature_dataset,\n",
        "                batch_size=dataloader.batch_size,\n",
        "                shuffle=True,\n",
        "                num_workers=0\n",
        "            )\n",
        "\n",
        "            # Pretrain this RBM with extracted features\n",
        "            self.rbm_layers[i] = pretrain_rbm(\n",
        "                self.rbm_layers[i], feature_loader, device, epochs)\n",
        "\n",
        "        print(\"DBN pretraining complete.\")\n",
        "        return self\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass through the DBN\"\"\"\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Flatten input if not already flat\n",
        "        if x.dim() > 2:\n",
        "            x = x.view(batch_size, -1)\n",
        "\n",
        "        # Forward through RBM layers\n",
        "        for rbm in self.rbm_layers:\n",
        "            x = rbm(x)\n",
        "\n",
        "        # Forward through FC layers\n",
        "        for fc in self.fc_layers:\n",
        "            x = fc(x)\n",
        "\n",
        "        # Output layer\n",
        "        x = self.output_layer(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Improved dynamic ensemble model\n",
        "class ImprovedDynamicEnsembleModel(nn.Module):\n",
        "    def __init__(self, cnn_model, dbn_model, output_dim):\n",
        "        super(ImprovedDynamicEnsembleModel, self).__init__()\n",
        "        self.cnn_model = cnn_model\n",
        "        self.dbn_model = dbn_model\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        # Class-wise attention - one attention weight per class\n",
        "        self.class_attention = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(2, 16),\n",
        "                nn.LeakyReLU(0.1),\n",
        "                nn.Linear(16, 2),\n",
        "                nn.Softmax(dim=1)\n",
        "            ) for _ in range(output_dim)\n",
        "        ])\n",
        "\n",
        "        # Gating network to decide contribution from each model\n",
        "        self.gate_network = nn.Sequential(\n",
        "            nn.Linear(output_dim * 2, 64),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Linear(64, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Combiner network\n",
        "        self.combiner = nn.Sequential(\n",
        "            nn.Linear(output_dim * 2, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Save original input for DBN\n",
        "        x_original = x.clone()\n",
        "\n",
        "        # Forward pass through CNN\n",
        "        cnn_logits = self.cnn_model(x_original)\n",
        "\n",
        "        # Forward pass through DBN\n",
        "        dbn_logits = self.dbn_model(x_original)\n",
        "\n",
        "        # Get probabilities\n",
        "        cnn_probs = F.softmax(cnn_logits, dim=1)\n",
        "        dbn_probs = F.softmax(dbn_logits, dim=1)\n",
        "\n",
        "        # Confidence values\n",
        "        cnn_conf = torch.max(cnn_probs, dim=1, keepdim=True)[0]\n",
        "        dbn_conf = torch.max(dbn_probs, dim=1, keepdim=True)[0]\n",
        "\n",
        "        # Calculate gating value to balance models\n",
        "        model_confs = torch.cat([cnn_logits, dbn_logits], dim=1)\n",
        "        gate_value = self.gate_network(model_confs)\n",
        "\n",
        "        # Class-wise weighting\n",
        "        weighted_outputs = []\n",
        "        for i in range(self.output_dim):\n",
        "            # Get logits for this class\n",
        "            class_values = torch.cat([\n",
        "                cnn_logits[:, i:i+1],\n",
        "                dbn_logits[:, i:i+1]\n",
        "            ], dim=1)\n",
        "\n",
        "            # Get weights for this class\n",
        "            weights = self.class_attention[i](class_values)\n",
        "\n",
        "            # Weight the outputs\n",
        "            weighted_class = (weights[:, 0:1] * cnn_logits[:, i:i+1] +\n",
        "                             weights[:, 1:2] * dbn_logits[:, i:i+1])\n",
        "            weighted_outputs.append(weighted_class)\n",
        "\n",
        "        # Stack all class outputs\n",
        "        class_weighted_output = torch.cat(weighted_outputs, dim=1)\n",
        "\n",
        "        # Combine with original outputs\n",
        "        combined = torch.cat([cnn_logits, dbn_logits], dim=1)\n",
        "\n",
        "        # Dynamic final output\n",
        "        final_output = gate_value * self.combiner(combined) + (1 - gate_value) * class_weighted_output\n",
        "\n",
        "        return final_output\n",
        "\n",
        "\n",
        "# Focal Loss implementation for handling class imbalance\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha  # Class weights\n",
        "        self.gamma = gamma  # Focusing parameter\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        # Standard cross entropy\n",
        "        ce_loss = F.cross_entropy(\n",
        "            inputs, targets, weight=self.alpha,\n",
        "            reduction='none'\n",
        "        )\n",
        "\n",
        "        # Get probabilities\n",
        "        pt = torch.exp(-ce_loss)\n",
        "\n",
        "        # Calculate focal term\n",
        "        focal_term = (1 - pt) ** self.gamma\n",
        "\n",
        "        # Apply focal term to CE loss\n",
        "        loss = focal_term * ce_loss\n",
        "\n",
        "        # Apply reduction\n",
        "        if self.reduction == 'mean':\n",
        "            return loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return loss.sum()\n",
        "        else:\n",
        "            return loss\n",
        "\n",
        "\n",
        "# Mixup data augmentation function\n",
        "def mixup_data(x, y, alpha=0.2, device='cuda'):\n",
        "    \"\"\"Mixup data augmentation function\"\"\"\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "\n",
        "    batch_size = x.size(0)\n",
        "    index = torch.randperm(batch_size).to(device)\n",
        "\n",
        "    mixed_x = lam * x + (1 - lam) * x[index]\n",
        "    y_a, y_b = y, y[index]\n",
        "\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "\n",
        "# Mixup criterion function\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    \"\"\"Criterion for mixup data augmentation\"\"\"\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "\n",
        "# Training function with advanced techniques\n",
        "def train_ensemble_model(ensemble_model, train_loader, val_loader, test_loader,\n",
        "                        device, class_weights=None, epochs=30, lr=0.001,\n",
        "                        focal_gamma=2.0, mixup_alpha=0.2, save_dir='.'):\n",
        "    print(\"Starting advanced training process...\")\n",
        "\n",
        "    # Create save directory if it doesn't exist\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Setup optimizer with weight decay\n",
        "    optimizer = optim.AdamW(ensemble_model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=3, verbose=True, min_lr=1e-6\n",
        "    )\n",
        "\n",
        "    # Loss function - use focal loss for imbalanced classes\n",
        "    if class_weights is not None:\n",
        "        class_weights = class_weights.to(device)\n",
        "\n",
        "    criterion = FocalLoss(alpha=class_weights, gamma=focal_gamma)\n",
        "\n",
        "    # Tracking metrics\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "    best_val_acc = 0.0\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "\n",
        "    # Training loop with early stopping\n",
        "    for epoch in range(epochs):\n",
        "        # Training phase\n",
        "        start_time = time.time()\n",
        "        ensemble_model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        # Progress bar for training\n",
        "        train_iterator = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n",
        "\n",
        "        for inputs, labels in train_iterator:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Apply mixup augmentation randomly with 50% probability\n",
        "            use_mixup = np.random.random() < 0.5 and mixup_alpha > 0\n",
        "\n",
        "            if use_mixup:\n",
        "                inputs, labels_a, labels_b, lam = mixup_data(inputs, labels, mixup_alpha, device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = ensemble_model(inputs)\n",
        "                loss = mixup_criterion(criterion, outputs, labels_a, labels_b, lam)\n",
        "            else:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = ensemble_model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping to avoid exploding gradients\n",
        "            torch.nn.utils.clip_grad_norm_(ensemble_model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update statistics\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy only for non-mixup batches\n",
        "            if not use_mixup:\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Calculate epoch metrics\n",
        "        epoch_train_loss = running_loss / len(train_loader)\n",
        "\n",
        "        # If we used mixup on all batches, run a separate evaluation pass\n",
        "        if total == 0:\n",
        "            ensemble_model.eval()\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            with torch.no_grad():\n",
        "                for inputs, labels in train_loader:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "                    outputs = ensemble_model(inputs)\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    total += labels.size(0)\n",
        "                    correct += (predicted == labels).sum().item()\n",
        "\n",
        "        epoch_train_acc = 100.0 * correct / total\n",
        "        train_losses.append(epoch_train_loss)\n",
        "        train_accs.append(epoch_train_acc)\n",
        "\n",
        "        # Validation phase\n",
        "        ensemble_model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        all_preds = []\n",
        "        all_targets = []\n",
        "\n",
        "        # Progress bar for validation\n",
        "        val_iterator = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_iterator:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = ensemble_model(inputs)\n",
        "\n",
        "                # Calculate loss\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                # Calculate accuracy\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "                # Save predictions and targets for confusion matrix\n",
        "                all_preds.extend(predicted.cpu().numpy())\n",
        "                all_targets.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Calculate validation metrics\n",
        "        epoch_val_loss = val_loss / len(val_loader)\n",
        "        epoch_val_acc = 100.0 * val_correct / val_total\n",
        "        val_losses.append(epoch_val_loss)\n",
        "        val_accs.append(epoch_val_acc)\n",
        "\n",
        "        # Update learning rate based on validation loss\n",
        "        scheduler.step(epoch_val_loss)\n",
        "\n",
        "        # Calculate epoch duration\n",
        "        epoch_time = time.time() - start_time\n",
        "\n",
        "        # Print epoch results\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Time: {epoch_time:.1f}s | \"\n",
        "              f\"Train Loss: {epoch_train_loss:.4f} | Train Acc: {epoch_train_acc:.2f}% | \"\n",
        "              f\"Val Loss: {epoch_val_loss:.4f} | Val Acc: {epoch_val_acc:.2f}%\")\n",
        "\n",
        "        # Check if this is the best model so far\n",
        "        if epoch_val_acc > best_val_acc:\n",
        "            best_val_acc = epoch_val_acc\n",
        "            best_val_loss = epoch_val_loss\n",
        "            patience_counter = 0\n",
        "\n",
        "            # Save the best model\n",
        "            torch.save(ensemble_model.state_dict(), os.path.join(save_dir, 'best_model.pt'))\n",
        "            print(f\"‚úÖ New best model saved with Val Acc: {epoch_val_acc:.2f}%\")\n",
        "\n",
        "            # Generate confusion matrix for best model\n",
        "            try:\n",
        "                cm = confusion_matrix(all_targets, all_preds)\n",
        "                plt.figure(figsize=(12, 10))\n",
        "                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "                plt.title(f'Validation Confusion Matrix - Epoch {epoch+1}')\n",
        "                plt.xlabel('Predicted')\n",
        "                plt.ylabel('True')\n",
        "                plt.savefig(os.path.join(save_dir, f'confusion_matrix_epoch_{epoch+1}.png'))\n",
        "                plt.close()\n",
        "\n",
        "                # Print detailed classification report\n",
        "                print(\"\\nClassification Report:\")\n",
        "                print(classification_report(all_targets, all_preds))\n",
        "            except Exception as e:\n",
        "                print(f\"Could not generate confusion matrix: {e}\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        # Early stopping check\n",
        "        if patience_counter >= 7:  # Stop if no improvement for 7 epochs\n",
        "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
        "            break\n",
        "\n",
        "    # Final evaluation on test set\n",
        "    print(\"\\nEvaluating final model on test set...\")\n",
        "    ensemble_model.load_state_dict(torch.load(os.path.join(save_dir, 'best_model.pt')))\n",
        "    ensemble_model.eval()\n",
        "    test_correct = 0\n",
        "    test_total = 0\n",
        "    all_test_preds = []\n",
        "    all_test_targets = []\n",
        "    all_test_scores = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = ensemble_model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            test_total += labels.size(0)\n",
        "            test_correct += (predicted == labels).sum().item()\n",
        "            all_test_preds.extend(predicted.cpu().numpy())\n",
        "            all_test_targets.extend(labels.cpu().numpy())\n",
        "            all_test_scores.extend(F.softmax(outputs, dim=1).cpu().numpy())\n",
        "\n",
        "    test_acc = 100.0 * test_correct / test_total\n",
        "    print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "    # Calculate metrics per class\n",
        "    from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
        "\n",
        "    # Convert to 1D arrays\n",
        "    y_true = np.array(all_test_targets)\n",
        "    y_pred = np.array(all_test_preds)\n",
        "    y_score = np.array(all_test_scores)\n",
        "\n",
        "    # Calculate precision, recall, and F1-score per class\n",
        "    precision, recall, f1, support = precision_recall_fscore_support(\n",
        "        y_true, y_pred, average=None)\n",
        "\n",
        "    # Calculate metrics\n",
        "    weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(\n",
        "        y_true, y_pred, average='weighted')\n",
        "\n",
        "    # Attempt to calculate ROC AUC score\n",
        "    try:\n",
        "        roc_auc = roc_auc_score(y_true, y_score, multi_class='ovr', average='weighted')\n",
        "        print(f\"ROC AUC Score (weighted): {roc_auc:.4f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not calculate ROC AUC: {e}\")\n",
        "\n",
        "    print(f\"Weighted Precision: {weighted_precision:.4f}\")\n",
        "    print(f\"Weighted Recall: {weighted_recall:.4f}\")\n",
        "    print(f\"Weighted F1 Score: {weighted_f1:.4f}\")\n",
        "\n",
        "    # Generate final confusion matrix and report\n",
        "    try:\n",
        "        cm = confusion_matrix(all_test_targets, all_test_preds)\n",
        "        plt.figure(figsize=(14, 12))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "        plt.title('Final Test Confusion Matrix')\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('True')\n",
        "        plt.savefig(os.path.join(save_dir, 'final_test_confusion_matrix.png'))\n",
        "        plt.close()\n",
        "\n",
        "        # Per-class metrics visualization\n",
        "        plt.figure(figsize=(14, 8))\n",
        "        indices = np.arange(len(precision))\n",
        "        width = 0.25\n",
        "\n",
        "        plt.bar(indices - width, precision, width, label='Precision')\n",
        "        plt.bar(indices, recall, width, label='Recall')\n",
        "        plt.bar(indices + width, f1, width, label='F1-Score')\n",
        "\n",
        "        plt.xlabel('Class')\n",
        "        plt.ylabel('Score')\n",
        "        plt.title('Per-Class Classification Metrics')\n",
        "        plt.xticks(indices)\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(save_dir, 'per_class_metrics.png'))\n",
        "        plt.close()\n",
        "\n",
        "        print(\"\\nFinal Classification Report:\")\n",
        "        print(classification_report(all_test_targets, all_test_preds))\n",
        "    except Exception as e:\n",
        "        print(f\"Could not generate final confusion matrix: {e}\")\n",
        "\n",
        "    # Plot training history\n",
        "    plt.figure(figsize=(15, 6))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.title('Loss over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(train_accs, label='Training Accuracy')\n",
        "    plt.plot(val_accs, label='Validation Accuracy')\n",
        "    plt.title('Accuracy over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(save_dir, 'training_history.png'))\n",
        "    plt.close()\n",
        "\n",
        "    return ensemble_model, test_acc\n"
      ],
      "metadata": {
        "id": "71EORVoQ9Y9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running Preprocessing dan CNN+DBN"
      ],
      "metadata": {
        "id": "mXGcyX409kyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function to run the whole pipeline\n",
        "def run_improved_iot_anomaly_detection(train_data, test_data, save_dir='model_results'):\n",
        "    print(\"\\nüöÄ Starting improved IoT network anomaly detection pipeline...\")\n",
        "\n",
        "    # Step 1: Preprocessing data\n",
        "    print(\"\\nüìä Running enhanced data preprocessing...\")\n",
        "    processed_data = enhanced_preprocessing(train_data, test_data)\n",
        "\n",
        "    # Extract necessary components from processed data\n",
        "    train_loader = processed_data['train_loader']\n",
        "    val_loader = processed_data['val_loader']\n",
        "    test_loader = processed_data['test_loader']\n",
        "    input_dim = processed_data['input_dim']\n",
        "    num_classes = processed_data['num_classes']\n",
        "    class_weights = processed_data['class_weights']\n",
        "\n",
        "    # Step 2: Set up device for training\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    class_weights = class_weights.to(device)\n",
        "    print(f\"\\nüîß Using device: {device}\")\n",
        "\n",
        "    # Step 3: Create improved CNN model\n",
        "    print(\"\\nüß† Creating improved CNN model...\")\n",
        "    cnn_model = RevisedCNN(\n",
        "        input_dim=input_dim,\n",
        "        output_dim=num_classes,\n",
        "        dropout_rate=0.3\n",
        "    ).to(device)\n",
        "    print(f\"CNN model created with {input_dim} input features and {num_classes} output classes\")\n",
        "\n",
        "    # Step 4: Create improved DBN model with pretraining\n",
        "    print(\"\\nüß† Creating improved DBN model...\")\n",
        "    dbn_model = ImprovedDBN(\n",
        "        input_dim=input_dim,\n",
        "        hidden_dims=[512, 384, 256],\n",
        "        output_dim=num_classes,\n",
        "        dropout_rate=0.3\n",
        "    ).to(device)\n",
        "\n",
        "    # Pretrain DBN layers\n",
        "    dbn_model.pretrain(train_loader, device, epochs=3)\n",
        "    print(\"DBN model created and pretrained\")\n",
        "\n",
        "    # Step 5: Create improved ensemble model\n",
        "    print(\"\\nüß† Creating improved dynamic ensemble model...\")\n",
        "    ensemble_model = ImprovedDynamicEnsembleModel(\n",
        "        cnn_model=cnn_model,\n",
        "        dbn_model=dbn_model,\n",
        "        output_dim=num_classes\n",
        "    ).to(device)\n",
        "    print(\"Ensemble model created\")\n",
        "\n",
        "    # Step 6: Train the ensemble model\n",
        "    print(\"\\nüèãÔ∏è‚Äç‚ôÄÔ∏è Starting advanced training process...\")\n",
        "    trained_model, test_accuracy = train_ensemble_model(\n",
        "        ensemble_model=ensemble_model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        test_loader=test_loader,\n",
        "        device=device,\n",
        "        class_weights=class_weights,\n",
        "        epochs=30,\n",
        "        lr=0.001,\n",
        "        focal_gamma=2.0,\n",
        "        mixup_alpha=0.2,\n",
        "        save_dir=save_dir\n",
        "    )\n",
        "\n",
        "    print(f\"\\nüèÅ Final test accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "    # Return the trained model and results\n",
        "    results = {\n",
        "        'model': trained_model,\n",
        "        'test_accuracy': test_accuracy,\n",
        "        'input_dim': input_dim,\n",
        "        'num_classes': num_classes,\n",
        "        'model_path': os.path.join(save_dir, 'best_model.pt')\n",
        "    }\n",
        "\n",
        "    return results\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Load dataset UNSW-NB15\n",
        "# Asumsikan Anda memiliki file CSV untuk training dan testing\n",
        "train_data = pd.read_csv('UNSW-NB15_training-set.csv')\n",
        "test_data = pd.read_csv('UNSW-NB15_testing-set.csv')\n",
        "\n",
        "# 2. Jalankan pipeline pemrosesan dan model\n",
        "results = run_improved_iot_anomaly_detection(train_data, test_data, save_dir='iot_anomaly_results')\n",
        "\n",
        "print(f\"Final model accuracy: {results['test_accuracy']:.2f}%\")\n",
        "print(f\"Model saved to: {results['model_path']}\")"
      ],
      "metadata": {
        "id": "_Az7ztq69efe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}