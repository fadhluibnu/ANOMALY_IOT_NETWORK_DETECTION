{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "FPnovK0Nggao"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fadhluibnu/ANOMALY_IOT_NETWORK_DETECTION/blob/main/Copy_of_IOT_ANOMALI_DETECTION_GROX_GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrQzerAvId5N",
        "outputId": "0527e40c-aebb-47a9-d3bc-48b051d05c94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHNc1DKfMvOS",
        "outputId": "ced76b7d-b176-4d84-b4f0-0c596ade5062"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“Š Training Data Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 82332 entries, 0 to 82331\n",
            "Data columns (total 45 columns):\n",
            " #   Column             Non-Null Count  Dtype  \n",
            "---  ------             --------------  -----  \n",
            " 0   id                 82332 non-null  int64  \n",
            " 1   dur                82332 non-null  float64\n",
            " 2   proto              82332 non-null  object \n",
            " 3   service            82332 non-null  object \n",
            " 4   state              82332 non-null  object \n",
            " 5   spkts              82332 non-null  int64  \n",
            " 6   dpkts              82332 non-null  int64  \n",
            " 7   sbytes             82332 non-null  int64  \n",
            " 8   dbytes             82332 non-null  int64  \n",
            " 9   rate               82332 non-null  float64\n",
            " 10  sttl               82332 non-null  int64  \n",
            " 11  dttl               82332 non-null  int64  \n",
            " 12  sload              82332 non-null  float64\n",
            " 13  dload              82332 non-null  float64\n",
            " 14  sloss              82332 non-null  int64  \n",
            " 15  dloss              82332 non-null  int64  \n",
            " 16  sinpkt             82332 non-null  float64\n",
            " 17  dinpkt             82332 non-null  float64\n",
            " 18  sjit               82332 non-null  float64\n",
            " 19  djit               82332 non-null  float64\n",
            " 20  swin               82332 non-null  int64  \n",
            " 21  stcpb              82332 non-null  int64  \n",
            " 22  dtcpb              82332 non-null  int64  \n",
            " 23  dwin               82332 non-null  int64  \n",
            " 24  tcprtt             82332 non-null  float64\n",
            " 25  synack             82332 non-null  float64\n",
            " 26  ackdat             82332 non-null  float64\n",
            " 27  smean              82332 non-null  int64  \n",
            " 28  dmean              82332 non-null  int64  \n",
            " 29  trans_depth        82332 non-null  int64  \n",
            " 30  response_body_len  82332 non-null  int64  \n",
            " 31  ct_srv_src         82332 non-null  int64  \n",
            " 32  ct_state_ttl       82332 non-null  int64  \n",
            " 33  ct_dst_ltm         82332 non-null  int64  \n",
            " 34  ct_src_dport_ltm   82332 non-null  int64  \n",
            " 35  ct_dst_sport_ltm   82332 non-null  int64  \n",
            " 36  ct_dst_src_ltm     82332 non-null  int64  \n",
            " 37  is_ftp_login       82332 non-null  int64  \n",
            " 38  ct_ftp_cmd         82332 non-null  int64  \n",
            " 39  ct_flw_http_mthd   82332 non-null  int64  \n",
            " 40  ct_src_ltm         82332 non-null  int64  \n",
            " 41  ct_srv_dst         82332 non-null  int64  \n",
            " 42  is_sm_ips_ports    82332 non-null  int64  \n",
            " 43  attack_cat         82332 non-null  object \n",
            " 44  label              82332 non-null  int64  \n",
            "dtypes: float64(11), int64(30), object(4)\n",
            "memory usage: 28.3+ MB\n",
            "None\n",
            "\n",
            "ðŸ“Š Test Data Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 175341 entries, 0 to 175340\n",
            "Data columns (total 45 columns):\n",
            " #   Column             Non-Null Count   Dtype  \n",
            "---  ------             --------------   -----  \n",
            " 0   id                 175341 non-null  int64  \n",
            " 1   dur                175341 non-null  float64\n",
            " 2   proto              175341 non-null  object \n",
            " 3   service            175341 non-null  object \n",
            " 4   state              175341 non-null  object \n",
            " 5   spkts              175341 non-null  int64  \n",
            " 6   dpkts              175341 non-null  int64  \n",
            " 7   sbytes             175341 non-null  int64  \n",
            " 8   dbytes             175341 non-null  int64  \n",
            " 9   rate               175341 non-null  float64\n",
            " 10  sttl               175341 non-null  int64  \n",
            " 11  dttl               175341 non-null  int64  \n",
            " 12  sload              175341 non-null  float64\n",
            " 13  dload              175341 non-null  float64\n",
            " 14  sloss              175341 non-null  int64  \n",
            " 15  dloss              175341 non-null  int64  \n",
            " 16  sinpkt             175341 non-null  float64\n",
            " 17  dinpkt             175341 non-null  float64\n",
            " 18  sjit               175341 non-null  float64\n",
            " 19  djit               175341 non-null  float64\n",
            " 20  swin               175341 non-null  int64  \n",
            " 21  stcpb              175341 non-null  int64  \n",
            " 22  dtcpb              175341 non-null  int64  \n",
            " 23  dwin               175341 non-null  int64  \n",
            " 24  tcprtt             175341 non-null  float64\n",
            " 25  synack             175341 non-null  float64\n",
            " 26  ackdat             175341 non-null  float64\n",
            " 27  smean              175341 non-null  int64  \n",
            " 28  dmean              175341 non-null  int64  \n",
            " 29  trans_depth        175341 non-null  int64  \n",
            " 30  response_body_len  175341 non-null  int64  \n",
            " 31  ct_srv_src         175341 non-null  int64  \n",
            " 32  ct_state_ttl       175341 non-null  int64  \n",
            " 33  ct_dst_ltm         175341 non-null  int64  \n",
            " 34  ct_src_dport_ltm   175341 non-null  int64  \n",
            " 35  ct_dst_sport_ltm   175341 non-null  int64  \n",
            " 36  ct_dst_src_ltm     175341 non-null  int64  \n",
            " 37  is_ftp_login       175341 non-null  int64  \n",
            " 38  ct_ftp_cmd         175341 non-null  int64  \n",
            " 39  ct_flw_http_mthd   175341 non-null  int64  \n",
            " 40  ct_src_ltm         175341 non-null  int64  \n",
            " 41  ct_srv_dst         175341 non-null  int64  \n",
            " 42  is_sm_ips_ports    175341 non-null  int64  \n",
            " 43  attack_cat         175341 non-null  object \n",
            " 44  label              175341 non-null  int64  \n",
            "dtypes: float64(11), int64(30), object(4)\n",
            "memory usage: 60.2+ MB\n",
            "None\n",
            "\n",
            "Training Data Shape: (82332, 45)\n",
            "Test Data Shape: (175341, 45)\n",
            "\n",
            "Missing Values in Training Data:\n",
            "id                   0\n",
            "dur                  0\n",
            "proto                0\n",
            "service              0\n",
            "state                0\n",
            "spkts                0\n",
            "dpkts                0\n",
            "sbytes               0\n",
            "dbytes               0\n",
            "rate                 0\n",
            "sttl                 0\n",
            "dttl                 0\n",
            "sload                0\n",
            "dload                0\n",
            "sloss                0\n",
            "dloss                0\n",
            "sinpkt               0\n",
            "dinpkt               0\n",
            "sjit                 0\n",
            "djit                 0\n",
            "swin                 0\n",
            "stcpb                0\n",
            "dtcpb                0\n",
            "dwin                 0\n",
            "tcprtt               0\n",
            "synack               0\n",
            "ackdat               0\n",
            "smean                0\n",
            "dmean                0\n",
            "trans_depth          0\n",
            "response_body_len    0\n",
            "ct_srv_src           0\n",
            "ct_state_ttl         0\n",
            "ct_dst_ltm           0\n",
            "ct_src_dport_ltm     0\n",
            "ct_dst_sport_ltm     0\n",
            "ct_dst_src_ltm       0\n",
            "is_ftp_login         0\n",
            "ct_ftp_cmd           0\n",
            "ct_flw_http_mthd     0\n",
            "ct_src_ltm           0\n",
            "ct_srv_dst           0\n",
            "is_sm_ips_ports      0\n",
            "attack_cat           0\n",
            "label                0\n",
            "dtype: int64\n",
            "\n",
            "Missing Values in Test Data:\n",
            "id                   0\n",
            "dur                  0\n",
            "proto                0\n",
            "service              0\n",
            "state                0\n",
            "spkts                0\n",
            "dpkts                0\n",
            "sbytes               0\n",
            "dbytes               0\n",
            "rate                 0\n",
            "sttl                 0\n",
            "dttl                 0\n",
            "sload                0\n",
            "dload                0\n",
            "sloss                0\n",
            "dloss                0\n",
            "sinpkt               0\n",
            "dinpkt               0\n",
            "sjit                 0\n",
            "djit                 0\n",
            "swin                 0\n",
            "stcpb                0\n",
            "dtcpb                0\n",
            "dwin                 0\n",
            "tcprtt               0\n",
            "synack               0\n",
            "ackdat               0\n",
            "smean                0\n",
            "dmean                0\n",
            "trans_depth          0\n",
            "response_body_len    0\n",
            "ct_srv_src           0\n",
            "ct_state_ttl         0\n",
            "ct_dst_ltm           0\n",
            "ct_src_dport_ltm     0\n",
            "ct_dst_sport_ltm     0\n",
            "ct_dst_src_ltm       0\n",
            "is_ftp_login         0\n",
            "ct_ftp_cmd           0\n",
            "ct_flw_http_mthd     0\n",
            "ct_src_ltm           0\n",
            "ct_srv_dst           0\n",
            "is_sm_ips_ports      0\n",
            "attack_cat           0\n",
            "label                0\n",
            "dtype: int64\n",
            "\n",
            "After Removing Duplicates - Train Shape: (82332, 45)\n",
            "After Removing Duplicates - Test Shape: (175341, 45)\n",
            "\n",
            "Shape after encoding - Train: (82332, 193)\n",
            "Shape after encoding - Test: (175341, 193)\n",
            "\n",
            "Max values after normalization: dur                   12.523997\n",
            "spkts                 79.358479\n",
            "dpkts                 95.181552\n",
            "sbytes                83.591693\n",
            "dbytes                96.680830\n",
            "rate                   6.174084\n",
            "sttl                   0.729291\n",
            "dttl                   1.348170\n",
            "sload                 28.930445\n",
            "dload                  8.437392\n",
            "sloss                 82.201235\n",
            "dloss                 98.742061\n",
            "sinpkt                 9.584124\n",
            "dinpkt                44.582828\n",
            "sjit                  26.046759\n",
            "djit                 127.270427\n",
            "swin                   0.954338\n",
            "stcpb                  2.308161\n",
            "dtcpb                  2.331002\n",
            "dwin                   0.993904\n",
            "tcprtt                32.455657\n",
            "synack                45.129041\n",
            "ackdat                52.676350\n",
            "smean                  6.545144\n",
            "dmean                  5.657121\n",
            "trans_depth          241.114860\n",
            "response_body_len    137.686717\n",
            "ct_srv_src             4.819867\n",
            "ct_state_ttl           4.339212\n",
            "ct_dst_ltm             6.326288\n",
            "ct_src_dport_ltm       6.445097\n",
            "ct_dst_sport_ltm       5.804726\n",
            "ct_dst_src_ltm         4.865795\n",
            "is_ftp_login          21.846029\n",
            "ct_ftp_cmd            21.534625\n",
            "ct_flw_http_mthd      24.848577\n",
            "ct_src_ltm             6.265499\n",
            "ct_srv_dst             4.750840\n",
            "is_sm_ips_ports        9.427730\n",
            "dtype: float64\n",
            "Min values after normalization: dur                 -0.213730\n",
            "spkts               -0.131922\n",
            "dpkts               -0.151816\n",
            "sbytes              -0.046434\n",
            "dbytes              -0.087369\n",
            "rate                -0.554509\n",
            "sttl                -1.782709\n",
            "dttl                -0.820395\n",
            "sload               -0.358883\n",
            "dload               -0.263498\n",
            "sloss               -0.073531\n",
            "dloss               -0.113244\n",
            "sinpkt              -0.122181\n",
            "dinpkt              -0.094169\n",
            "sjit                -0.112177\n",
            "djit                -0.147218\n",
            "swin                -1.047920\n",
            "stcpb               -0.779840\n",
            "dtcpb               -0.776754\n",
            "dwin                -1.006244\n",
            "tcprtt              -0.482025\n",
            "synack              -0.412910\n",
            "ackdat              -0.484073\n",
            "smean               -0.554172\n",
            "dmean               -0.475371\n",
            "trans_depth         -0.173648\n",
            "response_body_len   -0.041910\n",
            "ct_srv_src          -0.770643\n",
            "ct_state_ttl        -1.283074\n",
            "ct_dst_ltm          -0.563660\n",
            "ct_src_dport_ltm    -0.468312\n",
            "ct_dst_sport_ltm    -0.450186\n",
            "ct_dst_src_ltm      -0.565597\n",
            "is_ftp_login        -0.090857\n",
            "ct_ftp_cmd          -0.090617\n",
            "ct_flw_http_mthd    -0.203143\n",
            "ct_src_ltm          -0.640033\n",
            "ct_srv_dst          -0.734107\n",
            "is_sm_ips_ports     -0.106070\n",
            "dtype: float64\n",
            "\n",
            "After Clipping Outliers - Train Shape: (82332, 193)\n",
            "After Clipping Outliers - Test Shape: (175341, 193)\n",
            "\n",
            "Jumlah komponen utama setelah PCA: 146\n",
            "\n",
            "Label Distribution After SMOTE:\n",
            "attack_cat\n",
            "0    37000\n",
            "5    37000\n",
            "7    37000\n",
            "4    37000\n",
            "2    37000\n",
            "6    37000\n",
            "3    37000\n",
            "9    37000\n",
            "8    37000\n",
            "1    37000\n",
            "Name: count, dtype: int64\n",
            "\n",
            "NaN in X_train_tensor: tensor(False)\n",
            "Inf in X_train_tensor: tensor(False)\n",
            "\n",
            "Training DataLoader - Number of batches: 5782\n",
            "Test DataLoader - Number of batches: 2740\n",
            "\n",
            "X_train_tensor Shape After Unsqueeze: torch.Size([370000, 1, 146])\n",
            "X_test_tensor Shape After Unsqueeze: torch.Size([175341, 1, 146])\n",
            "ðŸš€ Using Device: cpu\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from scipy.stats import zscore\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.decomposition import PCA\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Pemuatan Data\n",
        "train_file = \"/content/drive/MyDrive/SEMESTER 4/RTI/ANOMALI DETECTION/UNSW_NB15_training-set.csv\"\n",
        "test_file = \"/content/drive/MyDrive/SEMESTER 4/RTI/ANOMALI DETECTION/UNSW_NB15_testing-set.csv\"\n",
        "train_data = pd.read_csv(train_file, on_bad_lines=\"skip\", low_memory=False)\n",
        "test_data = pd.read_csv(test_file, on_bad_lines=\"skip\", low_memory=False)\n",
        "\n",
        "# Eksplorasi Data\n",
        "print(\"\\nðŸ“Š Training Data Info:\")\n",
        "print(train_data.info())\n",
        "print(\"\\nðŸ“Š Test Data Info:\")\n",
        "print(test_data.info())\n",
        "print(\"\\nTraining Data Shape:\", train_data.shape)\n",
        "print(\"Test Data Shape:\", test_data.shape)\n",
        "print(\"\\nMissing Values in Training Data:\")\n",
        "print(train_data.isnull().sum())\n",
        "print(\"\\nMissing Values in Test Data:\")\n",
        "print(test_data.isnull().sum())\n",
        "\n",
        "# Penanganan Nilai Hilang\n",
        "train_data = train_data.dropna(subset=['attack_cat'])\n",
        "test_data = test_data.dropna(subset=['attack_cat'])\n",
        "numerical_cols = train_data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "numerical_cols = [col for col in numerical_cols if col not in ['id', 'label', 'attack_cat']]\n",
        "for col in numerical_cols:\n",
        "    train_data[col] = train_data[col].fillna(train_data[col].median())\n",
        "    test_data[col] = test_data[col].fillna(train_data[col].median())\n",
        "\n",
        "# Penanganan Duplikasi\n",
        "train_data = train_data.drop_duplicates()\n",
        "test_data = test_data.drop_duplicates()\n",
        "print(\"\\nAfter Removing Duplicates - Train Shape:\", train_data.shape)\n",
        "print(\"After Removing Duplicates - Test Shape:\", test_data.shape)\n",
        "\n",
        "# Encoding attack_cat\n",
        "attack_mapping = {\n",
        "    'Normal': 0, 'Generic': 1, 'Exploits': 2, 'Fuzzers': 3, 'DoS': 4,\n",
        "    'Reconnaissance': 5, 'Analysis': 6, 'Backdoor': 7, 'Shellcode': 8, 'Worms': 9\n",
        "}\n",
        "train_data['attack_cat'] = train_data['attack_cat'].map(attack_mapping)\n",
        "test_data['attack_cat'] = test_data['attack_cat'].map(attack_mapping)\n",
        "\n",
        "# Encoding Fitur Kategorikal\n",
        "categorical_cols = train_data.select_dtypes(include=['object']).columns\n",
        "encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "encoded_train = encoder.fit_transform(train_data[categorical_cols])\n",
        "encoded_test = encoder.transform(test_data[categorical_cols])\n",
        "encoded_train = pd.DataFrame(encoded_train, columns=encoder.get_feature_names_out(categorical_cols))\n",
        "encoded_test = pd.DataFrame(encoded_test, columns=encoder.get_feature_names_out(categorical_cols))\n",
        "train_data = train_data.drop(columns=categorical_cols).reset_index(drop=True)\n",
        "test_data = test_data.drop(columns=categorical_cols).reset_index(drop=True)\n",
        "train_data = pd.concat([train_data, encoded_train], axis=1)\n",
        "test_data = pd.concat([test_data, encoded_test], axis=1)\n",
        "print(\"\\nShape after encoding - Train:\", train_data.shape)\n",
        "print(\"Shape after encoding - Test:\", test_data.shape)\n",
        "\n",
        "# Normalisasi\n",
        "scaler = StandardScaler()\n",
        "train_data[numerical_cols] = scaler.fit_transform(train_data[numerical_cols])\n",
        "test_data[numerical_cols] = scaler.transform(test_data[numerical_cols])\n",
        "print(\"\\nMax values after normalization:\", train_data[numerical_cols].max())\n",
        "print(\"Min values after normalization:\", train_data[numerical_cols].min())\n",
        "\n",
        "# Penanganan Outlier dengan Clipping\n",
        "for col in numerical_cols:\n",
        "    lower_bound = train_data[col].quantile(0.05)\n",
        "    upper_bound = train_data[col].quantile(0.95)\n",
        "    train_data[col] = train_data[col].clip(lower=lower_bound, upper=upper_bound)\n",
        "    test_data[col] = test_data[col].clip(lower=lower_bound, upper=upper_bound)\n",
        "print(\"\\nAfter Clipping Outliers - Train Shape:\", train_data.shape)\n",
        "print(\"After Clipping Outliers - Test Shape:\", test_data.shape)\n",
        "\n",
        "# Seleksi Fitur dengan PCA (atau HBA sebagai alternatif)\n",
        "X_train_pca = train_data.drop(columns=['id', 'label', 'attack_cat'])\n",
        "X_test_pca = test_data.drop(columns=['id', 'label', 'attack_cat'])\n",
        "scaler = StandardScaler()\n",
        "X_train_pca_scaled = scaler.fit_transform(X_train_pca)\n",
        "X_test_pca_scaled = scaler.transform(X_test_pca)\n",
        "pca = PCA(n_components=0.95)\n",
        "X_train = pca.fit_transform(X_train_pca_scaled)\n",
        "X_test = pca.transform(X_test_pca_scaled)\n",
        "print(f\"\\nJumlah komponen utama setelah PCA: {X_train.shape[1]}\")\n",
        "\n",
        "# Penanganan Ketidakseimbangan dengan SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train, y_train = smote.fit_resample(X_train, train_data['attack_cat'])\n",
        "y_test = test_data['attack_cat']\n",
        "print(\"\\nLabel Distribution After SMOTE:\")\n",
        "print(pd.Series(y_train).value_counts())\n",
        "\n",
        "# Konversi ke Tensor\n",
        "train_data = train_data.replace([np.inf, -np.inf], np.nan).dropna()\n",
        "test_data = test_data.replace([np.inf, -np.inf], np.nan).dropna()\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.int64)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.int64)\n",
        "print(\"\\nNaN in X_train_tensor:\", torch.isnan(X_train_tensor).any())\n",
        "print(\"Inf in X_train_tensor:\", torch.isinf(X_train_tensor).any())\n",
        "\n",
        "# DataLoader\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx]\n",
        "batch_size = 64\n",
        "train_dataset = CustomDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = CustomDataset(X_test_tensor, y_test_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "print(\"\\nTraining DataLoader - Number of batches:\", len(train_loader))\n",
        "print(\"Test DataLoader - Number of batches:\", len(test_loader))\n",
        "\n",
        "# Tambah dimensi channel untuk CNN\n",
        "X_train_tensor = X_train_tensor.unsqueeze(1)\n",
        "X_test_tensor = X_test_tensor.unsqueeze(1)\n",
        "print(\"\\nX_train_tensor Shape After Unsqueeze:\", X_train_tensor.shape)\n",
        "print(\"X_test_tensor Shape After Unsqueeze:\", X_test_tensor.shape)\n",
        "\n",
        "# Cek GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"ðŸš€ Using Device: {device}\")\n",
        "if device.type == \"cuda\":\n",
        "    print(f\"ðŸ”§ GPU Name: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementasi Model"
      ],
      "metadata": {
        "id": "vbwTIMV5VRhl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN + DBN (Ensemble)"
      ],
      "metadata": {
        "id": "_FH-2l-HH1OC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Cek apakah tqdm tersedia\n",
        "try:\n",
        "    from tqdm import tqdm\n",
        "    use_tqdm = True\n",
        "except ImportError:\n",
        "    use_tqdm = False\n",
        "    print(\"tqdm tidak tersedia. Menggunakan mode quiet...\")\n",
        "\n",
        "# Function untuk konversi data ke tensor\n",
        "def convert_to_tensor(data):\n",
        "    \"\"\"Convert different data types to PyTorch tensor.\"\"\"\n",
        "    if isinstance(data, torch.Tensor):\n",
        "        return data\n",
        "\n",
        "    # Jika pandas Series atau DataFrame\n",
        "    if hasattr(data, 'values'):\n",
        "        data = data.values\n",
        "\n",
        "    # Jika numpy array atau list\n",
        "    if isinstance(data, (np.ndarray, list)):\n",
        "        if isinstance(data[0], (int, np.integer)):\n",
        "            return torch.tensor(data, dtype=torch.long)\n",
        "        else:\n",
        "            return torch.tensor(data, dtype=torch.float32)\n",
        "\n",
        "    # Jika tipe data lain, coba konversi\n",
        "    try:\n",
        "        return torch.tensor(data)\n",
        "    except:\n",
        "        raise TypeError(f\"Cannot convert {type(data)} to torch.Tensor\")\n",
        "\n",
        "# Fungsi untuk mendapatkan jumlah kelas unik\n",
        "def get_num_classes(y_data):\n",
        "    \"\"\"Get number of unique classes in target data.\"\"\"\n",
        "    # Convert to tensor if not already\n",
        "    y_tensor = convert_to_tensor(y_data)\n",
        "\n",
        "    # Check number of unique classes\n",
        "    return len(torch.unique(y_tensor))\n",
        "\n",
        "# Membuat data loaders\n",
        "def create_data_loaders(X_train, y_train, X_test, y_test, batch_size=32):\n",
        "    # Konversi semua data ke tensor\n",
        "    X_train_tensor = convert_to_tensor(X_train)\n",
        "    y_train_tensor = convert_to_tensor(y_train)\n",
        "    X_test_tensor = convert_to_tensor(X_test)\n",
        "    y_test_tensor = convert_to_tensor(y_test)\n",
        "\n",
        "    # Split training data untuk validation set\n",
        "    X_train_np = X_train_tensor.numpy()\n",
        "    y_train_np = y_train_tensor.numpy()\n",
        "\n",
        "    X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
        "        X_train_np, y_train_np, test_size=0.2, random_state=42,\n",
        "        stratify=y_train_np if len(y_train_np.shape) == 1 else None)\n",
        "\n",
        "    # Konversi kembali ke tensor\n",
        "    X_train_tensor_split = torch.tensor(X_train_split, dtype=torch.float32)\n",
        "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
        "    y_train_tensor_split = torch.tensor(y_train_split, dtype=torch.long)\n",
        "    y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
        "\n",
        "    # Buat dataset\n",
        "    train_dataset = TensorDataset(X_train_tensor_split, y_train_tensor_split)\n",
        "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "    # Buat data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "    return train_loader, val_loader, test_loader, X_train_tensor\n",
        "\n",
        "# CNN Model\n",
        "class CNN_Model(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(CNN_Model, self).__init__()\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=128, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm1d(128)\n",
        "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm1d(256)\n",
        "        self.pool2 = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "        self.conv3 = nn.Conv1d(in_channels=256, out_channels=512, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm1d(512)\n",
        "        self.pool3 = nn.AdaptiveMaxPool1d(output_size=1)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(512 * 1, 128)\n",
        "        self.fc2 = nn.Linear(128, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.pool3(x)\n",
        "\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# RBM Model\n",
        "class RBM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(RBM, self).__init__()\n",
        "        self.W = nn.Parameter(torch.randn(input_size, hidden_size) * 0.1)\n",
        "        self.b = nn.Parameter(torch.zeros(hidden_size))\n",
        "        self.c = nn.Parameter(torch.zeros(input_size))\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "    def forward(self, v):\n",
        "        batch_size = v.size(0)\n",
        "\n",
        "        # Reshape input ke 2D jika perlu\n",
        "        if v.dim() > 2:\n",
        "            v = v.view(batch_size, -1)\n",
        "\n",
        "        # Pra-proses input untuk memastikan dimensi yang benar\n",
        "        if v.size(1) != self.input_size:\n",
        "            v_resized = torch.zeros(batch_size, self.input_size, device=v.device)\n",
        "            min_size = min(v.size(1), self.input_size)\n",
        "            v_resized[:, :min_size] = v[:, :min_size]\n",
        "            v = v_resized\n",
        "\n",
        "        # Propagate visible to hidden\n",
        "        h = torch.sigmoid(torch.matmul(v, self.W) + self.b)\n",
        "        return h\n",
        "\n",
        "    def reconstruct(self, h):\n",
        "        # Reconstruct visible from hidden\n",
        "        v_reconstructed = torch.sigmoid(torch.matmul(h, self.W.t()) + self.c)\n",
        "        return v_reconstructed\n",
        "\n",
        "# DBN Model\n",
        "class DBN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim):\n",
        "        super(DBN, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "\n",
        "        # Two layers of RBM\n",
        "        self.rbm1 = RBM(input_dim, hidden_dim1)\n",
        "        self.rbm2 = RBM(hidden_dim1, hidden_dim2)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(hidden_dim2, 128)\n",
        "        self.fc2 = nn.Linear(128, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Flatten input jika bukan 2D\n",
        "        batch_size = x.size(0)\n",
        "        if x.dim() > 2:\n",
        "            x = x.view(batch_size, -1)\n",
        "\n",
        "        h1 = self.rbm1(x)\n",
        "        h2 = self.rbm2(h1)\n",
        "        x = F.relu(self.fc1(h2))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Ensemble (CNN + DBN)\n",
        "class EnsembleModel(nn.Module):\n",
        "    def __init__(self, cnn_model, dbn_model, output_dim):\n",
        "        super(EnsembleModel, self).__init__()\n",
        "        self.cnn_model = cnn_model\n",
        "        self.dbn_model = dbn_model\n",
        "        self.fc = nn.Linear(output_dim * 2, output_dim)  # Menggabungkan output CNN dan DBN\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Save original input for DBN\n",
        "        x_original = x.clone()\n",
        "\n",
        "        # Input untuk CNN (tambahkan dimensi channel)\n",
        "        x_cnn = x.unsqueeze(1)\n",
        "\n",
        "        # Forward pass melalui CNN dan DBN\n",
        "        cnn_output = self.cnn_model(x_cnn)\n",
        "        dbn_output = self.dbn_model(x_original)\n",
        "\n",
        "        # Gabungkan output CNN dan DBN\n",
        "        combined_output = torch.cat((cnn_output, dbn_output), dim=1)\n",
        "\n",
        "        # Final output\n",
        "        final_output = self.fc(combined_output)\n",
        "\n",
        "        return final_output\n",
        "\n",
        "# Fungsi untuk visualisasi progres training\n",
        "def plot_training_progress(train_losses, val_losses, train_accs, val_accs):\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # Plot losses\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot accuracies\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(train_accs, label='Training Accuracy')\n",
        "    plt.plot(val_accs, label='Validation Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Fungsi untuk evaluasi model\n",
        "def evaluate_model(model, data_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # Return average loss and accuracy\n",
        "    return running_loss / len(data_loader), 100 * correct / total\n",
        "\n",
        "# Fungsi utama untuk training\n",
        "def train_ensemble_model(ensemble_model, train_loader, val_loader, criterion, optimizer, num_epochs, device, verbose=0):\n",
        "    # Lists untuk menyimpan metrics\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "    best_val_acc = 0.0\n",
        "\n",
        "    print(f\"Training model for {num_epochs} epochs...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    epoch_interate = 1\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Start Epoch {1} : Loading...\")\n",
        "        epoch_interate += 1\n",
        "        ensemble_model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        # Training loop\n",
        "        if use_tqdm and verbose > 0:\n",
        "            loader = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        else:\n",
        "            loader = train_loader\n",
        "\n",
        "        for inputs, labels in loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Reset gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = ensemble_model(inputs)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Update weights\n",
        "            optimizer.step()\n",
        "\n",
        "            # Statistik\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Hitung dan simpan metrik training\n",
        "        epoch_train_loss = running_loss / len(train_loader)\n",
        "        epoch_train_acc = 100 * correct / total\n",
        "        train_losses.append(epoch_train_loss)\n",
        "        train_accs.append(epoch_train_acc)\n",
        "\n",
        "        # Evaluasi model pada validation set\n",
        "        val_loss, val_acc = evaluate_model(ensemble_model, val_loader, criterion, device)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "        # Print statistik per epoch\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
        "              f\"Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.2f}%, \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "        # Simpan model terbaik berdasarkan validation accuracy\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(ensemble_model.state_dict(), 'best_ensemble_model.pt')\n",
        "            print(f\"âœ… Model saved with Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "    # Waktu training total\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"Training completed in {total_time/60:.2f} minutes\")\n",
        "\n",
        "    # Plot and visualize training progress\n",
        "    plot_training_progress(train_losses, val_losses, train_accs, val_accs)\n",
        "\n",
        "    # Load best model\n",
        "    try:\n",
        "        ensemble_model.load_state_dict(torch.load('best_ensemble_model.pt'))\n",
        "        print(\"Loaded best model based on validation accuracy\")\n",
        "    except:\n",
        "        print(\"Couldn't load best model, using current model instead\")\n",
        "\n",
        "    return ensemble_model, train_losses, val_losses, train_accs, val_accs\n",
        "\n",
        "# Fungsi untuk evaluasi dan visualisasi hasil\n",
        "def evaluate_and_visualize(model, test_loader, device):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Confusion Matrix\n",
        "    try:\n",
        "        from sklearn.metrics import confusion_matrix, classification_report\n",
        "        import seaborn as sns\n",
        "\n",
        "        cm = confusion_matrix(all_labels, all_preds)\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('True')\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.show()\n",
        "\n",
        "        # Classification report\n",
        "        print(\"Classification Report:\")\n",
        "        print(classification_report(all_labels, all_preds))\n",
        "    except ImportError:\n",
        "        print(\"scikit-learn atau seaborn tidak tersedia untuk confusion matrix\")\n",
        "\n",
        "    # Accuracy\n",
        "    accuracy = sum(1 for p, l in zip(all_preds, all_labels) if p == l) / len(all_preds)\n",
        "    print(f\"Test Accuracy: {accuracy*100:.2f}%\")\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "# Fungsi utama untuk melatih dan mengevaluasi model\n",
        "def run_training(X_train, y_train, X_test, y_test, num_epochs=10, batch_size=32, verbose=0):\n",
        "    print(\"CNN+DBN Ensemble Model Training\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # Buat data loaders\n",
        "    train_loader, val_loader, test_loader, X_train_tensor = create_data_loaders(\n",
        "        X_train, y_train, X_test, y_test, batch_size)\n",
        "\n",
        "    # Konversi y_train ke tensor untuk menghitung jumlah kelas\n",
        "    y_train_tensor = convert_to_tensor(y_train)\n",
        "\n",
        "    # Definisikan output_dim berdasarkan jumlah kelas\n",
        "    output_dim = get_num_classes(y_train_tensor)\n",
        "\n",
        "    # Input dimension\n",
        "    input_dim = X_train_tensor.shape[1]\n",
        "    print(f\"Input dimension: {input_dim}, Output classes: {output_dim}\")\n",
        "\n",
        "    # Inisialisasi model\n",
        "    cnn_model = CNN_Model(input_dim=input_dim, output_dim=output_dim)\n",
        "    dbn_model = DBN(input_dim=input_dim, hidden_dim1=256, hidden_dim2=128, output_dim=output_dim)\n",
        "    ensemble_model = EnsembleModel(cnn_model, dbn_model, output_dim=output_dim)\n",
        "\n",
        "    # Setup device\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "    ensemble_model.to(device)\n",
        "\n",
        "    # Loss dan optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(ensemble_model.parameters(), lr=0.001)\n",
        "\n",
        "    # Test model dengan mini-batch\n",
        "    if verbose > 0:\n",
        "        print(\"\\nTesting model with sample batch...\")\n",
        "\n",
        "    try:\n",
        "        sample_inputs, sample_labels = next(iter(train_loader))\n",
        "        sample_inputs, sample_labels = sample_inputs.to(device), sample_labels.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            sample_outputs = ensemble_model(sample_inputs)\n",
        "\n",
        "        if verbose > 0:\n",
        "            print(f\"Sample input shape: {sample_inputs.shape}\")\n",
        "            print(f\"Sample output shape: {sample_outputs.shape}\")\n",
        "            print(\"Forward pass successful!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during test: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return\n",
        "\n",
        "    # Train model\n",
        "    print(\"\\nStarting model training...\")\n",
        "    ensemble_model, train_losses, val_losses, train_accs, val_accs = train_ensemble_model(\n",
        "        ensemble_model, train_loader, val_loader, criterion, optimizer, num_epochs, device, verbose)\n",
        "\n",
        "    # Evaluasi final\n",
        "    print(\"\\nPerforming final evaluation...\")\n",
        "    test_acc = evaluate_and_visualize(ensemble_model, test_loader, device)\n",
        "\n",
        "    return ensemble_model, test_acc\n",
        "\n",
        "# Gunakan fungsi ini untuk melatih model\n",
        "if __name__ == \"__main__\":\n",
        "    # Contoh penggunaan (ganti dengan data Anda sendiri)\n",
        "    try:\n",
        "        # verbose=0 untuk mengurangi output\n",
        "        model, accuracy = run_training(X_train, y_train, X_test, y_test, num_epochs=10, verbose=0)\n",
        "        print(f\"Final accuracy: {accuracy:.2f}%\")\n",
        "    except NameError:\n",
        "        print(\"X_train atau y_train tidak ditemukan.\")\n",
        "        print(\"Pastikan Anda telah mendefinisikan data sebelum menjalankan script ini.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 814
        },
        "id": "Ynl6AzyMqdxF",
        "outputId": "ccf2bef9-43df-4bcc-dd0c-2438dd018e07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN+DBN Ensemble Model Training\n",
            "------------------------------\n",
            "Input dimension: 146, Output classes: 10\n",
            "Using device: cpu\n",
            "\n",
            "Starting model training...\n",
            "Training model for 10 epochs...\n",
            "Start Epoch 1 : Loading...\n",
            "Epoch 1/10, Train Loss: 0.9031, Train Acc: 65.05%, Val Loss: 0.7518, Val Acc: 71.60%\n",
            "âœ… Model saved with Val Acc: 71.60%\n",
            "Start Epoch 1 : Loading...\n",
            "Epoch 2/10, Train Loss: 0.7265, Train Acc: 71.70%, Val Loss: 0.6857, Val Acc: 73.27%\n",
            "âœ… Model saved with Val Acc: 73.27%\n",
            "Start Epoch 1 : Loading...\n",
            "Epoch 3/10, Train Loss: 0.6594, Train Acc: 74.11%, Val Loss: 0.6356, Val Acc: 75.38%\n",
            "âœ… Model saved with Val Acc: 75.38%\n",
            "Start Epoch 1 : Loading...\n",
            "Epoch 4/10, Train Loss: 0.6220, Train Acc: 75.57%, Val Loss: 0.5934, Val Acc: 76.60%\n",
            "âœ… Model saved with Val Acc: 76.60%\n",
            "Start Epoch 1 : Loading...\n",
            "Epoch 5/10, Train Loss: 0.5964, Train Acc: 76.53%, Val Loss: 0.6149, Val Acc: 75.88%\n",
            "Start Epoch 1 : Loading...\n",
            "Epoch 6/10, Train Loss: 0.5753, Train Acc: 77.31%, Val Loss: 0.5559, Val Acc: 78.08%\n",
            "âœ… Model saved with Val Acc: 78.08%\n",
            "Start Epoch 1 : Loading...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-3358a9fc4876>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0;31m# verbose=0 untuk mengurangi output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Final accuracy: {accuracy:.2f}%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mNameError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-3358a9fc4876>\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m(X_train, y_train, X_test, y_test, num_epochs, batch_size, verbose)\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nStarting model training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m     ensemble_model, train_losses, val_losses, train_accs, val_accs = train_ensemble_model(\n\u001b[0m\u001b[1;32m    451\u001b[0m         ensemble_model, train_loader, val_loader, criterion, optimizer, num_epochs, device, verbose)\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-3358a9fc4876>\u001b[0m in \u001b[0;36mtrain_ensemble_model\u001b[0;34m(ensemble_model, train_loader, val_loader, criterion, optimizer, num_epochs, device, verbose)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensemble_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;31m# Calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-3358a9fc4876>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;31m# Forward pass melalui CNN dan DBN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mcnn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mdbn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdbn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_original\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-3358a9fc4876>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    368\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m             )\n\u001b[0;32m--> 370\u001b[0;31m         return F.conv1d(\n\u001b[0m\u001b[1;32m    371\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}