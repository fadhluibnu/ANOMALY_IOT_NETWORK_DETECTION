{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "https://github.com/fadhluibnu/ANOMALY_IOT_NETWORK_DETECTION/blob/main/ANOMALY_IOT_NETWORK_DETECTION.ipynb",
      "authorship_tag": "ABX9TyPDYnlPrkD/a7sqh3Y3am6f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fadhluibnu/ANOMALY_IOT_NETWORK_DETECTION/blob/main/ANOMALY_IOT_NETWORK_DETECTION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "49jouBvKqtmE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from scipy.stats import zscore\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.decomposition import PCA\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, RobustScaler\n",
        "from sklearn.feature_selection import mutual_info_classif, SelectKBest\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from scipy.stats import zscore\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def complete_improved_preprocessing(train_data, test_data):\n",
        "    \"\"\"\n",
        "    Complete preprocessing pipeline with enhancements for IoT network anomaly detection.\n",
        "    Includes all original steps plus improvements for better model performance.\n",
        "\n",
        "    Args:\n",
        "        train_data: Original training dataframe\n",
        "        test_data: Original testing dataframe\n",
        "\n",
        "    Returns:\n",
        "        processed_train_data, processed_test_data: Enhanced datasets ready for modeling\n",
        "    \"\"\"\n",
        "    print(\"\\nüîç Starting enhanced preprocessing pipeline...\")\n",
        "\n",
        "    # Make copies to avoid modifying originals\n",
        "    train_data = train_data.copy()\n",
        "    test_data = test_data.copy()\n",
        "\n",
        "    # 1. Data Exploration\n",
        "    print(\"\\nüìä Dataset Overview:\")\n",
        "    print(f\"Training data shape: {train_data.shape}\")\n",
        "    print(f\"Testing data shape: {test_data.shape}\")\n",
        "\n",
        "    # Check for missing values\n",
        "    train_missing = train_data.isnull().sum().sum()\n",
        "    test_missing = test_data.isnull().sum().sum()\n",
        "    print(f\"Missing values - Training: {train_missing}, Testing: {test_missing}\")\n",
        "\n",
        "    # 2. Drop rows with missing attack_cat (target variable)\n",
        "    print(\"\\nüîç Handling missing target values...\")\n",
        "    train_data = train_data.dropna(subset=['attack_cat'])\n",
        "    test_data = test_data.dropna(subset=['attack_cat'])\n",
        "    print(f\"After dropping rows with missing targets - Train: {train_data.shape}, Test: {test_data.shape}\")\n",
        "\n",
        "    # 3. Handle remaining missing values\n",
        "    print(\"\\nüîç Handling remaining missing values...\")\n",
        "    # Identify numerical and categorical columns\n",
        "    numerical_cols = train_data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "    numerical_cols = [col for col in numerical_cols if col not in ['id', 'label', 'attack_cat']]\n",
        "    categorical_cols = train_data.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "    # Enhanced missing value imputation\n",
        "    for col in numerical_cols:\n",
        "        # Use median for numerical features (more robust than mean)\n",
        "        train_data[col] = train_data[col].fillna(train_data[col].median())\n",
        "        test_data[col] = test_data[col].fillna(train_data[col].median())\n",
        "\n",
        "    for col in categorical_cols:\n",
        "        if col != 'attack_cat':\n",
        "            # Use mode (most frequent) for categorical features\n",
        "            train_data[col] = train_data[col].fillna(train_data[col].mode()[0])\n",
        "            test_data[col] = test_data[col].fillna(train_data[col].mode()[0])\n",
        "\n",
        "    # 4. Remove duplicate rows\n",
        "    print(\"\\nüîç Removing duplicate entries...\")\n",
        "    train_data = train_data.drop_duplicates()\n",
        "    test_data = test_data.drop_duplicates()\n",
        "    print(f\"After removing duplicates - Train: {train_data.shape}, Test: {test_data.shape}\")\n",
        "\n",
        "    # 5. Encode attack_cat (target variable)\n",
        "    print(\"\\nüîç Encoding target variable...\")\n",
        "    attack_mapping = {\n",
        "        'Normal': 0, 'Generic': 1, 'Exploits': 2, 'Fuzzers': 3, 'DoS': 4,\n",
        "        'Reconnaissance': 5, 'Analysis': 6, 'Backdoor': 7, 'Shellcode': 8, 'Worms': 9\n",
        "    }\n",
        "    train_data['attack_cat'] = train_data['attack_cat'].map(attack_mapping)\n",
        "    test_data['attack_cat'] = test_data['attack_cat'].map(attack_mapping)\n",
        "\n",
        "    # Check class distribution\n",
        "    class_counts = train_data['attack_cat'].value_counts().sort_index()\n",
        "    print(\"Class distribution after encoding:\")\n",
        "    for class_id, count in class_counts.items():\n",
        "        print(f\"  Class {class_id}: {count} samples ({100*count/len(train_data):.2f}%)\")\n",
        "\n",
        "    # 6. Advanced feature transformation for skewed features\n",
        "    print(\"\\nüîç Applying transformations for skewed numerical features...\")\n",
        "    for col in numerical_cols:\n",
        "        # Check if data is significantly skewed\n",
        "        skewness = train_data[col].skew()\n",
        "        if abs(skewness) > 1:  # If moderately or highly skewed\n",
        "            # Apply log transformation (adding a constant to handle zeros/negatives)\n",
        "            train_min = train_data[col].min()\n",
        "            offset = 1 - min(0, train_min)  # Ensure all values are positive\n",
        "\n",
        "            train_data[col] = np.log1p(train_data[col] + offset)\n",
        "            test_data[col] = np.log1p(test_data[col] + offset)\n",
        "            print(f\"  Applied log transform to {col} (skewness: {skewness:.2f})\")\n",
        "\n",
        "    # 7. Encoding categorical features with enhanced handling\n",
        "    print(\"\\nüîç Encoding categorical features...\")\n",
        "    # Use OneHotEncoder with improved handling for test data\n",
        "    if len(categorical_cols) > 0:\n",
        "        encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "        encoded_train = encoder.fit_transform(train_data[categorical_cols])\n",
        "        encoded_test = encoder.transform(test_data[categorical_cols])\n",
        "\n",
        "        # Get feature names\n",
        "        feature_names = encoder.get_feature_names_out(categorical_cols)\n",
        "\n",
        "        # Create DataFrames with encoded features\n",
        "        encoded_train_df = pd.DataFrame(encoded_train, columns=feature_names, index=train_data.index)\n",
        "        encoded_test_df = pd.DataFrame(encoded_test, columns=feature_names, index=test_data.index)\n",
        "\n",
        "        # Drop original categorical columns and join encoded ones\n",
        "        train_data = train_data.drop(columns=categorical_cols).reset_index(drop=True)\n",
        "        test_data = test_data.drop(columns=categorical_cols).reset_index(drop=True)\n",
        "\n",
        "        train_data = pd.concat([train_data, encoded_train_df.reset_index(drop=True)], axis=1)\n",
        "        test_data = pd.concat([test_data, encoded_test_df.reset_index(drop=True)], axis=1)\n",
        "\n",
        "        print(f\"  Encoded {len(categorical_cols)} categorical features into {encoded_train.shape[1]} binary features\")\n",
        "\n",
        "    # 8. Feature selection using mutual information\n",
        "    print(\"\\nüîç Performing feature selection...\")\n",
        "    # Exclude target and ID columns\n",
        "    X_train = train_data.drop(columns=['id', 'label', 'attack_cat'])\n",
        "    y_train = train_data['attack_cat']\n",
        "\n",
        "    # Use mutual information for selecting most informative features\n",
        "    k = int(X_train.shape[1] * 0.8)  # Keep top 80% of features\n",
        "    selector = SelectKBest(mutual_info_classif, k=k)\n",
        "    selector.fit(X_train, y_train)\n",
        "\n",
        "    # Get selected feature names\n",
        "    selected_features = X_train.columns[selector.get_support()].tolist()\n",
        "    print(f\"  Selected {len(selected_features)} features out of {X_train.shape[1]}\")\n",
        "\n",
        "    # Keep only selected features\n",
        "    features_to_keep = ['id', 'label', 'attack_cat'] + selected_features\n",
        "    train_data = train_data[features_to_keep]\n",
        "    test_data = test_data[features_to_keep]\n",
        "\n",
        "    # Update numerical columns list to reflect selected features only\n",
        "    numerical_cols = [col for col in selected_features if col in numerical_cols]\n",
        "\n",
        "    # 9. Enhanced normalization using RobustScaler\n",
        "    print(\"\\nüîç Applying robust scaling to numerical features...\")\n",
        "    scaler = RobustScaler()  # More robust to outliers than StandardScaler\n",
        "    train_data[numerical_cols] = scaler.fit_transform(train_data[numerical_cols])\n",
        "    test_data[numerical_cols] = scaler.transform(test_data[numerical_cols])\n",
        "\n",
        "    # 10. Improved outlier handling with Winsorization\n",
        "    print(\"\\nüîç Handling outliers with Winsorization...\")\n",
        "    for col in numerical_cols:\n",
        "        # Use more conservative percentiles for winsorization\n",
        "        lower_bound = train_data[col].quantile(0.01)\n",
        "        upper_bound = train_data[col].quantile(0.99)\n",
        "\n",
        "        # Apply clipping to both train and test data\n",
        "        train_data[col] = train_data[col].clip(lower=lower_bound, upper=upper_bound)\n",
        "        test_data[col] = test_data[col].clip(lower=lower_bound, upper=upper_bound)\n",
        "\n",
        "    # 11. Enhanced SMOTE for imbalanced data\n",
        "    print(\"\\nüîç Applying SMOTE with improved parameters...\")\n",
        "    # Prepare data for SMOTE\n",
        "    X_train = train_data.drop(columns=['id', 'label', 'attack_cat'])\n",
        "    y_train = train_data['attack_cat']\n",
        "\n",
        "    # Print class distribution before SMOTE\n",
        "    print(\"  Class distribution before SMOTE:\")\n",
        "    for class_id, count in y_train.value_counts().sort_index().items():\n",
        "        print(f\"    Class {class_id}: {count} samples ({100*count/len(y_train):.2f}%)\")\n",
        "\n",
        "    # Apply SMOTE with improved parameters\n",
        "    smote = SMOTE(random_state=42, k_neighbors=7, sampling_strategy='auto')\n",
        "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "    # Print class distribution after SMOTE\n",
        "    print(\"  Class distribution after SMOTE:\")\n",
        "    for class_id, count in pd.Series(y_train_resampled).value_counts().sort_index().items():\n",
        "        print(f\"    Class {class_id}: {count} samples ({100*count/len(y_train_resampled):.2f}%)\")\n",
        "\n",
        "    # Create new DataFrame with resampled data\n",
        "    train_data_resampled = pd.DataFrame(X_train_resampled, columns=X_train.columns)\n",
        "    train_data_resampled['attack_cat'] = y_train_resampled\n",
        "    train_data_resampled['id'] = range(len(train_data_resampled))\n",
        "    train_data_resampled['label'] = train_data_resampled['attack_cat'] != 0  # Binary label (0=Normal)\n",
        "\n",
        "    # 12. Check for any remaining issues\n",
        "    print(\"\\nüîç Final data quality check...\")\n",
        "    # Check for infinities\n",
        "    train_data_resampled = train_data_resampled.replace([np.inf, -np.inf], np.nan)\n",
        "    test_data = test_data.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "    # Check for NaN and fill if any\n",
        "    if train_data_resampled.isnull().sum().sum() > 0:\n",
        "        print(f\"  Found {train_data_resampled.isnull().sum().sum()} NaN values in training data. Filling with column medians.\")\n",
        "        for col in train_data_resampled.columns:\n",
        "            if train_data_resampled[col].isnull().sum() > 0:\n",
        "                if train_data_resampled[col].dtype in ['int64', 'float64']:\n",
        "                    train_data_resampled[col] = train_data_resampled[col].fillna(train_data_resampled[col].median())\n",
        "\n",
        "    if test_data.isnull().sum().sum() > 0:\n",
        "        print(f\"  Found {test_data.isnull().sum().sum()} NaN values in test data. Filling with column medians.\")\n",
        "        for col in test_data.columns:\n",
        "            if test_data[col].isnull().sum() > 0:\n",
        "                if test_data[col].dtype in ['int64', 'float64']:\n",
        "                    test_data[col] = test_data[col].fillna(test_data[col].median())\n",
        "\n",
        "    # 13. Prepare data in PyTorch format\n",
        "    print(\"\\nüîç Preparing data for PyTorch...\")\n",
        "    # Extract features and targets\n",
        "    X_train_final = train_data_resampled.drop(columns=['id', 'label', 'attack_cat'])\n",
        "    y_train_final = train_data_resampled['attack_cat']\n",
        "    X_test_final = test_data.drop(columns=['id', 'label', 'attack_cat'])\n",
        "    y_test_final = test_data['attack_cat']\n",
        "\n",
        "    # Convert to tensors\n",
        "    X_train_tensor = torch.tensor(X_train_final.values, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train_final.values, dtype=torch.long)\n",
        "    X_test_tensor = torch.tensor(X_test_final.values, dtype=torch.float32)\n",
        "    y_test_tensor = torch.tensor(y_test_final.values, dtype=torch.long)\n",
        "\n",
        "    # Create TensorDatasets\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "    # 14. Create DataLoaders with optimized parameters\n",
        "    batch_size = 64  # Adjust based on available memory\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        drop_last=False\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        drop_last=False\n",
        "    )\n",
        "\n",
        "    print(f\"\\n‚úÖ Preprocessing complete!\")\n",
        "    print(f\"  Final training set: {len(train_dataset)} samples with {X_train_final.shape[1]} features\")\n",
        "    print(f\"  Final test set: {len(test_dataset)} samples with {X_test_final.shape[1]} features\")\n",
        "\n",
        "    # 15. Visualize class distribution\n",
        "    try:\n",
        "        plt.figure(figsize=(12, 5))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        train_class_counts = pd.Series(y_train_final).value_counts().sort_index()\n",
        "        plt.bar(train_class_counts.index.astype(str), train_class_counts.values)\n",
        "        plt.title('Training Data Class Distribution')\n",
        "        plt.xlabel('Attack Category')\n",
        "        plt.ylabel('Count')\n",
        "        plt.xticks(rotation=45)\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        test_class_counts = pd.Series(y_test_final).value_counts().sort_index()\n",
        "        plt.bar(test_class_counts.index.astype(str), test_class_counts.values)\n",
        "        plt.title('Test Data Class Distribution')\n",
        "        plt.xlabel('Attack Category')\n",
        "        plt.ylabel('Count')\n",
        "        plt.xticks(rotation=45)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('class_distribution.png')\n",
        "        plt.show()\n",
        "    except:\n",
        "        print(\"  Could not create visualization (possibly running in non-graphical environment)\")\n",
        "\n",
        "    # Return processed data and PyTorch loaders\n",
        "    return {\n",
        "        'train_data': train_data_resampled,\n",
        "        'test_data': test_data,\n",
        "        'X_train': X_train_final,\n",
        "        'y_train': y_train_final,\n",
        "        'X_test': X_test_final,\n",
        "        'y_test': y_test_final,\n",
        "        'train_loader': train_loader,\n",
        "        'test_loader': test_loader,\n",
        "        'input_dim': X_train_final.shape[1],\n",
        "        'num_classes': len(np.unique(y_train_final))\n",
        "    }\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # This section runs when the script is executed directly\n",
        "    print(\"This is a preprocessing module for IoT Network Anomaly Detection.\")\n",
        "    print(\"Import this module and call the complete_improved_preprocessing function.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwKsR9UJrH5W",
        "outputId": "f36f7a75-5c95-4eee-fb34-5deb08c020a9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a preprocessing module for IoT Network Anomaly Detection.\n",
            "Import this module and call the complete_improved_preprocessing function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Improved CNN Model with Residual Connections\n",
        "class ImprovedCNN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, dropout_rate=0.4):\n",
        "        super(ImprovedCNN, self).__init__()\n",
        "\n",
        "        # Initial convolutional layer\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=128, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm1d(128)\n",
        "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Residual block 1\n",
        "        self.conv2a = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
        "        self.bn2a = nn.BatchNorm1d(256)\n",
        "        self.conv2b = nn.Conv1d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
        "        self.bn2b = nn.BatchNorm1d(256)\n",
        "        # Skip connection\n",
        "        self.skip_conn1 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=1)\n",
        "\n",
        "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Residual block 2\n",
        "        self.conv3a = nn.Conv1d(in_channels=256, out_channels=512, kernel_size=3, padding=1)\n",
        "        self.bn3a = nn.BatchNorm1d(512)\n",
        "        self.conv3b = nn.Conv1d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
        "        self.bn3b = nn.BatchNorm1d(512)\n",
        "        # Skip connection\n",
        "        self.skip_conn2 = nn.Conv1d(in_channels=256, out_channels=512, kernel_size=1)\n",
        "\n",
        "        self.pool3 = nn.AdaptiveMaxPool1d(output_size=1)\n",
        "        self.dropout3 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Attention mechanism for feature importance\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(512, 128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(512, 256)\n",
        "        self.bn_fc1 = nn.BatchNorm1d(256)\n",
        "        self.dropout_fc1 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.bn_fc2 = nn.BatchNorm1d(128)\n",
        "        self.dropout_fc2 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        self.fc3 = nn.Linear(128, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initial layer\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = F.leaky_relu(x, negative_slope=0.1)\n",
        "        x = self.pool1(x)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        # Residual block 1\n",
        "        residual = self.skip_conn1(x)\n",
        "        x = self.conv2a(x)\n",
        "        x = self.bn2a(x)\n",
        "        x = F.leaky_relu(x, negative_slope=0.1)\n",
        "        x = self.conv2b(x)\n",
        "        x = self.bn2b(x)\n",
        "        # Add residual connection\n",
        "        x = x + residual\n",
        "        x = F.leaky_relu(x, negative_slope=0.1)\n",
        "        x = self.pool2(x)\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        # Residual block 2\n",
        "        residual = self.skip_conn2(x)\n",
        "        x = self.conv3a(x)\n",
        "        x = self.bn3a(x)\n",
        "        x = F.leaky_relu(x, negative_slope=0.1)\n",
        "        x = self.conv3b(x)\n",
        "        x = self.bn3b(x)\n",
        "        # Add residual connection\n",
        "        x = x + residual\n",
        "        x = F.leaky_relu(x, negative_slope=0.1)\n",
        "        x = self.pool3(x)\n",
        "        x = self.dropout3(x)\n",
        "\n",
        "        # Flatten\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # Apply attention\n",
        "        att_weights = self.attention(x)\n",
        "        x = x * att_weights\n",
        "\n",
        "        # Fully connected layers\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn_fc1(x)\n",
        "        x = F.leaky_relu(x, negative_slope=0.1)\n",
        "        x = self.dropout_fc1(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = self.bn_fc2(x)\n",
        "        x = F.leaky_relu(x, negative_slope=0.1)\n",
        "        x = self.dropout_fc2(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Improved RBM with enhanced training\n",
        "class ImprovedRBM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(ImprovedRBM, self).__init__()\n",
        "        self.W = nn.Parameter(torch.randn(input_size, hidden_size) * 0.01)  # Smaller init for better stability\n",
        "        self.b = nn.Parameter(torch.zeros(hidden_size))\n",
        "        self.c = nn.Parameter(torch.zeros(input_size))\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Initialize parameters properly - Xavier/Glorot initialization\n",
        "        nn.init.xavier_uniform_(self.W)\n",
        "\n",
        "    def forward(self, v):\n",
        "        batch_size = v.size(0)\n",
        "\n",
        "        # Reshape input to 2D if needed\n",
        "        if v.dim() > 2:\n",
        "            v = v.view(batch_size, -1)\n",
        "\n",
        "        # Handle dimension mismatch more gracefully\n",
        "        if v.size(1) != self.input_size:\n",
        "            v_resized = torch.zeros(batch_size, self.input_size, device=v.device)\n",
        "            min_size = min(v.size(1), self.input_size)\n",
        "            v_resized[:, :min_size] = v[:, :min_size]\n",
        "            v = v_resized\n",
        "\n",
        "        # Apply normalization for better stability\n",
        "        v = F.normalize(v, p=2, dim=1)\n",
        "\n",
        "        # Propagate visible to hidden with LeakyReLU for better gradients\n",
        "        h_activation = torch.matmul(v, self.W) + self.b\n",
        "        h = torch.sigmoid(h_activation)\n",
        "\n",
        "        return h\n",
        "\n",
        "    def reconstruct(self, h):\n",
        "        # Reconstruct visible from hidden\n",
        "        v_activation = torch.matmul(h, self.W.t()) + self.c\n",
        "        v_reconstructed = torch.sigmoid(v_activation)\n",
        "        return v_reconstructed\n",
        "\n",
        "    def free_energy(self, v):\n",
        "        # Calculate free energy for monitoring convergence\n",
        "        wx_b = torch.matmul(v, self.W) + self.b\n",
        "        hidden_term = torch.sum(F.softplus(wx_b), dim=1)\n",
        "        visible_term = torch.matmul(v, self.c)\n",
        "        return -hidden_term - visible_term\n",
        "\n",
        "\n",
        "# Improved DBN with enhanced architecture\n",
        "class ImprovedDBN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims, output_dim, dropout_rate=0.4):\n",
        "        super(ImprovedDBN, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dims = hidden_dims\n",
        "\n",
        "        # Create a stack of RBMs with more layers for better feature hierarchy\n",
        "        self.rbm_layers = nn.ModuleList()\n",
        "\n",
        "        # First RBM\n",
        "        self.rbm_layers.append(ImprovedRBM(input_dim, hidden_dims[0]))\n",
        "\n",
        "        # Additional RBM layers\n",
        "        for i in range(1, len(hidden_dims)):\n",
        "            self.rbm_layers.append(ImprovedRBM(hidden_dims[i-1], hidden_dims[i]))\n",
        "\n",
        "        # Fully connected layers with dropout for regularization\n",
        "        self.fc_layers = nn.ModuleList()\n",
        "\n",
        "        # First FC layer from last RBM's output\n",
        "        self.fc_layers.append(nn.Sequential(\n",
        "            nn.Linear(hidden_dims[-1], 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Dropout(dropout_rate)\n",
        "        ))\n",
        "\n",
        "        # Second FC layer\n",
        "        self.fc_layers.append(nn.Sequential(\n",
        "            nn.Linear(256, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Dropout(dropout_rate)\n",
        "        ))\n",
        "\n",
        "        # Output layer\n",
        "        self.output_layer = nn.Linear(128, output_dim)\n",
        "\n",
        "        # Apply better initialization for fully connected layers\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Flatten input if not already flat\n",
        "        batch_size = x.size(0)\n",
        "        if x.dim() > 2:\n",
        "            x = x.view(batch_size, -1)\n",
        "\n",
        "        # Apply RBM layers sequentially\n",
        "        h = x\n",
        "        for rbm in self.rbm_layers:\n",
        "            h = rbm(h)\n",
        "\n",
        "        # Apply fully connected layers\n",
        "        for fc in self.fc_layers:\n",
        "            h = fc(h)\n",
        "\n",
        "        # Output layer\n",
        "        output = self.output_layer(h)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "# Improved Ensemble model with attention mechanisms\n",
        "class ImprovedEnsembleModel(nn.Module):\n",
        "    def __init__(self, cnn_model, dbn_model, output_dim):\n",
        "        super(ImprovedEnsembleModel, self).__init__()\n",
        "        self.cnn_model = cnn_model\n",
        "        self.dbn_model = dbn_model\n",
        "\n",
        "        # Dynamic weighting of CNN and DBN outputs using attention\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(output_dim * 2, 64),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Linear(64, 2),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "        # Improved combiner network\n",
        "        self.combiner = nn.Sequential(\n",
        "            nn.Linear(output_dim * 2, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Linear(256, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Linear(128, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Save original input for DBN\n",
        "        x_original = x.clone()\n",
        "\n",
        "        # Input for CNN (add channel dimension)\n",
        "        x_cnn = x.unsqueeze(1)\n",
        "\n",
        "        # Forward pass through CNN and DBN\n",
        "        cnn_output = self.cnn_model(x_cnn)\n",
        "        dbn_output = self.dbn_model(x_original)\n",
        "\n",
        "        # Concatenate outputs\n",
        "        combined = torch.cat((cnn_output, dbn_output), dim=1)\n",
        "\n",
        "        # Calculate attention weights\n",
        "        weights = self.attention(combined)\n",
        "\n",
        "        # Apply attention weights\n",
        "        weighted_cnn = cnn_output * weights[:, 0].unsqueeze(1)\n",
        "        weighted_dbn = dbn_output * weights[:, 1].unsqueeze(1)\n",
        "\n",
        "        # Combine weighted outputs\n",
        "        weighted_combined = torch.cat((weighted_cnn, weighted_dbn), dim=1)\n",
        "\n",
        "        # Final output through combiner network\n",
        "        final_output = self.combiner(weighted_combined)\n",
        "\n",
        "        return final_output"
      ],
      "metadata": {
        "id": "534T_CZwrJ2h"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Advanced training function with additional enhancements\n",
        "def train_improved_ensemble(model, train_loader, val_loader, test_loader, device, epochs=30):\n",
        "    print(\"Starting advanced training process...\")\n",
        "\n",
        "    # 1. Learning rate scheduler for better optimization\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=3, verbose=True, min_lr=1e-6\n",
        "    )\n",
        "\n",
        "    # 2. Use label smoothing for better generalization\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "    # Tracking metrics\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "    best_val_acc = 0.0\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "\n",
        "    # For confusion matrix visualization\n",
        "    from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "    # Training loop with early stopping\n",
        "    for epoch in range(epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        # Use tqdm for progress visualization if available\n",
        "        try:\n",
        "            from tqdm import tqdm\n",
        "            loader_iterator = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "        except ImportError:\n",
        "            loader_iterator = train_loader\n",
        "\n",
        "        for inputs, labels in loader_iterator:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping to prevent exploding gradients\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update statistics\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Calculate epoch metrics\n",
        "        epoch_train_loss = running_loss / len(train_loader)\n",
        "        epoch_train_acc = 100.0 * correct / total\n",
        "        train_losses.append(epoch_train_loss)\n",
        "        train_accs.append(epoch_train_acc)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        all_preds = []\n",
        "        all_targets = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(inputs)\n",
        "\n",
        "                # Calculate loss\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                # Calculate accuracy\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "                # Save predictions and targets for confusion matrix\n",
        "                all_preds.extend(predicted.cpu().numpy())\n",
        "                all_targets.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Calculate validation metrics\n",
        "        epoch_val_loss = val_loss / len(val_loader)\n",
        "        epoch_val_acc = 100.0 * val_correct / val_total\n",
        "        val_losses.append(epoch_val_loss)\n",
        "        val_accs.append(epoch_val_acc)\n",
        "\n",
        "        # Update learning rate based on validation loss\n",
        "        scheduler.step(epoch_val_loss)\n",
        "\n",
        "        # Print epoch results\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | \"\n",
        "              f\"Train Loss: {epoch_train_loss:.4f} | Train Acc: {epoch_train_acc:.2f}% | \"\n",
        "              f\"Val Loss: {epoch_val_loss:.4f} | Val Acc: {epoch_val_acc:.2f}%\")\n",
        "\n",
        "        # Check if this is the best model so far\n",
        "        if epoch_val_acc > best_val_acc:\n",
        "            best_val_acc = epoch_val_acc\n",
        "            best_val_loss = epoch_val_loss\n",
        "            patience_counter = 0\n",
        "\n",
        "            # Save the best model\n",
        "            torch.save(model.state_dict(), 'best_improved_ensemble.pt')\n",
        "            print(f\"‚úÖ New best model saved with Val Acc: {epoch_val_acc:.2f}%\")\n",
        "\n",
        "            # Generate confusion matrix for best model\n",
        "            try:\n",
        "                import matplotlib.pyplot as plt\n",
        "                import seaborn as sns\n",
        "\n",
        "                # Calculate confusion matrix\n",
        "                cm = confusion_matrix(all_targets, all_preds)\n",
        "                plt.figure(figsize=(10, 8))\n",
        "                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "                plt.title(f'Confusion Matrix - Epoch {epoch+1}')\n",
        "                plt.xlabel('Predicted')\n",
        "                plt.ylabel('True')\n",
        "                plt.savefig(f'confusion_matrix_epoch_{epoch+1}.png')\n",
        "                plt.close()\n",
        "\n",
        "                # Print detailed classification report\n",
        "                print(\"\\nClassification Report:\")\n",
        "                print(classification_report(all_targets, all_preds))\n",
        "            except Exception as e:\n",
        "                print(f\"Could not generate confusion matrix: {e}\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        # Early stopping check\n",
        "        if patience_counter >= 7:  # Stop if no improvement for 7 epochs\n",
        "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
        "            break\n",
        "\n",
        "    # Final evaluation on test set\n",
        "    print(\"\\nEvaluating final model on test set...\")\n",
        "    model.load_state_dict(torch.load('best_improved_ensemble.pt'))\n",
        "    model.eval()\n",
        "    test_correct = 0\n",
        "    test_total = 0\n",
        "    all_test_preds = []\n",
        "    all_test_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            test_total += labels.size(0)\n",
        "            test_correct += (predicted == labels).sum().item()\n",
        "            all_test_preds.extend(predicted.cpu().numpy())\n",
        "            all_test_targets.extend(labels.cpu().numpy())\n",
        "\n",
        "    test_acc = 100.0 * test_correct / test_total\n",
        "    print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "    # Generate final confusion matrix and report\n",
        "    try:\n",
        "        cm = confusion_matrix(all_test_targets, all_test_preds)\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "        plt.title('Final Test Confusion Matrix')\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('True')\n",
        "        plt.savefig('final_test_confusion_matrix.png')\n",
        "        plt.show()\n",
        "\n",
        "        print(\"\\nFinal Classification Report:\")\n",
        "        print(classification_report(all_test_targets, all_test_preds))\n",
        "    except Exception as e:\n",
        "        print(f\"Could not generate final confusion matrix: {e}\")\n",
        "\n",
        "    # Plot training history\n",
        "    plt.figure(figsize=(15, 6))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.title('Loss over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(train_accs, label='Training Accuracy')\n",
        "    plt.plot(val_accs, label='Validation Accuracy')\n",
        "    plt.title('Accuracy over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_history.png')\n",
        "    plt.show()\n",
        "\n",
        "    return model, test_acc"
      ],
      "metadata": {
        "id": "OImxLLf1rMI4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main implementation with all improvements integrated\n",
        "def run_improved_model(train_data, test_data):\n",
        "    print(\"\\nüöÄ Starting improved IoT anomaly detection pipeline...\")\n",
        "\n",
        "    # Step 1: Enhanced preprocessing\n",
        "    print(\"\\nüìä Performing enhanced data preprocessing...\")\n",
        "    # train_data_processed, test_data_processed = improved_preprocessing(train_data, test_data)\n",
        "\n",
        "    # # Prepare data for model\n",
        "    # X_train = train_data_processed.drop(columns=['id', 'label', 'attack_cat'])\n",
        "    # y_train = train_data_processed['attack_cat']\n",
        "    # X_test = test_data_processed.drop(columns=['id', 'label', 'attack_cat'])\n",
        "    # y_test = test_data_processed['attack_cat']\n",
        "\n",
        "    processed_data = complete_improved_preprocessing(train_data, test_data)\n",
        "\n",
        "    X_train = processed_data['X_train']\n",
        "    y_train = processed_data['y_train']\n",
        "    X_test = processed_data['X_test']\n",
        "    y_test = processed_data['y_test']\n",
        "\n",
        "    # Step 2: Convert to tensors\n",
        "    X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
        "    X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
        "    y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
        "\n",
        "    # Step 3: Create datasets and data loaders\n",
        "    # Split training data into train and validation sets\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
        "        X_train_tensor, y_train_tensor, test_size=0.15, random_state=42, stratify=y_train_tensor\n",
        "    )\n",
        "\n",
        "    # Create data loaders with appropriate batch sizes\n",
        "    batch_size = 128  # Larger batch size for faster training\n",
        "\n",
        "    train_dataset = TensorDataset(X_train_final, y_train_final)\n",
        "    val_dataset = TensorDataset(X_val, y_val)\n",
        "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "    print(f\"Train set: {len(train_dataset)} samples\")\n",
        "    print(f\"Validation set: {len(val_dataset)} samples\")\n",
        "    print(f\"Test set: {len(test_dataset)} samples\")\n",
        "\n",
        "    # Step 4: Check device and set up\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"\\nüîß Using device: {device}\")\n",
        "\n",
        "    # Step 5: Create improved models\n",
        "    input_dim = X_train.shape[1]\n",
        "    num_classes = len(torch.unique(y_train_tensor))\n",
        "\n",
        "    # CNN with improved architecture\n",
        "    cnn_model = ImprovedCNN(input_dim=input_dim, output_dim=num_classes)\n",
        "\n",
        "    # DBN with multiple hidden layers\n",
        "    dbn_model = ImprovedDBN(\n",
        "        input_dim=input_dim,\n",
        "        hidden_dims=[512, 384, 256],  # Deeper architecture\n",
        "        output_dim=num_classes\n",
        "    )\n",
        "\n",
        "    # Ensemble model\n",
        "    ensemble_model = ImprovedEnsembleModel(cnn_model, dbn_model, output_dim=num_classes)\n",
        "    ensemble_model = ensemble_model.to(device)\n",
        "\n",
        "    # Print model summary if pytorch_model_summary is available\n",
        "    try:\n",
        "        from pytorch_model_summary import summary\n",
        "        print(\"\\nüìã Model Architecture Summary:\")\n",
        "        print(summary(ensemble_model, torch.zeros((1, input_dim)).to(device), show_input=True))\n",
        "    except ImportError:\n",
        "        print(\"\\nCould not print model summary. Install pytorch_model_summary for detailed architecture view.\")\n",
        "\n",
        "    # Step 6: Train the model with improved training loop\n",
        "    print(\"\\nüèãÔ∏è‚Äç‚ôÄÔ∏è Starting advanced training process...\")\n",
        "    trained_model, test_accuracy = train_improved_ensemble(\n",
        "        model=ensemble_model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        test_loader=test_loader,\n",
        "        device=device,\n",
        "        epochs=30  # Train for more epochs with early stopping\n",
        "    )\n",
        "\n",
        "    print(f\"\\nüèÅ Final test accuracy: {test_accuracy:.2f}%\")\n",
        "    return trained_model, test_accuracy"
      ],
      "metadata": {
        "id": "gnYQ482prOGk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the improved modules\n",
        "# First, save the code snippets above as Python files\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Load your data\n",
        "    # Assuming train_data and test_data are already loaded from your CSV files\n",
        "\n",
        "    train_data = pd.read_csv(\"/content/drive/MyDrive/ANOMALI DING DING/UNSW_NB15_training-set.csv\")\n",
        "    test_data = pd.read_csv(\"/content/drive/MyDrive/ANOMALI DING DING/UNSW_NB15_testing-set.csv\")\n",
        "\n",
        "    # Run the improved model\n",
        "    trained_model, accuracy = run_improved_model(train_data, test_data)\n",
        "\n",
        "    print(f\"Final model accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "    # Save the final model\n",
        "    # torch.save(trained_model.state_dict(), \"final_anomaly_detection_model.pt\")\n",
        "    # print(\"Model saved successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IECfHCNHrWnv",
        "outputId": "e01047a7-755c-4c85-a380-8adab4ee38d8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üöÄ Starting improved IoT anomaly detection pipeline...\n",
            "\n",
            "üìä Performing enhanced data preprocessing...\n",
            "\n",
            "üîç Starting enhanced preprocessing pipeline...\n",
            "\n",
            "üìä Dataset Overview:\n",
            "Training data shape: (82332, 45)\n",
            "Testing data shape: (175341, 45)\n",
            "Missing values - Training: 0, Testing: 0\n",
            "\n",
            "üîç Handling missing target values...\n",
            "After dropping rows with missing targets - Train: (82332, 45), Test: (175341, 45)\n",
            "\n",
            "üîç Handling remaining missing values...\n",
            "\n",
            "üîç Removing duplicate entries...\n",
            "After removing duplicates - Train: (82332, 45), Test: (175341, 45)\n",
            "\n",
            "üîç Encoding target variable...\n",
            "Class distribution after encoding:\n",
            "  Class 0: 37000 samples (44.94%)\n",
            "  Class 1: 18871 samples (22.92%)\n",
            "  Class 2: 11132 samples (13.52%)\n",
            "  Class 3: 6062 samples (7.36%)\n",
            "  Class 4: 4089 samples (4.97%)\n",
            "  Class 5: 3496 samples (4.25%)\n",
            "  Class 6: 677 samples (0.82%)\n",
            "  Class 7: 583 samples (0.71%)\n",
            "  Class 8: 378 samples (0.46%)\n",
            "  Class 9: 44 samples (0.05%)\n",
            "\n",
            "üîç Applying transformations for skewed numerical features...\n",
            "  Applied log transform to dur (skewness: 9.52)\n",
            "  Applied log transform to spkts (skewness: 47.75)\n",
            "  Applied log transform to dpkts (skewness: 49.30)\n",
            "  Applied log transform to sbytes (skewness: 53.78)\n",
            "  Applied log transform to dbytes (skewness: 52.55)\n",
            "  Applied log transform to rate (skewness: 3.50)\n",
            "  Applied log transform to sload (skewness: 9.49)\n",
            "  Applied log transform to dload (skewness: 4.75)\n",
            "  Applied log transform to sloss (skewness: 52.47)\n",
            "  Applied log transform to dloss (skewness: 54.47)\n",
            "  Applied log transform to sinpkt (skewness: 9.27)\n",
            "  Applied log transform to dinpkt (skewness: 23.01)\n",
            "  Applied log transform to sjit (skewness: 15.35)\n",
            "  Applied log transform to djit (skewness: 60.56)\n",
            "  Applied log transform to tcprtt (skewness: 9.06)\n",
            "  Applied log transform to synack (skewness: 13.81)\n",
            "  Applied log transform to ackdat (skewness: 9.63)\n",
            "  Applied log transform to smean (skewness: 3.54)\n",
            "  Applied log transform to dmean (skewness: 3.11)\n",
            "  Applied log transform to trans_depth (skewness: 170.79)\n",
            "  Applied log transform to response_body_len (skewness: 74.64)\n",
            "  Applied log transform to ct_srv_src (skewness: 1.86)\n",
            "  Applied log transform to ct_state_ttl (skewness: 1.52)\n",
            "  Applied log transform to ct_dst_ltm (skewness: 2.67)\n",
            "  Applied log transform to ct_src_dport_ltm (skewness: 2.86)\n",
            "  Applied log transform to ct_dst_sport_ltm (skewness: 2.51)\n",
            "  Applied log transform to ct_dst_src_ltm (skewness: 2.19)\n",
            "  Applied log transform to is_ftp_login (skewness: 11.04)\n",
            "  Applied log transform to ct_ftp_cmd (skewness: 11.24)\n",
            "  Applied log transform to ct_flw_http_mthd (skewness: 12.56)\n",
            "  Applied log transform to ct_src_ltm (skewness: 2.44)\n",
            "  Applied log transform to ct_srv_dst (skewness: 1.90)\n",
            "  Applied log transform to is_sm_ips_ports (skewness: 9.32)\n",
            "\n",
            "üîç Encoding categorical features...\n",
            "  Encoded 4 categorical features into 161 binary features\n",
            "\n",
            "üîç Performing feature selection...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"['attack_cat'] not found in axis\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-935c3880aa7b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Run the improved model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_improved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Final model accuracy: {accuracy:.2f}%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-29568046bfaf>\u001b[0m in \u001b[0;36mrun_improved_model\u001b[0;34m(train_data, test_data)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# y_test = test_data_processed['attack_cat']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mprocessed_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomplete_improved_preprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessed_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-b2ae5a8045bb>\u001b[0m in \u001b[0;36mcomplete_improved_preprocessing\u001b[0;34m(train_data, test_data)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nüîç Performing feature selection...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;31m# Exclude target and ID columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'label'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'attack_cat'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attack_cat'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5579\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[0;36m1.0\u001b[0m     \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5580\u001b[0m         \"\"\"\n\u001b[0;32m-> 5581\u001b[0;31m         return super().drop(\n\u001b[0m\u001b[1;32m   5582\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5583\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4786\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4787\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4788\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4790\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4828\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4829\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4830\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4831\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   7068\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7069\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7070\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{labels[mask].tolist()} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7071\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7072\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['attack_cat'] not found in axis\""
          ]
        }
      ]
    }
  ]
}